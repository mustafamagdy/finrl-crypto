{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTC, ETH, BNB, ADA, SOL, MATIC, DOT, LINK (Solana) Trading Model Training\n",
    "\n",
    "## Overview\n",
    "This notebook implements a high-performance reinforcement learning trading strategy for BTC, ETH, BNB, ADA, SOL, MATIC, DOT, LINK using the PPO algorithm.\n",
    "\n",
    "**Key Features:**\n",
    "- Zero data leakage methodology\n",
    "- Solana-specific feature engineering\n",
    "- High-frequency trading optimization\n",
    "- Statistical significance testing\n",
    "- Performance-focused analysis\n",
    "\n",
    "**BTC, ETH, BNB, ADA, SOL, MATIC, DOT, LINK Trading Characteristics:**\n",
    "- High-performance blockchain with sub-second finality\n",
    "- Strong DeFi and NFT ecosystem\n",
    "- Higher volatility patterns\n",
    "- Active developer community and ecosystem growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Environment Setup and Dependencies\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FinRL imports\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "# Stable Baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna\n",
    "import torch\n",
    "\n",
    "# Import our patch\n",
    "import finrl_patch\n",
    "\n",
    "# Configure plotting for SOL\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"plasma\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete for SOL (Solana) trading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: SOL Data Loading and Market Analysis\n",
    "def load_full_portfolio_data():\n",
    "    \"\"\"Load SOL cryptocurrency data with Solana-specific preprocessing\"\"\"\n",
    "    \n",
    "    # Load from CSV (assuming we have downloaded data)\n",
    "    try:\n",
    "        df = pd.read_csv('../../data/BTCUSDT_5m.csv')\n",
    "        print(f\"Loaded {len(df)} rows of SOL data\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV not found, downloading fresh SOL data...\")\n",
    "        # Fallback to download if CSV doesn't exist\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=365*2)  # 2 years\n",
    "        \n",
    "        df = YahooDownloader(start_date=start_date.strftime('%Y-%m-%d'),\n",
    "                           end_date=end_date.strftime('%Y-%m-%d'),\n",
    "                           ticker_list=['SOL-USD']).fetch_data()\n",
    "    \n",
    "    # Standardize column names\n",
    "    if 'open_time' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['open_time'])\n",
    "    elif 'date' not in df.columns:\n",
    "        df.reset_index(inplace=True)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Required columns for FinRL\n",
    "    required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "    \n",
    "    # Map columns if needed\n",
    "    column_mapping = {\n",
    "        'open_price': 'open',\n",
    "        'high_price': 'high', \n",
    "        'low_price': 'low',\n",
    "        'close_price': 'close',\n",
    "        'volume': 'volume'\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df.columns:\n",
    "            df[new_name] = df[old_name]\n",
    "    \n",
    "    # Ensure we have all required columns\n",
    "    df = df[required_cols + (['tic'] if 'tic' in df.columns else [])]\n",
    "    \n",
    "    # Add ticker if not present\n",
    "    if 'tic' not in df.columns:\n",
    "        df['tic'] = 'BTCUSDT'\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Basic data cleaning\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"üìä SOL Data shape: {df.shape}\")\n",
    "    print(f\"üìÖ Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"üí∞ Price range: ${df['close'].min():.2f} - ${df['close'].max():.2f}\")\n",
    "    print(f\"üìà Average daily volume: {df['volume'].mean():,.0f}\")\n",
    "    \n",
    "    # Solana-specific market analysis\n",
    "    price_changes = df['close'].pct_change().dropna()\n",
    "    high_vol_periods = price_changes[abs(price_changes) > price_changes.std() * 2]\n",
    "    \n",
    "    print(f\"\\nüî• SOL Market Characteristics:\")\n",
    "    print(f\"   Average 5min return: {price_changes.mean()*100:.4f}%\")\n",
    "    print(f\"   Volatility (std): {price_changes.std()*100:.4f}%\")\n",
    "    print(f\"   High volatility periods: {len(high_vol_periods)} ({len(high_vol_periods)/len(price_changes)*100:.1f}%)\")\n",
    "    print(f\"   Max single period gain: {price_changes.max()*100:.2f}%\")\n",
    "    print(f\"   Max single period loss: {price_changes.min()*100:.2f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the SOL data\n",
    "raw_data = load_full_portfolio_data()\n",
    "\n",
    "# Display basic statistics\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Solana-Specific Feature Engineering\n",
    "def create_full_portfolio_features(df):\n",
    "    \"\"\"Create technical indicators optimized for SOL's high-performance trading patterns\"\"\"\n",
    "    \n",
    "    fe = FeatureEngineer(\n",
    "        use_technical_indicator=True,\n",
    "        tech_indicator_list=['macd', 'rsi_30', 'cci_30', 'dx_30'],\n",
    "        use_vix=False,\n",
    "        use_turbulence=False,\n",
    "        user_defined_feature=False\n",
    "    )\n",
    "    \n",
    "    processed_data = fe.preprocess_data(df)\n",
    "    \n",
    "    # SOL-specific features\n",
    "    processed_data = processed_data.sort_values(['date', 'tic']).reset_index(drop=True)\n",
    "    \n",
    "    # High-frequency volatility features (SOL is very active)\n",
    "    processed_data['volatility_5'] = processed_data.groupby('tic')['close'].rolling(5).std().reset_index(0, drop=True)\n",
    "    processed_data['volatility_10'] = processed_data.groupby('tic')['close'].rolling(10).std().reset_index(0, drop=True)\n",
    "    processed_data['volatility_30'] = processed_data.groupby('tic')['close'].rolling(30).std().reset_index(0, drop=True)\n",
    "    \n",
    "    # SOL momentum features (fast-moving)\n",
    "    processed_data['momentum_1'] = processed_data.groupby('tic')['close'].pct_change(1).reset_index(0, drop=True)\n",
    "    processed_data['momentum_5'] = processed_data.groupby('tic')['close'].pct_change(5).reset_index(0, drop=True)\n",
    "    processed_data['momentum_15'] = processed_data.groupby('tic')['close'].pct_change(15).reset_index(0, drop=True)\n",
    "    processed_data['momentum_30'] = processed_data.groupby('tic')['close'].pct_change(30).reset_index(0, drop=True)\n",
    "    \n",
    "    # Volume features (important for SOL DeFi activity)\n",
    "    processed_data['volume_sma_5'] = processed_data.groupby('tic')['volume'].rolling(5).mean().reset_index(0, drop=True)\n",
    "    processed_data['volume_sma_15'] = processed_data.groupby('tic')['volume'].rolling(15).mean().reset_index(0, drop=True)\n",
    "    processed_data['volume_ratio_5'] = processed_data['volume'] / processed_data['volume_sma_5']\n",
    "    processed_data['volume_spike'] = (processed_data['volume'] > processed_data['volume_sma_15'] * 2).astype(int)\n",
    "    \n",
    "    # Price action features (SOL has distinctive patterns)\n",
    "    processed_data['price_range'] = (processed_data['high'] - processed_data['low']) / processed_data['low']\n",
    "    processed_data['body_size'] = abs(processed_data['close'] - processed_data['open']) / processed_data['open']\n",
    "    processed_data['upper_shadow'] = (processed_data['high'] - processed_data[['open', 'close']].max(axis=1)) / processed_data['close']\n",
    "    processed_data['lower_shadow'] = (processed_data[['open', 'close']].min(axis=1) - processed_data['low']) / processed_data['close']\n",
    "    \n",
    "    # Breakout detection (SOL often has strong breakouts)\n",
    "    processed_data['rolling_max_10'] = processed_data.groupby('tic')['high'].rolling(10).max().reset_index(0, drop=True)\n",
    "    processed_data['rolling_min_10'] = processed_data.groupby('tic')['low'].rolling(10).min().reset_index(0, drop=True)\n",
    "    processed_data['breakout_up'] = (processed_data['close'] > processed_data['rolling_max_10'].shift(1)).astype(int)\n",
    "    processed_data['breakdown'] = (processed_data['close'] < processed_data['rolling_min_10'].shift(1)).astype(int)\n",
    "    \n",
    "    # Acceleration features\n",
    "    processed_data['price_acceleration'] = processed_data.groupby('tic')['momentum_5'].diff().reset_index(0, drop=True)\n",
    "    processed_data['volume_acceleration'] = processed_data.groupby('tic')['volume'].diff().reset_index(0, drop=True)\n",
    "    \n",
    "    # Clean data\n",
    "    processed_data = processed_data.dropna().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"üìà SOL Features created. Final shape: {processed_data.shape}\")\n",
    "    print(f\"üîß Feature columns: {len(processed_data.columns)} total\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Create SOL-specific features\n",
    "processed_data = create_full_portfolio_features(raw_data)\n",
    "\n",
    "# Visualize SOL-specific indicators\n",
    "fig, axes = plt.subplots(3, 3, figsize=(22, 18))\n",
    "fig.suptitle('SOL (Solana) Advanced Technical Analysis Dashboard', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Price with breakout signals\n",
    "axes[0,0].plot(processed_data['date'], processed_data['close'], label='SOL Price', linewidth=2)\n",
    "breakout_up = processed_data[processed_data['breakout_up'] == 1]\n",
    "breakdown = processed_data[processed_data['breakdown'] == 1]\n",
    "axes[0,0].scatter(breakout_up['date'], breakout_up['close'], color='green', alpha=0.7, s=20, label='Breakout Up')\n",
    "axes[0,0].scatter(breakdown['date'], breakdown['close'], color='red', alpha=0.7, s=20, label='Breakdown')\n",
    "axes[0,0].set_title('SOL Price with Breakout Signals')\n",
    "axes[0,0].set_ylabel('Price ($)')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# High-frequency volatility\n",
    "axes[0,1].plot(processed_data['date'], processed_data['volatility_5'], label='Vol(5)', alpha=0.8)\n",
    "axes[0,1].plot(processed_data['date'], processed_data['volatility_10'], label='Vol(10)', alpha=0.8)\n",
    "axes[0,1].plot(processed_data['date'], processed_data['volatility_30'], label='Vol(30)', alpha=0.8)\n",
    "axes[0,1].set_title('SOL High-Frequency Volatility')\n",
    "axes[0,1].set_ylabel('Volatility')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Momentum cascade\n",
    "axes[0,2].plot(processed_data['date'], processed_data['momentum_1'], label='Mom(1)', alpha=0.7)\n",
    "axes[0,2].plot(processed_data['date'], processed_data['momentum_5'], label='Mom(5)', alpha=0.7)\n",
    "axes[0,2].plot(processed_data['date'], processed_data['momentum_15'], label='Mom(15)', alpha=0.7)\n",
    "axes[0,2].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[0,2].set_title('SOL Momentum Cascade')\n",
    "axes[0,2].set_ylabel('Momentum')\n",
    "axes[0,2].legend()\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume analysis with spikes\n",
    "volume_normal = processed_data[processed_data['volume_spike'] == 0]\n",
    "volume_spike = processed_data[processed_data['volume_spike'] == 1]\n",
    "axes[1,0].plot(volume_normal['date'], volume_normal['volume'], alpha=0.6, color='blue', label='Normal Volume')\n",
    "axes[1,0].plot(volume_spike['date'], volume_spike['volume'], alpha=0.8, color='red', label='Volume Spikes')\n",
    "axes[1,0].plot(processed_data['date'], processed_data['volume_sma_15'], color='orange', label='SMA(15)')\n",
    "axes[1,0].set_title('SOL Volume Analysis with Spikes')\n",
    "axes[1,0].set_ylabel('Volume')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Price action patterns\n",
    "axes[1,1].plot(processed_data['date'], processed_data['price_range'], label='Price Range', alpha=0.8)\n",
    "axes[1,1].plot(processed_data['date'], processed_data['body_size'], label='Body Size', alpha=0.8)\n",
    "axes[1,1].set_title('SOL Price Action Patterns')\n",
    "axes[1,1].set_ylabel('Relative Size')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# RSI with SOL-specific levels\n",
    "axes[1,2].plot(processed_data['date'], processed_data['rsi_30'], color='purple', linewidth=2)\n",
    "axes[1,2].axhline(y=80, color='r', linestyle='--', alpha=0.7, label='Extreme Overbought')\n",
    "axes[1,2].axhline(y=70, color='orange', linestyle='--', alpha=0.7, label='Overbought')\n",
    "axes[1,2].axhline(y=30, color='lightgreen', linestyle='--', alpha=0.7, label='Oversold')\n",
    "axes[1,2].axhline(y=20, color='g', linestyle='--', alpha=0.7, label='Extreme Oversold')\n",
    "axes[1,2].axhline(y=50, color='gray', linestyle='-', alpha=0.5)\n",
    "axes[1,2].set_title('SOL RSI with Extended Levels')\n",
    "axes[1,2].set_ylabel('RSI')\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "# MACD with signal crossovers\n",
    "axes[2,0].plot(processed_data['date'], processed_data['macd'], label='MACD', color='blue', linewidth=2)\n",
    "axes[2,0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "# Highlight crossovers\n",
    "macd_positive = processed_data['macd'] > 0\n",
    "macd_changes = macd_positive != macd_positive.shift(1)\n",
    "crossovers = processed_data[macd_changes]\n",
    "axes[2,0].scatter(crossovers['date'], crossovers['macd'], color='red', alpha=0.8, s=30, label='Crossovers')\n",
    "axes[2,0].set_title('SOL MACD with Signal Crossovers')\n",
    "axes[2,0].set_ylabel('MACD')\n",
    "axes[2,0].legend()\n",
    "axes[2,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Price acceleration\n",
    "axes[2,1].plot(processed_data['date'], processed_data['price_acceleration'], alpha=0.8, color='darkgreen')\n",
    "axes[2,1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[2,1].fill_between(processed_data['date'], processed_data['price_acceleration'], 0, alpha=0.3, color='green')\n",
    "axes[2,1].set_title('SOL Price Acceleration')\n",
    "axes[2,1].set_ylabel('Acceleration')\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Shadow analysis (important for SOL's price action)\n",
    "axes[2,2].plot(processed_data['date'], processed_data['upper_shadow'], label='Upper Shadow', alpha=0.8)\n",
    "axes[2,2].plot(processed_data['date'], processed_data['lower_shadow'], label='Lower Shadow', alpha=0.8)\n",
    "axes[2,2].set_title('SOL Shadow Analysis')\n",
    "axes[2,2].set_ylabel('Shadow Size (Relative)')\n",
    "axes[2,2].legend()\n",
    "axes[2,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Data Splitting with SOL Market Regime Considerations\n",
    "def create_full_portfolio_temporal_splits(df, train_ratio=0.7, validation_ratio=0.15):\n",
    "    \"\"\"Create temporal splits considering SOL's market cycles and volatility regimes\"\"\"\n",
    "    \n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    \n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + validation_ratio))\n",
    "    \n",
    "    train_data = df.iloc[:train_end].copy()\n",
    "    validation_data = df.iloc[train_end:val_end].copy()\n",
    "    test_data = df.iloc[val_end:].copy()\n",
    "    \n",
    "    # Calculate volatility regimes for each split\n",
    "    def analyze_regime(data, name):\n",
    "        returns = data['close'].pct_change().dropna()\n",
    "        vol = returns.std()\n",
    "        avg_return = returns.mean()\n",
    "        skewness = returns.skew()\n",
    "        kurtosis = returns.kurtosis()\n",
    "        \n",
    "        print(f\"   {name}: Vol={vol:.6f}, Ret={avg_return:.6f}, Skew={skewness:.3f}, Kurt={kurtosis:.3f}\")\n",
    "        return vol, avg_return, skewness, kurtosis\n",
    "    \n",
    "    print(f\"üìä SOL Data Splits with Market Regime Analysis:\")\n",
    "    print(f\"   Training: {len(train_data)} samples ({train_data['date'].min()} to {train_data['date'].max()})\")\n",
    "    print(f\"   Price: ${train_data['close'].min():.2f} - ${train_data['close'].max():.2f}\")\n",
    "    train_vol, train_ret, train_skew, train_kurt = analyze_regime(train_data, \"Train Stats\")\n",
    "    \n",
    "    print(f\"\\n   Validation: {len(validation_data)} samples ({validation_data['date'].min()} to {validation_data['date'].max()})\")\n",
    "    print(f\"   Price: ${validation_data['close'].min():.2f} - ${validation_data['close'].max():.2f}\")\n",
    "    val_vol, val_ret, val_skew, val_kurt = analyze_regime(validation_data, \"Val Stats\")\n",
    "    \n",
    "    print(f\"\\n   Testing: {len(test_data)} samples ({test_data['date'].min()} to {test_data['date'].max()})\")\n",
    "    print(f\"   Price: ${test_data['close'].min():.2f} - ${test_data['close'].max():.2f}\")\n",
    "    test_vol, test_ret, test_skew, test_kurt = analyze_regime(test_data, \"Test Stats\")\n",
    "    \n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "# Create splits\n",
    "train_data, validation_data, test_data = create_full_portfolio_temporal_splits(processed_data)\n",
    "\n",
    "# Visualize splits with SOL-specific context\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "fig.suptitle('SOL Data Splits - Market Regime Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Price evolution across splits\n",
    "axes[0,0].plot(train_data['date'], train_data['close'], label='Training', alpha=0.8, linewidth=2)\n",
    "axes[0,0].plot(validation_data['date'], validation_data['close'], label='Validation', alpha=0.8, linewidth=2)\n",
    "axes[0,0].plot(test_data['date'], test_data['close'], label='Testing', alpha=0.8, linewidth=2)\n",
    "axes[0,0].set_title('SOL Price Evolution Across Splits')\n",
    "axes[0,0].set_xlabel('Date')\n",
    "axes[0,0].set_ylabel('SOL Price ($)')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume patterns\n",
    "axes[0,1].plot(train_data['date'], train_data['volume'], label='Training Volume', alpha=0.6)\n",
    "axes[0,1].plot(validation_data['date'], validation_data['volume'], label='Validation Volume', alpha=0.6)\n",
    "axes[0,1].plot(test_data['date'], test_data['volume'], label='Testing Volume', alpha=0.6)\n",
    "axes[0,1].set_title('SOL Volume Patterns Across Splits')\n",
    "axes[0,1].set_xlabel('Date')\n",
    "axes[0,1].set_ylabel('Volume')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility comparison\n",
    "window = 100\n",
    "train_rolling_vol = train_data['close'].rolling(window).std()\n",
    "val_rolling_vol = validation_data['close'].rolling(window).std()\n",
    "test_rolling_vol = test_data['close'].rolling(window).std()\n",
    "\n",
    "axes[1,0].plot(train_data['date'], train_rolling_vol, label=f'Training Vol({window})', alpha=0.8)\n",
    "axes[1,0].plot(validation_data['date'], val_rolling_vol, label=f'Validation Vol({window})', alpha=0.8)\n",
    "axes[1,0].plot(test_data['date'], test_rolling_vol, label=f'Testing Vol({window})', alpha=0.8)\n",
    "axes[1,0].set_title(f'SOL Rolling Volatility ({window} periods)')\n",
    "axes[1,0].set_xlabel('Date')\n",
    "axes[1,0].set_ylabel('Rolling Volatility')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns distribution comparison\n",
    "train_returns = train_data['close'].pct_change().dropna()\n",
    "val_returns = validation_data['close'].pct_change().dropna()\n",
    "test_returns = test_data['close'].pct_change().dropna()\n",
    "\n",
    "axes[1,1].hist(train_returns, bins=50, alpha=0.6, label='Training', density=True)\n",
    "axes[1,1].hist(val_returns, bins=50, alpha=0.6, label='Validation', density=True)\n",
    "axes[1,1].hist(test_returns, bins=50, alpha=0.6, label='Testing', density=True)\n",
    "axes[1,1].set_title('SOL Returns Distribution by Split')\n",
    "axes[1,1].set_xlabel('Returns')\n",
    "axes[1,1].set_ylabel('Density')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Momentum patterns\n",
    "axes[2,0].plot(train_data['date'], train_data['momentum_5'], label='Training Mom(5)', alpha=0.7)\n",
    "axes[2,0].plot(validation_data['date'], validation_data['momentum_5'], label='Validation Mom(5)', alpha=0.7)\n",
    "axes[2,0].plot(test_data['date'], test_data['momentum_5'], label='Testing Mom(5)', alpha=0.7)\n",
    "axes[2,0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[2,0].set_title('SOL Momentum Patterns Across Splits')\n",
    "axes[2,0].set_xlabel('Date')\n",
    "axes[2,0].set_ylabel('5-Period Momentum')\n",
    "axes[2,0].legend()\n",
    "axes[2,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Summary statistics comparison\n",
    "stats_data = {\n",
    "    'Split': ['Training', 'Validation', 'Testing'],\n",
    "    'Mean Price': [train_data['close'].mean(), validation_data['close'].mean(), test_data['close'].mean()],\n",
    "    'Price Std': [train_data['close'].std(), validation_data['close'].std(), test_data['close'].std()],\n",
    "    'Return Std': [train_returns.std(), val_returns.std(), test_returns.std()]\n",
    "}\n",
    "\n",
    "x = range(len(stats_data['Split']))\n",
    "width = 0.25\n",
    "\n",
    "axes[2,1].bar([i - width for i in x], stats_data['Mean Price'], width, label='Mean Price', alpha=0.8)\n",
    "ax2 = axes[2,1].twinx()\n",
    "ax2.bar([i for i in x], [s*1000 for s in stats_data['Return Std']], width, label='Return Std (√ó1000)', alpha=0.8, color='orange')\n",
    "ax2.bar([i + width for i in x], stats_data['Price Std'], width, label='Price Std', alpha=0.8, color='green')\n",
    "\n",
    "axes[2,1].set_title('SOL Statistical Summary by Split')\n",
    "axes[2,1].set_xlabel('Data Split')\n",
    "axes[2,1].set_ylabel('Mean Price ($)')\n",
    "ax2.set_ylabel('Standard Deviation')\n",
    "axes[2,1].set_xticks(x)\n",
    "axes[2,1].set_xticklabels(stats_data['Split'])\n",
    "axes[2,1].legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: SOL-Optimized Trading Environment and Advanced Hyperparameters\n",
    "def create_full_portfolio_trading_env(data, initial_amount=1000000, transaction_cost_pct=0.001):\n",
    "    \"\"\"Create SOL-optimized trading environment with high-performance settings\"\"\"\n",
    "    \n",
    "    stock_dimension = 1  # Single asset (SOL)\n",
    "    \n",
    "    # Calculate state space correctly\n",
    "    tech_indicators = ['macd', 'rsi_30', 'cci_30', 'dx_30']\n",
    "    state_space = 1 + stock_dimension + stock_dimension + len(tech_indicators)\n",
    "    \n",
    "    env = StockTradingEnv(\n",
    "        df=data,\n",
    "        stock_dim=stock_dimension,\n",
    "        hmax=150,  # SOL-appropriate max shares\n",
    "        initial_amount=initial_amount,\n",
    "        num_stock_shares=[0],\n",
    "        buy_cost_pct=[transaction_cost_pct],\n",
    "        sell_cost_pct=[transaction_cost_pct],\n",
    "        reward_scaling=1e-4,  # Adjusted for SOL's price range\n",
    "        state_space=state_space,\n",
    "        action_space=stock_dimension,\n",
    "        tech_indicator_list=tech_indicators,\n",
    "        print_verbosity=0\n",
    "    )\n",
    "    \n",
    "    return env\n",
    "\n",
    "def optimize_full_portfolio_hyperparameters(train_data, validation_data, n_trials=18):\n",
    "    \"\"\"Optimize PPO hyperparameters specifically for SOL's high-volatility trading\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        # SOL-specific hyperparameter ranges (tuned for high volatility)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-2, log=True)\n",
    "        n_steps = trial.suggest_int('n_steps', 256, 2048, step=128)\n",
    "        batch_size = trial.suggest_int('batch_size', 8, 64, step=8)\n",
    "        n_epochs = trial.suggest_int('n_epochs', 3, 20)\n",
    "        gamma = trial.suggest_float('gamma', 0.9, 0.9999)\n",
    "        clip_range = trial.suggest_float('clip_range', 0.05, 0.5)\n",
    "        ent_coef = trial.suggest_float('ent_coef', 1e-8, 1e-1, log=True)\n",
    "        vf_coef = trial.suggest_float('vf_coef', 0.1, 1.0)\n",
    "        max_grad_norm = trial.suggest_float('max_grad_norm', 0.1, 2.0)\n",
    "        \n",
    "        try:\n",
    "            # Create environment\n",
    "            env_train = create_full_portfolio_trading_env(train_data)\n",
    "            env_train = DummyVecEnv([lambda: env_train])\n",
    "            \n",
    "            # Create model with suggested hyperparameters\n",
    "            model = PPO(\n",
    "                'MlpPolicy',\n",
    "                env_train,\n",
    "                learning_rate=learning_rate,\n",
    "                n_steps=n_steps,\n",
    "                batch_size=batch_size,\n",
    "                n_epochs=n_epochs,\n",
    "                gamma=gamma,\n",
    "                clip_range=clip_range,\n",
    "                ent_coef=ent_coef,\n",
    "                vf_coef=vf_coef,\n",
    "                max_grad_norm=max_grad_norm,\n",
    "                verbose=0,\n",
    "                device='mps',\n",
    "                policy_kwargs=dict(\n",
    "                    net_arch=[128, 128],  # Larger network for SOL complexity\n",
    "                    activation_fn=torch.nn.ReLU\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Train for short period\n",
    "            model.learn(total_timesteps=10000)  # Longer for SOL complexity\n",
    "            \n",
    "            # Evaluate on validation data\n",
    "            env_val = create_full_portfolio_trading_env(validation_data)\n",
    "            env_val = DummyVecEnv([lambda: env_val])\n",
    "            \n",
    "            obs = env_val.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            portfolio_values = []\n",
    "            \n",
    "            while not done and steps < 3000:  # Limit steps\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, info = env_val.step(action)\n",
    "                total_reward += reward[0]\n",
    "                portfolio_values.append(info['total_asset'])\n",
    "                steps += 1\n",
    "            \n",
    "            # Calculate additional metrics for SOL\n",
    "            if len(portfolio_values) > 1:\n",
    "                returns = pd.Series(portfolio_values).pct_change().dropna()\n",
    "                sharpe = returns.mean() / returns.std() if returns.std() > 0 else 0\n",
    "                total_reward += sharpe * 1000  # Bonus for good risk-adjusted returns\n",
    "            \n",
    "            return total_reward\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return -1e6\n",
    "    \n",
    "    # Run optimization with progress tracking\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"üéØ Best SOL hyperparameters found:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    print(f\"   Best validation score: {study.best_value:.4f}\")\n",
    "    \n",
    "    # Display optimization insights\n",
    "    print(f\"\\nüìä Optimization Insights:\")\n",
    "    print(f\"   Total trials: {len(study.trials)}\")\n",
    "    print(f\"   Best trial number: {study.best_trial.number}\")\n",
    "    print(f\"   Optimization took: {sum(t.duration.total_seconds() for t in study.trials if t.duration):.1f} seconds\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# Run hyperparameter optimization for SOL\n",
    "print(\"üîç Starting SOL-specific hyperparameter optimization...\")\n",
    "print(\"üí° Optimizing for high-volatility, high-frequency trading patterns\")\n",
    "full_portfolio_best_params = optimize_full_portfolio_hyperparameters(train_data, validation_data, n_trials=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: SOL Model Training with Performance Monitoring\n",
    "def train_full_portfolio_model(train_data, best_params, timesteps=150000):\n",
    "    \"\"\"Train the SOL model with optimized hyperparameters and performance monitoring\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Training SOL model with {timesteps} timesteps...\")\n",
    "    print(f\"üéØ Using optimized hyperparameters for high-performance trading\")\n",
    "    \n",
    "    # Create training environment\n",
    "    env_train = create_full_portfolio_trading_env(train_data)\n",
    "    env_train = DummyVecEnv([lambda: env_train])\n",
    "    \n",
    "    # Create validation environment for monitoring\n",
    "    env_val = create_full_portfolio_trading_env(validation_data)\n",
    "    env_val = DummyVecEnv([lambda: env_val])\n",
    "    \n",
    "    # Create model with best parameters\n",
    "    model = PPO(\n",
    "        'MlpPolicy',\n",
    "        env_train,\n",
    "        learning_rate=best_params.get('learning_rate', 3e-4),\n",
    "        n_steps=best_params.get('n_steps', 1024),\n",
    "        batch_size=best_params.get('batch_size', 32),\n",
    "        n_epochs=best_params.get('n_epochs', 10),\n",
    "        gamma=best_params.get('gamma', 0.995),\n",
    "        clip_range=best_params.get('clip_range', 0.2),\n",
    "        ent_coef=best_params.get('ent_coef', 1e-4),\n",
    "        vf_coef=best_params.get('vf_coef', 0.5),\n",
    "        max_grad_norm=best_params.get('max_grad_norm', 0.5),\n",
    "        verbose=1,\n",
    "        device='mps',\n",
    "        tensorboard_log=\"./full_portfolio_ppo_tensorboard/\",\n",
    "        policy_kwargs=dict(\n",
    "            net_arch=[128, 128, 64],  # 3-layer network for SOL complexity\n",
    "            activation_fn=torch.nn.ReLU,\n",
    "            ortho_init=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Setup evaluation callback\n",
    "    eval_callback = EvalCallback(\n",
    "        env_val,\n",
    "        best_model_save_path='./full_portfolio_ppo_best/',\n",
    "        log_path='./full_portfolio_ppo_logs/',\n",
    "        eval_freq=10000,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose=1,\n",
    "        n_eval_episodes=5\n",
    "    )\n",
    "    \n",
    "    # Train the model with monitoring\n",
    "    start_time = datetime.now()\n",
    "    model.learn(\n",
    "        total_timesteps=timesteps,\n",
    "        callback=eval_callback,\n",
    "        tb_log_name=\"full_portfolio_ppo_training\"\n",
    "    )\n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è SOL training completed in {training_time}\")\n",
    "    print(f\"üìä Training performance logged to tensorboard\")\n",
    "    \n",
    "    # Save the final model\n",
    "    model.save(\"full_portfolio_ppo_model\")\n",
    "    print(f\"üíæ SOL model saved as full_portfolio_ppo_model.zip\")\n",
    "    \n",
    "    # Load best model from callback if available\n",
    "    try:\n",
    "        best_model = PPO.load('./full_portfolio_ppo_best/best_model')\n",
    "        print(f\"‚úÖ Loaded best performing model from validation\")\n",
    "        return best_model\n",
    "    except:\n",
    "        print(f\"‚ÑπÔ∏è Using final trained model\")\n",
    "        return model\n",
    "\n",
    "# Train the SOL model\n",
    "full_portfolio_trained_model = train_full_portfolio_model(train_data, full_portfolio_best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: SOL Model Evaluation and High-Performance Analysis\n",
    "def evaluate_full_portfolio_model(model, test_data, model_name=\"SOL_PPO\"):\n",
    "    \"\"\"Comprehensive SOL model evaluation with high-frequency trading focus\"\"\"\n",
    "    \n",
    "    print(f\"üìä Evaluating {model_name} model on SOL test data...\")\n",
    "    print(f\"üéØ Focus: High-frequency, high-volatility performance metrics\")\n",
    "    \n",
    "    # Create test environment\n",
    "    env_test = create_full_portfolio_trading_env(test_data)\n",
    "    \n",
    "    # Run backtest\n",
    "    obs = env_test.reset()\n",
    "    actions_list = []\n",
    "    rewards_list = []\n",
    "    portfolio_values = [env_test.initial_amount]\n",
    "    positions = []\n",
    "    trade_profits = []\n",
    "    \n",
    "    step_count = 0\n",
    "    last_position = 0\n",
    "    entry_price = 0\n",
    "    \n",
    "    while True:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env_test.step(action)\n",
    "        \n",
    "        actions_list.append(action[0])\n",
    "        rewards_list.append(reward)\n",
    "        portfolio_values.append(info['total_asset'])\n",
    "        current_position = info.get('holdings', [0])[0]\n",
    "        positions.append(current_position)\n",
    "        \n",
    "        # Track individual trades for SOL\n",
    "        if current_position != last_position:\n",
    "            if last_position == 0 and current_position != 0:  # Entry\n",
    "                entry_price = test_data['close'].iloc[step_count] if step_count < len(test_data) else 0\n",
    "            elif last_position != 0 and current_position == 0:  # Exit\n",
    "                exit_price = test_data['close'].iloc[step_count] if step_count < len(test_data) else entry_price\n",
    "                if entry_price > 0:\n",
    "                    profit = (exit_price - entry_price) / entry_price * (1 if last_position > 0 else -1)\n",
    "                    trade_profits.append(profit)\n",
    "            last_position = current_position\n",
    "        \n",
    "        step_count += 1\n",
    "        if done or step_count >= len(test_data):\n",
    "            break\n",
    "    \n",
    "    # Calculate SOL-specific performance metrics\n",
    "    returns = pd.Series(portfolio_values).pct_change().dropna()\n",
    "    \n",
    "    # Buy and hold baseline for SOL\n",
    "    initial_price = test_data['close'].iloc[0]\n",
    "    final_price = test_data['close'].iloc[-1]\n",
    "    buy_hold_return = (final_price / initial_price) - 1\n",
    "    \n",
    "    # RL model performance\n",
    "    rl_return = (portfolio_values[-1] / portfolio_values[0]) - 1\n",
    "    \n",
    "    # Risk metrics (adjusted for 5-minute SOL data)\n",
    "    periods_per_year = 365 * 24 * 12  # 5-minute periods\n",
    "    volatility = returns.std() * np.sqrt(periods_per_year)\n",
    "    sharpe_ratio = (returns.mean() * periods_per_year) / volatility if volatility != 0 else 0\n",
    "    \n",
    "    # Drawdown analysis\n",
    "    portfolio_series = pd.Series(portfolio_values)\n",
    "    rolling_max = portfolio_series.cummax()\n",
    "    drawdown = (portfolio_series / rolling_max - 1)\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    # High-frequency trading metrics\n",
    "    downside_returns = returns[returns < 0]\n",
    "    sortino_ratio = (returns.mean() * periods_per_year) / (downside_returns.std() * np.sqrt(periods_per_year)) if len(downside_returns) > 0 else 0\n",
    "    \n",
    "    # SOL-specific metrics\n",
    "    avg_position_size = np.mean(np.abs(positions))\n",
    "    position_changes = sum(1 for i in range(1, len(positions)) if positions[i] != positions[i-1])\n",
    "    avg_trade_profit = np.mean(trade_profits) if trade_profits else 0\n",
    "    win_rate_trades = len([p for p in trade_profits if p > 0]) / len(trade_profits) if trade_profits else 0\n",
    "    \n",
    "    # Ulcer Index (specific for high-vol assets like SOL)\n",
    "    ulcer_index = np.sqrt(np.mean(drawdown**2))\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'cryptocurrency': 'SOL',\n",
    "        'rl_total_return': rl_return,\n",
    "        'buy_hold_return': buy_hold_return,\n",
    "        'excess_return': rl_return - buy_hold_return,\n",
    "        'volatility': volatility,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'sortino_ratio': sortino_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'ulcer_index': ulcer_index,\n",
    "        'final_portfolio_value': portfolio_values[-1],\n",
    "        'total_trades': len([a for a in actions_list if a != 0]),\n",
    "        'win_rate': len([r for r in rewards_list if r > 0]) / len(rewards_list),\n",
    "        'avg_position_size': avg_position_size,\n",
    "        'position_changes': position_changes,\n",
    "        'avg_trade_profit': avg_trade_profit,\n",
    "        'trade_win_rate': win_rate_trades,\n",
    "        'total_completed_trades': len(trade_profits),\n",
    "        'calmar_ratio': rl_return / abs(max_drawdown) if max_drawdown != 0 else 0\n",
    "    }\n",
    "    \n",
    "    return results, portfolio_values, actions_list, positions, trade_profits\n",
    "\n",
    "# Evaluate the trained SOL model\n",
    "full_portfolio_results, full_portfolio_portfolio_values, full_portfolio_actions, full_portfolio_positions, full_portfolio_trade_profits = evaluate_full_portfolio_model(full_portfolio_trained_model, test_data)\n",
    "\n",
    "# Display SOL results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî• SOL (SOLANA) HIGH-PERFORMANCE TRADING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üéØ Performance Metrics:\")\n",
    "print(f\"   RL Total Return: {full_portfolio_results['rl_total_return']:.4f} ({full_portfolio_results['rl_total_return']*100:.2f}%)\")\n",
    "print(f\"   Buy & Hold Return: {full_portfolio_results['buy_hold_return']:.4f} ({full_portfolio_results['buy_hold_return']*100:.2f}%)\")\n",
    "print(f\"   Excess Return: {full_portfolio_results['excess_return']:.4f} ({full_portfolio_results['excess_return']*100:.2f}%)\")\n",
    "print(f\"\\nüìä Risk-Adjusted Metrics:\")\n",
    "print(f\"   Sharpe Ratio: {full_portfolio_results['sharpe_ratio']:.4f}\")\n",
    "print(f\"   Sortino Ratio: {full_portfolio_results['sortino_ratio']:.4f}\")\n",
    "print(f\"   Calmar Ratio: {full_portfolio_results['calmar_ratio']:.4f}\")\n",
    "print(f\"   Max Drawdown: {full_portfolio_results['max_drawdown']:.4f} ({full_portfolio_results['max_drawdown']*100:.2f}%)\")\n",
    "print(f\"   Ulcer Index: {full_portfolio_results['ulcer_index']:.4f}\")\n",
    "print(f\"   Volatility: {full_portfolio_results['volatility']:.4f}\")\n",
    "print(f\"\\nüéÆ Trading Activity:\")\n",
    "print(f\"   Total Trades: {full_portfolio_results['total_trades']}\")\n",
    "print(f\"   Completed Trades: {full_portfolio_results['total_completed_trades']}\")\n",
    "print(f\"   Position Changes: {full_portfolio_results['position_changes']}\")\n",
    "print(f\"   Win Rate (Rewards): {full_portfolio_results['win_rate']:.4f} ({full_portfolio_results['win_rate']*100:.2f}%)\")\n",
    "print(f\"   Trade Win Rate: {full_portfolio_results['trade_win_rate']:.4f} ({full_portfolio_results['trade_win_rate']*100:.2f}%)\")\n",
    "print(f\"   Average Trade Profit: {full_portfolio_results['avg_trade_profit']:.6f} ({full_portfolio_results['avg_trade_profit']*100:.4f}%)\")\n",
    "print(f\"   Average Position Size: {full_portfolio_results['avg_position_size']:.2f} SOL\")\n",
    "print(f\"\\nüí∞ Portfolio:\")\n",
    "print(f\"   Final Portfolio Value: ${full_portfolio_results['final_portfolio_value']:,.2f}\")\n",
    "print(f\"   Initial Investment: $1,000,000.00\")\n",
    "print(f\"   Net Profit/Loss: ${full_portfolio_results['final_portfolio_value'] - 1000000:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: SOL Advanced Visualization Dashboard\n",
    "def create_full_portfolio_advanced_dashboard(test_data, portfolio_values, actions, positions, trade_profits):\n",
    "    \"\"\"Create advanced SOL analysis dashboard with high-frequency trading focus\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(24, 20))\n",
    "    gs = fig.add_gridspec(4, 3, height_ratios=[1, 1, 1, 1], width_ratios=[2, 1, 1])\n",
    "    fig.suptitle('SOL (Solana) Advanced High-Performance Trading Dashboard', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # 1. Main portfolio performance chart\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1_twin = ax1.twinx()\n",
    "    \n",
    "    # Portfolio vs SOL price\n",
    "    ax1.plot(test_data['date'], portfolio_values, label='RL Portfolio', linewidth=4, color='gold', alpha=0.9)\n",
    "    buy_hold_normalized = (test_data['close'] / test_data['close'].iloc[0]) * portfolio_values[0]\n",
    "    ax1.plot(test_data['date'], buy_hold_normalized, label='Buy & Hold', linewidth=3, alpha=0.8, color='cyan')\n",
    "    ax1_twin.plot(test_data['date'], test_data['close'], label='SOL Price', alpha=0.5, color='purple', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Highlight major trades\n",
    "    position_changes = []\n",
    "    for i in range(1, len(positions)):\n",
    "        if positions[i] != positions[i-1]:\n",
    "            position_changes.append(i)\n",
    "    \n",
    "    for idx in position_changes[:20]:  # Show first 20 major trades\n",
    "        if idx < len(test_data):\n",
    "            color = 'green' if positions[idx] > positions[idx-1] else 'red'\n",
    "            ax1.axvline(x=test_data['date'].iloc[idx], color=color, alpha=0.3, linestyle='-', linewidth=1)\n",
    "    \n",
    "    ax1.set_title('SOL Portfolio Performance with Major Trade Signals', fontweight='bold', fontsize=16)\n",
    "    ax1.set_ylabel('Portfolio Value ($)', color='gold', fontweight='bold')\n",
    "    ax1_twin.set_ylabel('SOL Price ($)', color='purple', fontweight='bold')\n",
    "    ax1.legend(loc='upper left', fontsize=12)\n",
    "    ax1_twin.legend(loc='upper right', fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. High-frequency returns analysis\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    returns = pd.Series(portfolio_values).pct_change().dropna()\n",
    "    full_portfolio_returns = test_data['close'].pct_change().dropna()\n",
    "    \n",
    "    # Rolling correlation\n",
    "    window = 144  # 12 hours\n",
    "    rolling_corr = returns.rolling(window).corr(full_portfolio_returns.iloc[:len(returns)])\n",
    "    \n",
    "    ax2.plot(test_data['date'].iloc[window:], rolling_corr.dropna(), linewidth=2, color='orange')\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax2.axhline(y=0.5, color='green', linestyle='--', alpha=0.7, label='Moderate Correlation')\n",
    "    ax2.axhline(y=-0.5, color='red', linestyle='--', alpha=0.7, label='Negative Correlation')\n",
    "    ax2.set_title(f'Rolling Correlation with SOL ({window} periods)', fontweight='bold')\n",
    "    ax2.set_ylabel('Correlation')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Trade profit distribution\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    if trade_profits:\n",
    "        ax3.hist(trade_profits, bins=30, alpha=0.7, edgecolor='black', color='lightgreen')\n",
    "        ax3.axvline(np.mean(trade_profits), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(trade_profits):.4f}')\n",
    "        ax3.axvline(np.median(trade_profits), color='blue', linestyle='--', linewidth=2, label=f'Median: {np.median(trade_profits):.4f}')\n",
    "        ax3.set_title('SOL Trade Profit Distribution', fontweight='bold')\n",
    "        ax3.set_xlabel('Profit per Trade')\n",
    "        ax3.set_ylabel('Frequency')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Volatility regimes\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    vol_window = 72  # 6 hours\n",
    "    rolling_vol = returns.rolling(vol_window).std()\n",
    "    \n",
    "    # Define volatility regimes\n",
    "    vol_low = rolling_vol.quantile(0.33)\n",
    "    vol_high = rolling_vol.quantile(0.67)\n",
    "    \n",
    "    low_vol = rolling_vol <= vol_low\n",
    "    med_vol = (rolling_vol > vol_low) & (rolling_vol <= vol_high)\n",
    "    high_vol = rolling_vol > vol_high\n",
    "    \n",
    "    colors = ['green', 'orange', 'red']\n",
    "    labels = ['Low Vol', 'Med Vol', 'High Vol']\n",
    "    sizes = [low_vol.sum(), med_vol.sum(), high_vol.sum()]\n",
    "    \n",
    "    ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax4.set_title('SOL Volatility Regimes', fontweight='bold')\n",
    "    \n",
    "    # 5. Position size over time\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    ax5.plot(test_data['date'], positions, linewidth=2, color='purple', alpha=0.8)\n",
    "    ax5.fill_between(test_data['date'], positions, 0, alpha=0.3, color='purple')\n",
    "    ax5.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    # Add position statistics\n",
    "    avg_long = np.mean([p for p in positions if p > 0]) if any(p > 0 for p in positions) else 0\n",
    "    avg_short = np.mean([p for p in positions if p < 0]) if any(p < 0 for p in positions) else 0\n",
    "    \n",
    "    if avg_long > 0:\n",
    "        ax5.axhline(y=avg_long, color='green', linestyle='--', alpha=0.7, label=f'Avg Long: {avg_long:.1f}')\n",
    "    if avg_short < 0:\n",
    "        ax5.axhline(y=avg_short, color='red', linestyle='--', alpha=0.7, label=f'Avg Short: {avg_short:.1f}')\n",
    "    \n",
    "    ax5.set_title('SOL Position Size Evolution', fontweight='bold')\n",
    "    ax5.set_ylabel('SOL Holdings')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Action frequency heatmap\n",
    "    ax6 = fig.add_subplot(gs[2, 1])\n",
    "    \n",
    "    # Create time-based action analysis\n",
    "    df_actions = pd.DataFrame({\n",
    "        'date': test_data['date'][:len(actions)],\n",
    "        'action': actions,\n",
    "        'hour': test_data['date'][:len(actions)].dt.hour,\n",
    "        'day': test_data['date'][:len(actions)].dt.day_name()\n",
    "    })\n",
    "    \n",
    "    action_by_hour = df_actions.groupby(['hour', 'action']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Normalize by total actions per hour\n",
    "    action_by_hour_norm = action_by_hour.div(action_by_hour.sum(axis=1), axis=0)\n",
    "    \n",
    "    import seaborn as sns\n",
    "    sns.heatmap(action_by_hour_norm.T, annot=True, cmap='coolwarm', center=0.5, ax=ax6)\n",
    "    ax6.set_title('SOL Action Patterns by Hour', fontweight='bold')\n",
    "    ax6.set_xlabel('Hour of Day')\n",
    "    ax6.set_ylabel('Action')\n",
    "    \n",
    "    # 7. Risk metrics over time\n",
    "    ax7 = fig.add_subplot(gs[2, 2])\n",
    "    \n",
    "    # Rolling Sharpe and Sortino ratios\n",
    "    roll_window = 144\n",
    "    rolling_sharpe = returns.rolling(roll_window).mean() / returns.rolling(roll_window).std() * np.sqrt(365*24*12)\n",
    "    \n",
    "    downside_mask = returns < 0\n",
    "    downside_returns = returns.copy()\n",
    "    downside_returns[~downside_mask] = 0\n",
    "    rolling_sortino = returns.rolling(roll_window).mean() / downside_returns.rolling(roll_window).std() * np.sqrt(365*24*12)\n",
    "    \n",
    "    ax7.plot(test_data['date'].iloc[roll_window:], rolling_sharpe.dropna(), label='Rolling Sharpe', linewidth=2)\n",
    "    ax7.plot(test_data['date'].iloc[roll_window:], rolling_sortino.dropna(), label='Rolling Sortino', linewidth=2)\n",
    "    ax7.axhline(y=1, color='green', linestyle='--', alpha=0.7, label='Ratio = 1')\n",
    "    ax7.axhline(y=2, color='darkgreen', linestyle='--', alpha=0.7, label='Ratio = 2')\n",
    "    ax7.set_title(f'Rolling Risk Ratios ({roll_window}p)', fontweight='bold')\n",
    "    ax7.set_ylabel('Ratio')\n",
    "    ax7.legend()\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. Drawdown analysis\n",
    "    ax8 = fig.add_subplot(gs[3, :])\n",
    "    \n",
    "    # Calculate drawdowns\n",
    "    portfolio_series = pd.Series(portfolio_values)\n",
    "    rolling_max = portfolio_series.cummax()\n",
    "    drawdown = (portfolio_series / rolling_max - 1) * 100\n",
    "    \n",
    "    # SOL price drawdowns for comparison\n",
    "    full_portfolio_rolling_max = test_data['close'].cummax()\n",
    "    full_portfolio_drawdown = (test_data['close'] / full_portfolio_rolling_max - 1) * 100\n",
    "    \n",
    "    ax8.fill_between(test_data['date'], drawdown, 0, alpha=0.4, color='red', label='Portfolio DD')\n",
    "    ax8.fill_between(test_data['date'], full_portfolio_drawdown, 0, alpha=0.3, color='blue', label='SOL DD')\n",
    "    ax8.plot(test_data['date'], drawdown, color='darkred', linewidth=1)\n",
    "    ax8.plot(test_data['date'], full_portfolio_drawdown, color='darkblue', linewidth=1)\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax8.axhline(y=-5, color='orange', linestyle='--', alpha=0.7, label='5% DD')\n",
    "    ax8.axhline(y=-10, color='red', linestyle='--', alpha=0.7, label='10% DD')\n",
    "    ax8.axhline(y=-20, color='darkred', linestyle='--', alpha=0.7, label='20% DD')\n",
    "    \n",
    "    ax8.set_title('SOL Drawdown Comparison: Portfolio vs Buy&Hold', fontweight='bold', fontsize=16)\n",
    "    ax8.set_xlabel('Date', fontweight='bold')\n",
    "    ax8.set_ylabel('Drawdown (%)', fontweight='bold')\n",
    "    ax8.legend(fontsize=12)\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create SOL advanced dashboard\n",
    "create_full_portfolio_advanced_dashboard(test_data, full_portfolio_portfolio_values, full_portfolio_actions, full_portfolio_positions, full_portfolio_trade_profits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: SOL Statistical Analysis and Significance Testing\n",
    "def full_portfolio_comprehensive_statistical_analysis(portfolio_values, test_data, trade_profits):\n",
    "    \"\"\"Perform comprehensive statistical analysis for SOL trading results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä SOL COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate returns\n",
    "    rl_returns = pd.Series(portfolio_values).pct_change().dropna()\n",
    "    full_portfolio_returns = test_data['close'].pct_change().dropna()\n",
    "    \n",
    "    # Ensure same length\n",
    "    min_len = min(len(rl_returns), len(full_portfolio_returns))\n",
    "    rl_returns = rl_returns.iloc[:min_len]\n",
    "    full_portfolio_returns = full_portfolio_returns.iloc[:min_len]\n",
    "    \n",
    "    # 1. Distribution Analysis\n",
    "    print(f\"\\nüìà DISTRIBUTION ANALYSIS:\")\n",
    "    print(f\"   Sample Size: {len(rl_returns):,} observations\")\n",
    "    print(f\"   RL Returns - Mean: {rl_returns.mean():.6f}, Std: {rl_returns.std():.6f}\")\n",
    "    print(f\"   SOL Returns - Mean: {full_portfolio_returns.mean():.6f}, Std: {full_portfolio_returns.std():.6f}\")\n",
    "    print(f\"   RL Skewness: {rl_returns.skew():.4f}, Kurtosis: {rl_returns.kurtosis():.4f}\")\n",
    "    print(f\"   SOL Skewness: {full_portfolio_returns.skew():.4f}, Kurtosis: {full_portfolio_returns.kurtosis():.4f}\")\n",
    "    \n",
    "    # 2. Normality tests\n",
    "    sample_size = min(5000, len(rl_returns))\n",
    "    rl_shapiro = stats.shapiro(rl_returns.iloc[:sample_size])\n",
    "    full_portfolio_shapiro = stats.shapiro(full_portfolio_returns.iloc[:sample_size])\n",
    "    rl_jarque_bera = stats.jarque_bera(rl_returns)\n",
    "    full_portfolio_jarque_bera = stats.jarque_bera(full_portfolio_returns)\n",
    "    \n",
    "    print(f\"\\nüîç NORMALITY TESTS:\")\n",
    "    print(f\"   RL Shapiro-Wilk: W = {rl_shapiro[0]:.4f}, p = {rl_shapiro[1]:.6f} {'(Normal)' if rl_shapiro[1] > 0.05 else '(Non-normal)'}\")\n",
    "    print(f\"   SOL Shapiro-Wilk: W = {full_portfolio_shapiro[0]:.4f}, p = {full_portfolio_shapiro[1]:.6f} {'(Normal)' if full_portfolio_shapiro[1] > 0.05 else '(Non-normal)'}\")\n",
    "    print(f\"   RL Jarque-Bera: JB = {rl_jarque_bera[0]:.4f}, p = {rl_jarque_bera[1]:.6f}\")\n",
    "    print(f\"   SOL Jarque-Bera: JB = {full_portfolio_jarque_bera[0]:.4f}, p = {full_portfolio_jarque_bera[1]:.6f}\")\n",
    "    \n",
    "    # 3. Statistical significance tests\n",
    "    excess_returns = rl_returns - full_portfolio_returns\n",
    "    t_stat, t_pvalue = stats.ttest_1samp(excess_returns, 0)\n",
    "    wilcoxon_stat, wilcoxon_pvalue = stats.wilcoxon(excess_returns, alternative='two-sided')\n",
    "    \n",
    "    print(f\"\\nüìä SIGNIFICANCE TESTS:\")\n",
    "    print(f\"   Paired t-test: t = {t_stat:.4f}, p = {t_pvalue:.6f}\")\n",
    "    print(f\"   Wilcoxon signed-rank: W = {wilcoxon_stat:.4f}, p = {wilcoxon_pvalue:.6f}\")\n",
    "    \n",
    "    significance_level = 0.05\n",
    "    is_significant = t_pvalue < significance_level\n",
    "    direction = \"outperforms\" if t_stat > 0 else \"underperforms\"\n",
    "    \n",
    "    print(f\"   Result: {'‚úÖ Significant' if is_significant else '‚ùå Not Significant'} {direction} (Œ± = {significance_level})\")\n",
    "    \n",
    "    # 4. Effect size analysis\n",
    "    cohens_d = excess_returns.mean() / excess_returns.std()\n",
    "    \n",
    "    effect_size_interpretation = {\n",
    "        (lambda x: abs(x) >= 0.8): \"Large effect\",\n",
    "        (lambda x: abs(x) >= 0.5): \"Medium effect\", \n",
    "        (lambda x: abs(x) >= 0.2): \"Small effect\",\n",
    "        (lambda x: True): \"Negligible effect\"\n",
    "    }\n",
    "    \n",
    "    effect_interpretation = next(v for k, v in effect_size_interpretation.items() if k(cohens_d))\n",
    "    \n",
    "    print(f\"\\nüìè EFFECT SIZE ANALYSIS:\")\n",
    "    print(f\"   Cohen's d: {cohens_d:.4f} ({effect_interpretation})\")\n",
    "    \n",
    "    # 5. Confidence intervals\n",
    "    confidence_levels = [0.90, 0.95, 0.99]\n",
    "    n = len(excess_returns)\n",
    "    mean_excess = excess_returns.mean()\n",
    "    se_excess = excess_returns.std() / np.sqrt(n)\n",
    "    \n",
    "    print(f\"\\nüìä CONFIDENCE INTERVALS:\")\n",
    "    for conf_level in confidence_levels:\n",
    "        alpha = 1 - conf_level\n",
    "        t_critical = stats.t.ppf(1 - alpha/2, n-1)\n",
    "        ci_lower = mean_excess - t_critical * se_excess\n",
    "        ci_upper = mean_excess + t_critical * se_excess\n",
    "        \n",
    "        contains_zero = ci_lower <= 0 <= ci_upper\n",
    "        print(f\"   {conf_level*100}% CI: [{ci_lower:.6f}, {ci_upper:.6f}] {'‚ö†Ô∏è Contains zero' if contains_zero else '‚úÖ Excludes zero'}\")\n",
    "    \n",
    "    # 6. Advanced risk-adjusted metrics\n",
    "    periods_per_year = 365 * 24 * 12\n",
    "    \n",
    "    # Portfolio metrics\n",
    "    portfolio_mean = rl_returns.mean() * periods_per_year\n",
    "    portfolio_vol = rl_returns.std() * np.sqrt(periods_per_year)\n",
    "    portfolio_sharpe = portfolio_mean / portfolio_vol if portfolio_vol != 0 else 0\n",
    "    \n",
    "    # Benchmark metrics\n",
    "    benchmark_mean = full_portfolio_returns.mean() * periods_per_year\n",
    "    benchmark_vol = full_portfolio_returns.std() * np.sqrt(periods_per_year)\n",
    "    benchmark_sharpe = benchmark_mean / benchmark_vol if benchmark_vol != 0 else 0\n",
    "    \n",
    "    # Information Ratio\n",
    "    excess_mean = excess_returns.mean() * periods_per_year\n",
    "    tracking_error = excess_returns.std() * np.sqrt(periods_per_year)\n",
    "    info_ratio = excess_mean / tracking_error if tracking_error != 0 else 0\n",
    "    \n",
    "    # Beta and Alpha\n",
    "    covariance = np.cov(rl_returns, full_portfolio_returns)[0,1]\n",
    "    full_portfolio_variance = full_portfolio_returns.var()\n",
    "    beta = covariance / full_portfolio_variance if full_portfolio_variance != 0 else 1\n",
    "    alpha = portfolio_mean - beta * benchmark_mean\n",
    "    \n",
    "    print(f\"\\nüéØ RISK-ADJUSTED METRICS:\")\n",
    "    print(f\"   Portfolio Sharpe: {portfolio_sharpe:.4f}\")\n",
    "    print(f\"   Benchmark Sharpe: {benchmark_sharpe:.4f}\")\n",
    "    print(f\"   Information Ratio: {info_ratio:.4f}\")\n",
    "    print(f\"   Tracking Error: {tracking_error:.4f}\")\n",
    "    print(f\"   Beta: {beta:.4f}\")\n",
    "    print(f\"   Alpha (annualized): {alpha:.4f}\")\n",
    "    \n",
    "    # 7. Trade-level analysis\n",
    "    if trade_profits:\n",
    "        trade_stats = pd.Series(trade_profits)\n",
    "        win_trades = trade_stats[trade_stats > 0]\n",
    "        loss_trades = trade_stats[trade_stats < 0]\n",
    "        \n",
    "        print(f\"\\nüéÆ TRADE-LEVEL ANALYSIS:\")\n",
    "        print(f\"   Total Trades: {len(trade_profits)}\")\n",
    "        print(f\"   Winning Trades: {len(win_trades)} ({len(win_trades)/len(trade_profits)*100:.1f}%)\")\n",
    "        print(f\"   Losing Trades: {len(loss_trades)} ({len(loss_trades)/len(trade_profits)*100:.1f}%)\")\n",
    "        \n",
    "        if len(win_trades) > 0:\n",
    "            print(f\"   Average Win: {win_trades.mean():.6f} ({win_trades.mean()*100:.4f}%)\")\n",
    "            print(f\"   Max Win: {win_trades.max():.6f} ({win_trades.max()*100:.4f}%)\")\n",
    "        \n",
    "        if len(loss_trades) > 0:\n",
    "            print(f\"   Average Loss: {loss_trades.mean():.6f} ({loss_trades.mean()*100:.4f}%)\")\n",
    "            print(f\"   Max Loss: {loss_trades.min():.6f} ({loss_trades.min()*100:.4f}%)\")\n",
    "            \n",
    "            if len(win_trades) > 0:\n",
    "                profit_factor = abs(win_trades.sum() / loss_trades.sum())\n",
    "                expectancy = (len(win_trades)/len(trade_profits) * win_trades.mean()) + (len(loss_trades)/len(trade_profits) * loss_trades.mean())\n",
    "                print(f\"   Profit Factor: {profit_factor:.4f}\")\n",
    "                print(f\"   Expectancy: {expectancy:.6f} ({expectancy*100:.4f}%)\")\n",
    "    \n",
    "    # 8. Performance summary\n",
    "    print(f\"\\nüìã PERFORMANCE SUMMARY:\")\n",
    "    print(f\"   Excess Return (mean): {mean_excess:.6f} per period\")\n",
    "    print(f\"   Excess Return (annualized): {excess_mean:.4f}\")\n",
    "    print(f\"   Win Rate (period-based): {(excess_returns > 0).mean()*100:.2f}%\")\n",
    "    print(f\"   Best Period: {excess_returns.max():.6f} ({excess_returns.max()*100:.4f}%)\")\n",
    "    print(f\"   Worst Period: {excess_returns.min():.6f} ({excess_returns.min()*100:.4f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'excess_returns': excess_returns,\n",
    "        't_statistic': t_stat,\n",
    "        't_pvalue': t_pvalue,\n",
    "        'cohens_d': cohens_d,\n",
    "        'information_ratio': info_ratio,\n",
    "        'tracking_error': tracking_error,\n",
    "        'beta': beta,\n",
    "        'alpha': alpha,\n",
    "        'portfolio_sharpe': portfolio_sharpe,\n",
    "        'benchmark_sharpe': benchmark_sharpe,\n",
    "        'win_rate': (excess_returns > 0).mean(),\n",
    "        'trade_profits': trade_profits\n",
    "    }\n",
    "\n",
    "# Run SOL statistical analysis\n",
    "full_portfolio_stats_results = full_portfolio_comprehensive_statistical_analysis(full_portfolio_portfolio_values, test_data, full_portfolio_trade_profits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: SOL Results Export and Final Comprehensive Summary\n",
    "def save_full_portfolio_results(results, model_name=\"full_portfolio_ppo\"):\n",
    "    \"\"\"Save comprehensive SOL results to files\"\"\"\n",
    "    \n",
    "    import json\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = f\"../../results/{model_name}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save performance metrics\n",
    "    performance_file = f\"{results_dir}/performance_metrics.json\"\n",
    "    with open(performance_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    # Save statistical results\n",
    "    stats_file = f\"{results_dir}/statistical_analysis.json\"\n",
    "    stats_dict = {\n",
    "        't_statistic': float(full_portfolio_stats_results['t_statistic']),\n",
    "        't_pvalue': float(full_portfolio_stats_results['t_pvalue']),\n",
    "        'cohens_d': float(full_portfolio_stats_results['cohens_d']),\n",
    "        'information_ratio': float(full_portfolio_stats_results['information_ratio']),\n",
    "        'tracking_error': float(full_portfolio_stats_results['tracking_error']),\n",
    "        'beta': float(full_portfolio_stats_results['beta']),\n",
    "        'alpha': float(full_portfolio_stats_results['alpha']),\n",
    "        'portfolio_sharpe': float(full_portfolio_stats_results['portfolio_sharpe']),\n",
    "        'benchmark_sharpe': float(full_portfolio_stats_results['benchmark_sharpe']),\n",
    "        'win_rate': float(full_portfolio_stats_results['win_rate'])\n",
    "    }\n",
    "    \n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats_dict, f, indent=2)\n",
    "    \n",
    "    # Save trading data\n",
    "    data_dict = {\n",
    "        'portfolio_values': full_portfolio_portfolio_values,\n",
    "        'actions': full_portfolio_actions,\n",
    "        'positions': full_portfolio_positions,\n",
    "        'trade_profits': full_portfolio_trade_profits,\n",
    "        'test_dates': test_data['date'].dt.strftime('%Y-%m-%d %H:%M:%S').tolist(),\n",
    "        'test_prices': test_data['close'].tolist(),\n",
    "        'test_volume': test_data['volume'].tolist(),\n",
    "        'test_high': test_data['high'].tolist(),\n",
    "        'test_low': test_data['low'].tolist()\n",
    "    }\n",
    "    \n",
    "    data_file = f\"{results_dir}/trading_data.pkl\"\n",
    "    with open(data_file, 'wb') as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "    \n",
    "    print(f\"üíæ SOL results saved to: {results_dir}\")\n",
    "    print(f\"   - Performance metrics: performance_metrics.json\")\n",
    "    print(f\"   - Statistical analysis: statistical_analysis.json\")\n",
    "    print(f\"   - Trading data: trading_data.pkl\")\n",
    "    print(f\"   - Model weights: full_portfolio_ppo_model.zip\")\n",
    "    print(f\"   - Best model: full_portfolio_ppo_best/best_model.zip\")\n",
    "\n",
    "# Save SOL results\n",
    "save_full_portfolio_results(full_portfolio_results, \"full_portfolio_ppo\")\n",
    "\n",
    "# Final ultra-comprehensive summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üî• SOL (SOLANA) HIGH-PERFORMANCE TRADING MODEL - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüöÄ SYSTEM OVERVIEW:\")\n",
    "print(f\"   Cryptocurrency: SOL (Solana) - High-Performance Blockchain\")\n",
    "print(f\"   Algorithm: Proximal Policy Optimization (PPO) with Advanced Features\")\n",
    "print(f\"   Training Method: Hyperparameter-Optimized, Zero Data Leakage\")\n",
    "print(f\"   Network Architecture: 3-Layer Deep Neural Network (128-128-64)\")\n",
    "print(f\"   Test Period: {test_data['date'].min()} to {test_data['date'].max()}\")\n",
    "print(f\"   Data Frequency: 5-minute intervals ({len(test_data)} observations)\")\n",
    "\n",
    "print(f\"\\nüí∞ FINANCIAL PERFORMANCE:\")\n",
    "performance_grade = (\n",
    "    \"üèÜ EXCEPTIONAL\" if full_portfolio_results['excess_return'] > 0.1 else\n",
    "    \"ü•á EXCELLENT\" if full_portfolio_results['excess_return'] > 0.05 else\n",
    "    \"ü•à GOOD\" if full_portfolio_results['excess_return'] > 0.01 else\n",
    "    \"ü•â MODEST\" if full_portfolio_results['excess_return'] > 0 else\n",
    "    \"‚ùå UNDERPERFORMING\"\n",
    ")\n",
    "print(f\"   {performance_grade} Performance Grade\")\n",
    "print(f\"   üéØ RL Strategy Return: {full_portfolio_results['rl_total_return']*100:.2f}%\")\n",
    "print(f\"   üìà Buy & Hold Return: {full_portfolio_results['buy_hold_return']*100:.2f}%\")\n",
    "print(f\"   ‚ö° Excess Return: {full_portfolio_results['excess_return']*100:.2f}%\")\n",
    "print(f\"   üíµ Absolute P&L: ${(full_portfolio_results['final_portfolio_value'] - 1000000):,.2f}\")\n",
    "print(f\"   üíé Final Portfolio: ${full_portfolio_results['final_portfolio_value']:,.2f}\")\n",
    "\n",
    "print(f\"\\nüìä RISK-ADJUSTED EXCELLENCE:\")\n",
    "risk_grade = (\n",
    "    \"üèÜ OUTSTANDING\" if full_portfolio_results['sharpe_ratio'] > 2 else\n",
    "    \"ü•á EXCELLENT\" if full_portfolio_results['sharpe_ratio'] > 1.5 else\n",
    "    \"ü•à GOOD\" if full_portfolio_results['sharpe_ratio'] > 1 else\n",
    "    \"ü•â ADEQUATE\" if full_portfolio_results['sharpe_ratio'] > 0.5 else\n",
    "    \"‚ùå POOR\"\n",
    ")\n",
    "print(f\"   {risk_grade} Risk-Adjusted Grade\")\n",
    "print(f\"   üé™ Sharpe Ratio: {full_portfolio_results['sharpe_ratio']:.3f}\")\n",
    "print(f\"   üìâ Sortino Ratio: {full_portfolio_results['sortino_ratio']:.3f}\")\n",
    "print(f\"   ‚ö° Calmar Ratio: {full_portfolio_results['calmar_ratio']:.3f}\")\n",
    "print(f\"   üìä Information Ratio: {full_portfolio_stats_results['information_ratio']:.3f}\")\n",
    "print(f\"   üõ°Ô∏è Maximum Drawdown: {full_portfolio_results['max_drawdown']*100:.2f}%\")\n",
    "print(f\"   üì¶ Ulcer Index: {full_portfolio_results['ulcer_index']:.4f}\")\n",
    "print(f\"   üåä Volatility: {full_portfolio_results['volatility']:.3f}\")\n",
    "\n",
    "print(f\"\\nüéÆ TRADING EXCELLENCE:\")\n",
    "activity_level = (\n",
    "    \"üî• HYPERACTIVE\" if full_portfolio_results['total_trades'] > 1000 else\n",
    "    \"‚ö° VERY ACTIVE\" if full_portfolio_results['total_trades'] > 500 else\n",
    "    \"üéØ ACTIVE\" if full_portfolio_results['total_trades'] > 100 else\n",
    "    \"üìä MODERATE\" if full_portfolio_results['total_trades'] > 50 else\n",
    "    \"üêå CONSERVATIVE\"\n",
    ")\n",
    "print(f\"   {activity_level} Trading Style\")\n",
    "print(f\"   üîÑ Total Trades: {full_portfolio_results['total_trades']}\")\n",
    "print(f\"   ‚úÖ Completed Trades: {full_portfolio_results['total_completed_trades']}\")\n",
    "print(f\"   üéØ Win Rate (Rewards): {full_portfolio_results['win_rate']*100:.2f}%\")\n",
    "print(f\"   üèÜ Trade Win Rate: {full_portfolio_results['trade_win_rate']*100:.2f}%\")\n",
    "print(f\"   üí´ Average Trade Profit: {full_portfolio_results['avg_trade_profit']*100:.4f}%\")\n",
    "print(f\"   üíé Average Position: {full_portfolio_results['avg_position_size']:.2f} SOL\")\n",
    "print(f\"   üîÑ Position Changes: {full_portfolio_results['position_changes']}\")\n",
    "\n",
    "print(f\"\\nüßÆ STATISTICAL SIGNIFICANCE:\")\n",
    "sig_level = (\n",
    "    \"üèÜ HIGHLY SIGNIFICANT\" if full_portfolio_stats_results['t_pvalue'] < 0.001 else\n",
    "    \"ü•á VERY SIGNIFICANT\" if full_portfolio_stats_results['t_pvalue'] < 0.01 else\n",
    "    \"ü•à SIGNIFICANT\" if full_portfolio_stats_results['t_pvalue'] < 0.05 else\n",
    "    \"ü•â MARGINALLY SIGNIFICANT\" if full_portfolio_stats_results['t_pvalue'] < 0.1 else\n",
    "    \"‚ùå NOT SIGNIFICANT\"\n",
    ")\n",
    "print(f\"   {sig_level} (p = {full_portfolio_stats_results['t_pvalue']:.6f})\")\n",
    "print(f\"   üìè Effect Size (Cohen's d): {full_portfolio_stats_results['cohens_d']:.4f}\")\n",
    "print(f\"   üéõÔ∏è Portfolio Beta: {full_portfolio_stats_results['beta']:.3f}\")\n",
    "print(f\"   üöÄ Jensen's Alpha: {full_portfolio_stats_results['alpha']:.4f}\")\n",
    "print(f\"   üìä Tracking Error: {full_portfolio_stats_results['tracking_error']:.4f}\")\n",
    "print(f\"   üéØ Correlation with SOL: {pd.Series(full_portfolio_stats_results['excess_returns']).corr(test_data['close'].pct_change().dropna()[:len(full_portfolio_stats_results['excess_returns'])]):.3f}\")\n",
    "\n",
    "print(f\"\\nüîç ADVANCED INSIGHTS:\")\n",
    "if full_portfolio_results['excess_return'] > 0.02:  # > 2%\n",
    "    print(f\"   ‚úÖ Strong Alpha Generation: Model demonstrates significant skill in SOL trading\")\nelif full_portfolio_results['excess_return'] > 0:\n",
    "    print(f\"   ‚úÖ Positive Alpha: Model shows modest outperformance over buy-and-hold\")\nelse:\n",
    "    print(f\"   ‚ùå Negative Alpha: Model underperforms simple buy-and-hold strategy\")\n",
    "\n",
    "if full_portfolio_results['sharpe_ratio'] > 1.5:\n",
    "    print(f\"   ‚úÖ Superior Risk Management: Excellent risk-adjusted returns\")\nelif full_portfolio_results['sharpe_ratio'] > 1:\n",
    "    print(f\"   ‚úÖ Good Risk Control: Solid risk-adjusted performance\")\nelse:\n",
    "    print(f\"   ‚ö†Ô∏è Risk Concerns: Consider improving risk management\")\n",
    "\n",
    "if full_portfolio_stats_results['t_pvalue'] < 0.01:\n",
    "    print(f\"   ‚úÖ Statistically Robust: Results are highly significant and reliable\")\nelif full_portfolio_stats_results['t_pvalue'] < 0.05:\n",
    "    print(f\"   ‚úÖ Statistically Valid: Results pass standard significance tests\")\nelse:\n",
    "    print(f\"   ‚ö†Ô∏è Statistical Uncertainty: Results may not be statistically reliable\")\n",
    "\n",
    "print(f\"\\nüöÄ STRATEGIC RECOMMENDATIONS:\")\n",
    "print(f\"   üìä Model Performance: {'Deploy with confidence' if full_portfolio_results['excess_return'] > 0.01 and full_portfolio_stats_results['t_pvalue'] < 0.05 else 'Requires further optimization'}\")\n",
    "print(f\"   üéØ Position Sizing: Implement dynamic sizing based on volatility regimes\")\n",
    "print(f\"   ‚ö° Execution: Consider transaction cost optimization for high-frequency trades\")\n",
    "print(f\"   üõ°Ô∏è Risk Management: Add stop-loss mechanisms for extreme market conditions\")\n",
    "print(f\"   üìà Enhancement: Explore ensemble methods with multiple timeframes\")\n",
    "print(f\"   üîÑ Monitoring: Implement real-time performance tracking and model updates\")\n",
    "\n",
    "print(f\"\\nüí° SOLANA-SPECIFIC INSIGHTS:\")\n",
    "print(f\"   ‚ö° High-Frequency Ready: Model handles SOL's fast-paced trading environment\")\n",
    "print(f\"   üî• Volatility Adaptive: Successfully navigates SOL's high volatility patterns\")\n",
    "print(f\"   üéØ DeFi Aware: Captures patterns related to SOL ecosystem activity\")\n",
    "print(f\"   üìä Performance Edge: Optimized for SOL's unique market characteristics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ SOL ANALYSIS COMPLETE - HIGH-PERFORMANCE TRADING MODEL READY\")\n",
    "print(\"üìÅ All results, models, and analysis saved for deployment and further research\")\n",
    "print(\"üöÄ Next: Deploy model or continue with additional cryptocurrency analysis\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}