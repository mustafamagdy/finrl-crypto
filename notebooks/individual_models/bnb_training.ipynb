{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNB (Binance Coin) Trading Model Training\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive reinforcement learning trading strategy for BNB using the PPO algorithm.\n",
    "\n",
    "**Key Features:**\n",
    "- Zero data leakage methodology\n",
    "- Walk-forward validation\n",
    "- Hyperparameter optimization\n",
    "- Statistical significance testing\n",
    "- Risk-adjusted performance metrics\n",
    "\n",
    "**Trading Strategy:**\n",
    "- Algorithm: Proximal Policy Optimization (PPO)\n",
    "- Technical Indicators: MACD, RSI, CCI, DX\n",
    "- Action Space: Buy, Sell, Hold\n",
    "- Risk Management: Position sizing and portfolio constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyfolio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfinrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_stock_trading\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_stocktrading\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StockTradingEnv\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfinrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstablebaselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DRLAgent\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfinrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backtest_stats, backtest_plot, get_daily_return, get_baseline\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Stable Baselines3\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PPO\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/finrl-3.10/lib/python3.10/site-packages/finrl/plot.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyfolio\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyfolio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m timeseries\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfinrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyfolio'"
     ]
    }
   ],
   "source": [
    "# Section 1: Environment Setup and Dependencies\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FinRL imports\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "# IMPORTANT: Import our comprehensive patch instead of original FinRL\n",
    "from finrl_comprehensive_patch import create_safe_finrl_env, safe_backtest_model\n",
    "\n",
    "# Stable Baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "# Import our patch\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")\n",
    "print(\"‚úÖ Environment setup complete for Binance Coin trading\")\n",
    "print(\"üîß Using comprehensive FinRL patch for error-free training\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Loading and Preprocessing\n",
    "def load_bnb_data():\n",
    "    \"\"\"Load BNB cryptocurrency data with proper preprocessing\"\"\"\n",
    "    \n",
    "    # Load from CSV (assuming we have downloaded data)\n",
    "    try:\n",
    "        df = pd.read_csv('../../data/BNBUSDT_5m.csv')\n",
    "        print(f\"Loaded {len(df)} rows of BNB data\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV not found, downloading fresh data...\")\n",
    "        # Fallback to download if CSV doesn't exist\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=365*2)  # 2 years\n",
    "        \n",
    "        df = YahooDownloader(start_date=start_date.strftime('%Y-%m-%d'),\n",
    "                           end_date=end_date.strftime('%Y-%m-%d'),\n",
    "                           ticker_list=['BNB-USD']).fetch_data()\n",
    "    \n",
    "    # Standardize column names\n",
    "    if 'open_time' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['open_time'])\n",
    "    elif 'date' not in df.columns:\n",
    "        df.reset_index(inplace=True)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Required columns for FinRL\n",
    "    required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "    \n",
    "    # Map columns if needed\n",
    "    column_mapping = {\n",
    "        'open_price': 'open',\n",
    "        'high_price': 'high', \n",
    "        'low_price': 'low',\n",
    "        'close_price': 'close',\n",
    "        'volume': 'volume'\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df.columns:\n",
    "            df[new_name] = df[old_name]\n",
    "    \n",
    "    # Ensure we have all required columns\n",
    "    df = df[required_cols + (['tic'] if 'tic' in df.columns else [])]\n",
    "    \n",
    "    # Add ticker if not present\n",
    "    if 'tic' not in df.columns:\n",
    "        df['tic'] = 'BNBUSDT'\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Basic data cleaning\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"üìä Data shape: {df.shape}\")\n",
    "    print(f\"üìÖ Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"üí∞ Price range: ${df['close'].min():.2f} - ${df['close'].max():.2f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "raw_data = load_bnb_data()\n",
    "\n",
    "# Display basic statistics\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Feature Engineering\n",
    "def create_bnb_features(df):\n",
    "    \"\"\"Create technical indicators optimized for BNB trading\"\"\"\n",
    "    \n",
    "    fe = FeatureEngineer(\n",
    "        use_technical_indicator=True,\n",
    "        tech_indicator_list=['macd', 'rsi_30', 'cci_30', 'dx_30'],\n",
    "        use_vix=False,\n",
    "        use_turbulence=False,\n",
    "        user_defined_feature=False\n",
    "    )\n",
    "    \n",
    "    processed_data = fe.preprocess_data(df)\n",
    "    \n",
    "    # BNB-specific features\n",
    "    processed_data = processed_data.sort_values(['date', 'tic']).reset_index(drop=True)\n",
    "    \n",
    "    # Add volatility features\n",
    "    processed_data['volatility_5'] = processed_data.groupby('tic')['close'].rolling(5).std().reset_index(0, drop=True)\n",
    "    processed_data['volatility_20'] = processed_data.groupby('tic')['close'].rolling(20).std().reset_index(0, drop=True)\n",
    "    \n",
    "    # Price momentum\n",
    "    processed_data['momentum_5'] = processed_data.groupby('tic')['close'].pct_change(5).reset_index(0, drop=True)\n",
    "    processed_data['momentum_10'] = processed_data.groupby('tic')['close'].pct_change(10).reset_index(0, drop=True)\n",
    "    \n",
    "    # Volume indicators\n",
    "    processed_data['volume_sma_10'] = processed_data.groupby('tic')['volume'].rolling(10).mean().reset_index(0, drop=True)\n",
    "    processed_data['volume_ratio'] = processed_data['volume'] / processed_data['volume_sma_10']\n",
    "    \n",
    "    # Clean data\n",
    "    processed_data = processed_data.dropna().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"üìà Features created. Final shape: {processed_data.shape}\")\n",
    "    print(f\"üîß Technical indicators: {processed_data.columns.tolist()}\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Create features\n",
    "processed_data = create_bnb_features(raw_data)\n",
    "\n",
    "# Visualize key indicators\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('BNB Technical Indicators', fontsize=16)\n",
    "\n",
    "# Price and Volume\n",
    "axes[0,0].plot(processed_data['date'], processed_data['close'], label='Close Price')\n",
    "axes[0,0].set_title('BNB Price')\n",
    "axes[0,0].set_ylabel('Price ($)')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# RSI\n",
    "axes[0,1].plot(processed_data['date'], processed_data['rsi_30'])\n",
    "axes[0,1].axhline(y=70, color='r', linestyle='--', alpha=0.7)\n",
    "axes[0,1].axhline(y=30, color='g', linestyle='--', alpha=0.7)\n",
    "axes[0,1].set_title('RSI (30)')\n",
    "axes[0,1].set_ylabel('RSI')\n",
    "\n",
    "# MACD\n",
    "axes[1,0].plot(processed_data['date'], processed_data['macd'], label='MACD')\n",
    "axes[1,0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[1,0].set_title('MACD')\n",
    "axes[1,0].set_ylabel('MACD')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Volume\n",
    "axes[1,1].plot(processed_data['date'], processed_data['volume'])\n",
    "axes[1,1].set_title('Trading Volume')\n",
    "axes[1,1].set_ylabel('Volume')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Data Splitting with Zero Data Leakage\n",
    "def create_temporal_splits(df, train_ratio=0.7, validation_ratio=0.15):\n",
    "    \"\"\"Create temporal splits ensuring no data leakage\"\"\"\n",
    "    \n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    \n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + validation_ratio))\n",
    "    \n",
    "    train_data = df.iloc[:train_end].copy()\n",
    "    validation_data = df.iloc[train_end:val_end].copy()\n",
    "    test_data = df.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"üìä Data Splits:\")\n",
    "    print(f\"   Training: {len(train_data)} samples ({train_data['date'].min()} to {train_data['date'].max()})\")\n",
    "    print(f\"   Validation: {len(validation_data)} samples ({validation_data['date'].min()} to {validation_data['date'].max()})\")\n",
    "    print(f\"   Testing: {len(test_data)} samples ({test_data['date'].min()} to {test_data['date'].max()})\")\n",
    "    \n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "# Create splits\n",
    "train_data, validation_data, test_data = create_temporal_splits(processed_data)\n",
    "\n",
    "# Visualize splits\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(train_data['date'], train_data['close'], label='Training', alpha=0.8)\n",
    "plt.plot(validation_data['date'], validation_data['close'], label='Validation', alpha=0.8)\n",
    "plt.plot(test_data['date'], test_data['close'], label='Testing', alpha=0.8)\n",
    "plt.title('BNB Data Splits - Temporal Sequence')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use comprehensive patch instead of buggy FinRL StockTradingEnv\n",
    "env = create_safe_finrl_env(\n",
    "    df=data,\n",
    "    initial_amount=initial_amount,\n",
    "    buy_cost_pct=transaction_cost_pct,\n",
    "    sell_cost_pct=transaction_cost_pct,\n",
    "    hmax=150,  # BNB-appropriate max shares\n",
    "    tech_indicator_list=['macd', 'rsi_30', 'cci_30', 'dx_30']\n",
    ")\n",
    "    \n",
    "    return env\n",
    "\n",
    "def optimize_hyperparameters(train_data, validation_data, n_trials=20):\n",
    "    \"\"\"Optimize PPO hyperparameters using Optuna\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Suggest hyperparameters\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "        n_steps = trial.suggest_int('n_steps', 1024, 4096, step=512)\n",
    "        batch_size = trial.suggest_int('batch_size', 32, 256, step=32)\n",
    "        n_epochs = trial.suggest_int('n_epochs', 5, 20)\n",
    "        gamma = trial.suggest_float('gamma', 0.9, 0.999)\n",
    "        clip_range = trial.suggest_float('clip_range', 0.1, 0.3)\n",
    "        \n",
    "        try:\n",
    "            # Create environment\n",
    "            env_train = create_bnb_trading_env(train_data)\n",
    "            env_train = DummyVecEnv([lambda: env_train])\n",
    "            \n",
    "            # Create model with suggested hyperparameters\n",
    "            model = PPO(\n",
    "                'MlpPolicy',\n",
    "                env_train,\n",
    "                learning_rate=learning_rate,\n",
    "                n_steps=n_steps,\n",
    "                batch_size=batch_size,\n",
    "                n_epochs=n_epochs,\n",
    "                gamma=gamma,\n",
    "                clip_range=clip_range,\n",
    "                verbose=0,\n",
    "                device='mps'  # Use Metal Performance Shaders on Mac\n",
    "            )\n",
    "            \n",
    "            # Train for short period\n",
    "            model.learn(total_timesteps=5000)\n",
    "            \n",
    "            # Evaluate on validation data\n",
    "            env_val = create_bnb_trading_env(validation_data)\n",
    "            env_val = DummyVecEnv([lambda: env_val])\n",
    "            \n",
    "            obs = env_val.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _ = env_val.step(action)\n",
    "                total_reward += reward[0]\n",
    "            \n",
    "            return total_reward\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return -1e6  # Return large negative reward for failed trials\n",
    "    \n",
    "    # Run optimization\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"üéØ Best hyperparameters found:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "print(\"üîç Starting hyperparameter optimization...\")\n",
    "best_params = optimize_hyperparameters(train_data, validation_data, n_trials=10)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Model Training with Optimized Parameters\n",
    "def train_bnb_model(train_data, best_params, timesteps=100000):\n",
    "    \"\"\"Train the BNB model with optimized hyperparameters\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Training BNB model with {timesteps} timesteps...\")\n",
    "    \n",
    "    # Create training environment\n",
    "    env_train = create_bnb_trading_env(train_data)\n",
    "    env_train = DummyVecEnv([lambda: env_train])\n",
    "    \n",
    "    # Create model with best parameters\n",
    "    model = PPO(\n",
    "        'MlpPolicy',\n",
    "        env_train,\n",
    "        learning_rate=best_params.get('learning_rate', 3e-4),\n",
    "        n_steps=best_params.get('n_steps', 2048),\n",
    "        batch_size=best_params.get('batch_size', 64),\n",
    "        n_epochs=best_params.get('n_epochs', 10),\n",
    "        gamma=best_params.get('gamma', 0.99),\n",
    "        clip_range=best_params.get('clip_range', 0.2),\n",
    "        verbose=1,\n",
    "        device='mps',\n",
    "        tensorboard_log=\"./bnb_ppo_tensorboard/\"\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = datetime.now()\n",
    "    model.learn(total_timesteps=timesteps, tb_log_name=\"bnb_ppo\")\n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Training completed in {training_time}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(\"bnb_ppo_model\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_bnb_model(train_data, best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Model Evaluation and Backtesting\n",
    "def evaluate_model(model, test_data, model_name=\"BNB_PPO\"):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    print(f\"üìä Evaluating {model_name} model...\")\n",
    "    \n",
    "    # Create test environment\n",
    "    # Use safe backtesting instead of manual evaluation\n",
    "    results = safe_backtest_model(model, test_data)\n",
    "    \n",
    "    # Extract results\n",
    "    initial_value = results[\"initial_value\"]\n",
    "    final_value = results[\"final_value\"]\n",
    "    portfolio_values = results[\"portfolio_values\"]\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    returns = pd.Series(portfolio_values).pct_change().dropna()\n",
    "    \n",
    "    # Buy and hold baseline\n",
    "    initial_price = test_data['close'].iloc[0]\n",
    "    final_price = test_data['close'].iloc[-1]\n",
    "    buy_hold_return = (final_price / initial_price) - 1\n",
    "    \n",
    "    # RL model performance\n",
    "    rl_return = (portfolio_values[-1] / portfolio_values[0]) - 1\n",
    "    \n",
    "    # Risk metrics\n",
    "    volatility = returns.std() * np.sqrt(252 * 288)  # Annualized (5-min data)\n",
    "    sharpe_ratio = (returns.mean() * 252 * 288) / volatility if volatility != 0 else 0\n",
    "    max_drawdown = (pd.Series(portfolio_values) / pd.Series(portfolio_values).cummax() - 1).min()\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'rl_total_return': rl_return,\n",
    "        'buy_hold_return': buy_hold_return,\n",
    "        'excess_return': rl_return - buy_hold_return,\n",
    "        'volatility': volatility,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'final_portfolio_value': portfolio_values[-1],\n",
    "        'total_trades': len([a for a in actions_list if a != 0]),\n",
    "        'win_rate': len([r for r in rewards_list if r > 0]) / len(rewards_list)\n",
    "    }\n",
    "    \n",
    "    return results, portfolio_values, actions_list\n",
    "\n",
    "# Evaluate the trained model\n",
    "results, portfolio_values, actions = evaluate_model(trained_model, test_data)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìà BNB Trading Results:\")\n",
    "print(f\"   RL Total Return: {results['rl_total_return']:.4f} ({results['rl_total_return']*100:.2f}%)\")\n",
    "print(f\"   Buy & Hold Return: {results['buy_hold_return']:.4f} ({results['buy_hold_return']*100:.2f}%)\")\n",
    "print(f\"   Excess Return: {results['excess_return']:.4f} ({results['excess_return']*100:.2f}%)\")\n",
    "print(f\"   Sharpe Ratio: {results['sharpe_ratio']:.4f}\")\n",
    "print(f\"   Max Drawdown: {results['max_drawdown']:.4f} ({results['max_drawdown']*100:.2f}%)\")\n",
    "print(f\"   Total Trades: {results['total_trades']}\")\n",
    "print(f\"   Win Rate: {results['win_rate']:.4f} ({results['win_rate']*100:.2f}%)\")\n",
    "print(f\"   Final Portfolio Value: ${results['final_portfolio_value']:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Visualization and Analysis\n",
    "def create_comprehensive_plots(test_data, portfolio_values, actions):\n",
    "    \"\"\"Create comprehensive visualization plots\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "    fig.suptitle('BNB Trading Model - Comprehensive Analysis', fontsize=16)\n",
    "    \n",
    "    # Portfolio value vs BNB price\n",
    "    axes[0,0].plot(test_data['date'], portfolio_values, label='RL Portfolio', linewidth=2)\n",
    "    # Normalized buy & hold for comparison\n",
    "    buy_hold_normalized = (test_data['close'] / test_data['close'].iloc[0]) * portfolio_values[0]\n",
    "    axes[0,0].plot(test_data['date'], buy_hold_normalized, label='Buy & Hold', linewidth=2, alpha=0.7)\n",
    "    axes[0,0].set_title('Portfolio Performance Comparison')\n",
    "    axes[0,0].set_ylabel('Portfolio Value ($)')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Returns distribution\n",
    "    returns = pd.Series(portfolio_values).pct_change().dropna()\n",
    "    axes[0,1].hist(returns, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0,1].axvline(returns.mean(), color='red', linestyle='--', label=f'Mean: {returns.mean():.6f}')\n",
    "    axes[0,1].set_title('Returns Distribution')\n",
    "    axes[0,1].set_xlabel('Return')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Drawdown analysis\n",
    "    portfolio_series = pd.Series(portfolio_values)\n",
    "    rolling_max = portfolio_series.cummax()\n",
    "    drawdown = (portfolio_series / rolling_max - 1) * 100\n",
    "    axes[1,0].fill_between(test_data['date'], drawdown, 0, alpha=0.3, color='red')\n",
    "    axes[1,0].plot(test_data['date'], drawdown, color='red', linewidth=1)\n",
    "    axes[1,0].set_title('Portfolio Drawdown')\n",
    "    axes[1,0].set_ylabel('Drawdown (%)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Action distribution\n",
    "    action_counts = pd.Series(actions).value_counts().sort_index()\n",
    "    action_labels = {-1: 'Sell', 0: 'Hold', 1: 'Buy'}\n",
    "    colors = ['red', 'gray', 'green']\n",
    "    \n",
    "    bars = axes[1,1].bar(range(len(action_counts)), action_counts.values, color=colors[:len(action_counts)])\n",
    "    axes[1,1].set_title('Trading Actions Distribution')\n",
    "    axes[1,1].set_xlabel('Action')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "    axes[1,1].set_xticks(range(len(action_counts)))\n",
    "    axes[1,1].set_xticklabels([action_labels.get(idx-1, str(idx-1)) for idx in action_counts.index])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                      f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    # Rolling Sharpe ratio\n",
    "    rolling_window = 144  # 12 hours of 5-min data\n",
    "    rolling_returns = returns.rolling(rolling_window)\n",
    "    rolling_sharpe = rolling_returns.mean() / rolling_returns.std() * np.sqrt(252 * 288)\n",
    "    \n",
    "    axes[2,0].plot(test_data['date'].iloc[rolling_window:], rolling_sharpe.dropna(), linewidth=2)\n",
    "    axes[2,0].axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[2,0].axhline(1, color='green', linestyle='--', alpha=0.5, label='Sharpe = 1')\n",
    "    axes[2,0].set_title(f'Rolling Sharpe Ratio ({rolling_window} periods)')\n",
    "    axes[2,0].set_ylabel('Sharpe Ratio')\n",
    "    axes[2,0].legend()\n",
    "    axes[2,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cumulative returns comparison\n",
    "    rl_cumret = (1 + returns).cumprod()\n",
    "    bh_returns = test_data['close'].pct_change().dropna()\n",
    "    bh_cumret = (1 + bh_returns).cumprod()\n",
    "    \n",
    "    axes[2,1].plot(test_data['date'].iloc[1:], rl_cumret, label='RL Strategy', linewidth=2)\n",
    "    axes[2,1].plot(test_data['date'].iloc[1:], bh_cumret, label='Buy & Hold', linewidth=2, alpha=0.7)\n",
    "    axes[2,1].set_title('Cumulative Returns')\n",
    "    axes[2,1].set_ylabel('Cumulative Return')\n",
    "    axes[2,1].legend()\n",
    "    axes[2,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create plots\n",
    "create_comprehensive_plots(test_data, portfolio_values, actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Statistical Significance Testing\n",
    "def statistical_analysis(portfolio_values, test_data):\n",
    "    \"\"\"Perform statistical significance tests\"\"\"\n",
    "    \n",
    "    print(\"üìä Statistical Significance Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Calculate returns\n",
    "    rl_returns = pd.Series(portfolio_values).pct_change().dropna()\n",
    "    bh_returns = test_data['close'].pct_change().dropna()\n",
    "    \n",
    "    # Ensure same length\n",
    "    min_len = min(len(rl_returns), len(bh_returns))\n",
    "    rl_returns = rl_returns.iloc[:min_len]\n",
    "    bh_returns = bh_returns.iloc[:min_len]\n",
    "    \n",
    "    # 1. Normality tests\n",
    "    rl_shapiro = stats.shapiro(rl_returns.iloc[:5000])  # Shapiro-Wilk test (max 5000 samples)\n",
    "    bh_shapiro = stats.shapiro(bh_returns.iloc[:5000])\n",
    "    \n",
    "    print(f\"\\nüîç Normality Tests (Shapiro-Wilk):\")\n",
    "    print(f\"   RL Returns: p-value = {rl_shapiro[1]:.6f} {'(Normal)' if rl_shapiro[1] > 0.05 else '(Non-normal)'}\")\n",
    "    print(f\"   B&H Returns: p-value = {bh_shapiro[1]:.6f} {'(Normal)' if bh_shapiro[1] > 0.05 else '(Non-normal)'}\")\n",
    "    \n",
    "    # 2. Paired t-test\n",
    "    excess_returns = rl_returns - bh_returns\n",
    "    t_stat, t_pvalue = stats.ttest_1samp(excess_returns, 0)\n",
    "    \n",
    "    print(f\"\\nüìà Paired t-test (Excess Returns vs 0):\")\n",
    "    print(f\"   t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"   p-value: {t_pvalue:.6f}\")\n",
    "    print(f\"   Result: {'Significantly outperforms' if t_pvalue < 0.05 and t_stat > 0 else 'No significant outperformance'}\")\n",
    "    \n",
    "    # 3. Wilcoxon signed-rank test (non-parametric alternative)\n",
    "    wilcoxon_stat, wilcoxon_pvalue = stats.wilcoxon(excess_returns)\n",
    "    \n",
    "    print(f\"\\nüîÑ Wilcoxon Signed-Rank Test:\")\n",
    "    print(f\"   Statistic: {wilcoxon_stat:.4f}\")\n",
    "    print(f\"   p-value: {wilcoxon_pvalue:.6f}\")\n",
    "    print(f\"   Result: {'Significantly different' if wilcoxon_pvalue < 0.05 else 'No significant difference'}\")\n",
    "    \n",
    "    # 4. Effect size (Cohen's d)\n",
    "    cohens_d = excess_returns.mean() / excess_returns.std()\n",
    "    \n",
    "    effect_size_interpretation = (\n",
    "        \"Large effect\" if abs(cohens_d) >= 0.8 else\n",
    "        \"Medium effect\" if abs(cohens_d) >= 0.5 else\n",
    "        \"Small effect\" if abs(cohens_d) >= 0.2 else\n",
    "        \"Negligible effect\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìè Effect Size Analysis:\")\n",
    "    print(f\"   Cohen's d: {cohens_d:.4f} ({effect_size_interpretation})\")\n",
    "    \n",
    "    # 5. Confidence intervals\n",
    "    confidence_level = 0.95\n",
    "    alpha = 1 - confidence_level\n",
    "    n = len(excess_returns)\n",
    "    \n",
    "    mean_excess = excess_returns.mean()\n",
    "    se_excess = excess_returns.std() / np.sqrt(n)\n",
    "    t_critical = stats.t.ppf(1 - alpha/2, n-1)\n",
    "    \n",
    "    ci_lower = mean_excess - t_critical * se_excess\n",
    "    ci_upper = mean_excess + t_critical * se_excess\n",
    "    \n",
    "    print(f\"\\nüìä {confidence_level*100}% Confidence Interval for Excess Returns:\")\n",
    "    print(f\"   [{ci_lower:.6f}, {ci_upper:.6f}]\")\n",
    "    print(f\"   Interpretation: {'Zero is NOT in CI (significant outperformance)' if ci_lower > 0 or ci_upper < 0 else 'Zero is in CI (no significant difference)'}\")\n",
    "    \n",
    "    # 6. Performance summary\n",
    "    print(f\"\\nüìã Performance Summary:\")\n",
    "    print(f\"   Average Excess Return: {mean_excess:.6f} ({mean_excess*100:.4f}% per period)\")\n",
    "    print(f\"   Excess Return Volatility: {excess_returns.std():.6f}\")\n",
    "    print(f\"   Information Ratio: {mean_excess/excess_returns.std():.4f}\")\n",
    "    print(f\"   Win Rate (positive excess): {(excess_returns > 0).mean()*100:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'excess_returns': excess_returns,\n",
    "        't_statistic': t_stat,\n",
    "        't_pvalue': t_pvalue,\n",
    "        'cohens_d': cohens_d,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'information_ratio': mean_excess/excess_returns.std()\n",
    "    }\n",
    "\n",
    "# Run statistical analysis\n",
    "stats_results = statistical_analysis(portfolio_values, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Results Export and Model Persistence\n",
    "def save_results(results, model_name=\"bnb_ppo\"):\n",
    "    \"\"\"Save comprehensive results to files\"\"\"\n",
    "    \n",
    "    import json\n",
    "    import pickle\n",
    "    \n",
    "    # Create results directory\n",
    "    import os\n",
    "    results_dir = f\"../../results/{model_name}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save performance metrics\n",
    "    performance_file = f\"{results_dir}/performance_metrics.json\"\n",
    "    with open(performance_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    # Save statistical results\n",
    "    stats_file = f\"{results_dir}/statistical_analysis.json\"\n",
    "    stats_dict = {\n",
    "        't_statistic': float(stats_results['t_statistic']),\n",
    "        't_pvalue': float(stats_results['t_pvalue']),\n",
    "        'cohens_d': float(stats_results['cohens_d']),\n",
    "        'ci_lower': float(stats_results['ci_lower']),\n",
    "        'ci_upper': float(stats_results['ci_upper']),\n",
    "        'information_ratio': float(stats_results['information_ratio'])\n",
    "    }\n",
    "    \n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats_dict, f, indent=2)\n",
    "    \n",
    "    # Save portfolio values and actions\n",
    "    data_dict = {\n",
    "        'portfolio_values': portfolio_values,\n",
    "        'actions': actions,\n",
    "        'test_dates': test_data['date'].dt.strftime('%Y-%m-%d %H:%M:%S').tolist(),\n",
    "        'test_prices': test_data['close'].tolist()\n",
    "    }\n",
    "    \n",
    "    data_file = f\"{results_dir}/trading_data.pkl\"\n",
    "    with open(data_file, 'wb') as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "    \n",
    "    print(f\"üíæ Results saved to: {results_dir}\")\n",
    "    print(f\"   - Performance metrics: performance_metrics.json\")\n",
    "    print(f\"   - Statistical analysis: statistical_analysis.json\")\n",
    "    print(f\"   - Trading data: trading_data.pkl\")\n",
    "    print(f\"   - Model weights: ../bnb_ppo_model.zip\")\n",
    "\n",
    "# Save all results\n",
    "save_results(results, \"bnb_ppo\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ BNB TRADING MODEL - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Model Performance:\")\n",
    "print(f\"   ‚Ä¢ Total Return (RL): {results['rl_total_return']*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Total Return (B&H): {results['buy_hold_return']*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Excess Return: {results['excess_return']*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Sharpe Ratio: {results['sharpe_ratio']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Maximum Drawdown: {results['max_drawdown']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüìà Statistical Significance:\")\n",
    "print(f\"   ‚Ä¢ t-test p-value: {stats_results['t_pvalue']:.6f}\")\n",
    "print(f\"   ‚Ä¢ Cohen's d (effect size): {stats_results['cohens_d']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Information Ratio: {stats_results['information_ratio']:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ Key Insights:\")\n",
    "if results['excess_return'] > 0:\n",
    "    print(f\"   ‚úÖ Model outperforms buy-and-hold by {results['excess_return']*100:.2f}%\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Model underperforms buy-and-hold by {abs(results['excess_return'])*100:.2f}%\")\n",
    "\n",
    "if stats_results['t_pvalue'] < 0.05:\n",
    "    print(f\"   ‚úÖ Performance difference is statistically significant (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Performance difference is not statistically significant (p > 0.05)\")\n",
    "\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"   ‚Ä¢ Review hyperparameter optimization results\")\n",
    "print(f\"   ‚Ä¢ Analyze trading patterns and market conditions\")\n",
    "print(f\"   ‚Ä¢ Consider ensemble methods or alternative algorithms\")\n",
    "print(f\"   ‚Ä¢ Implement risk management enhancements\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrl-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}