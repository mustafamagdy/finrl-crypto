{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Transformer Training for Cryptocurrency Trading\n",
    "\n",
    "**Phase 1 Implementation - GPU Accelerated Training**\n",
    "\n",
    "This notebook implements the enhanced transformer model with:\n",
    "- Extended sequence length (250 steps)\n",
    "- Temporal attention bias\n",
    "- Multi-scale processing\n",
    "- Advanced feature engineering\n",
    "- GPU acceleration support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All requirements installed successfully!\n",
      "🚀 Default device: cuda\n",
      "💻 GPU: NVIDIA GeForce RTX 4070\n",
      "🧠 GPU Memory: 12.5 GB\n",
      "\n",
      "🎯 Ready for enhanced transformer training!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install All Requirements for Enhanced Transformer Training\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"🚀 Installing Enhanced Transformer Requirements...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Update pip first\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Install stable PyTorch version (2.1.0 is stable and doesn't have the API issues)\n",
    "print(\"🔥 Installing PyTorch with GPU support...\")\n",
    "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install core ML libraries\n",
    "print(\"📊 Installing core ML libraries...\")\n",
    "!pip install numpy pandas scikit-learn matplotlib seaborn plotly\n",
    "\n",
    "# Install technical analysis libraries\n",
    "print(\"📈 Installing technical analysis libraries...\")\n",
    "!pip install ta talib-binary\n",
    "\n",
    "# Install utilities\n",
    "print(\"🔧 Installing utilities...\")\n",
    "!pip install tqdm psutil requests ipywidgets\n",
    "\n",
    "# Install Jupyter\n",
    "print(\"📓 Installing Jupyter...\")\n",
    "!pip install jupyter\n",
    "\n",
    "# Install stable-baselines3 for RL\n",
    "!pip install stable-baselines3\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(\"✅ All requirements installed successfully!\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"🚀 Default device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(f\"💻 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "  print(f\"🧠 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "elif torch.backends.mps.is_available():\n",
    "  print(\"🍎 Apple Silicon GPU available\")\n",
    "else:\n",
    "  print(\"⚠️ No GPU detected, using CPU\")\n",
    "\n",
    "print(\"\\n🎯 Ready for enhanced transformer training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "🔧 PyTorch version: 2.8.0+cu128\n",
      "💻 GPU: NVIDIA GeForce RTX 4070\n",
      "🧠 GPU Memory: 12.5 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and handle PyTorch import issues\n",
    "import torch\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Workaround for PyTorch import issues\n",
    "try:\n",
    "    from torch._utils_internal import justknobs_check\n",
    "except ImportError:\n",
    "    # Create a dummy function if the import fails\n",
    "    def justknobs_check(name, default=False):\n",
    "        return default\n",
    "\n",
    "# Fix for PyTorch 2.2+ pytree API changes\n",
    "try:\n",
    "    # Check if register_pytree_node exists\n",
    "    from torch.utils._pytree import register_pytree_node\n",
    "except ImportError:\n",
    "    # Apply monkey patch for older pytree API\n",
    "    import torch.utils._pytree as pytree\n",
    "    if not hasattr(pytree, 'register_pytree_node'):\n",
    "        def register_pytree_node(*args, **kwargs):\n",
    "            # Dummy implementation for compatibility\n",
    "            pass\n",
    "        pytree.register_pytree_node = register_pytree_node\n",
    "\n",
    "# Disable torch.compile to avoid more issues\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "print(f\"🔧 PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"💻 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"🧠 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.10.6)\n",
      "Requirement already satisfied: ta in /opt/conda/lib/python3.10/site-packages (0.11.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement talib-binary (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for talib-binary\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.0+cu121)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.0+cu121)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/conda/lib/python3.10/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.10/site-packages (from triton==3.4.0->torch) (68.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl (2200.6 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (11.3.0)\n",
      "Collecting triton==2.1.0 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Installing collected packages: triton, torch\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.4.0\n",
      "\u001b[2K    Uninstalling triton-3.4.0:\n",
      "\u001b[2K      Successfully uninstalled triton-3.4.0\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: torch 2.8.0\u001b[0m \u001b[32m0/2\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling torch-2.8.0:[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.8.0\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torch]32m1/2\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "stable-baselines3 2.7.0 requires torch<3.0,>=2.3, but you have torch 2.1.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.1.0+cu121 triton-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m✅ Packages installed successfully\n",
      "PyTorch version: 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "# Install required packages with specific PyTorch version\n",
    "!pip install pandas numpy scikit-learn matplotlib ta talib-binary\n",
    "\n",
    "# Install stable PyTorch version\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "print(\"✅ Packages installed successfully\")\n",
    "\n",
    "# Verify PyTorch installation\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1cOc9wKmdHxL4sKBZVWhSoAKdMVLE32lj\n",
      "To: /mnt/crypto_5min_2years.csv\n",
      "100%|██████████| 42.7M/42.7M [00:05<00:00, 8.14MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: crypto_5min_2years.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip -q install gdown\n",
    "\n",
    "import gdown\n",
    "\n",
    "# Your shared link\n",
    "url = \"https://drive.google.com/file/d/1cOc9wKmdHxL4sKBZVWhSoAKdMVLE32lj/view?usp=sharing\"\n",
    "\n",
    "# Where to save locally\n",
    "out_path = \"crypto_5min_2years.csv\"  # add .zip/.csv/etc if you know the type\n",
    "\n",
    "gdown.download(url, out_path, fuzzy=True)  # fuzzy=True lets you pass 'view' URLs\n",
    "print(\"Saved to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading data from crypto_5min_2years.csv...\n",
      "✅ Raw data shape: (630721, 7)\n",
      "📅 Date range: 2023-09-10 12:15:00 to 2025-09-09 12:15:00\n",
      "💰 Symbols: Unknown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-09-10 12:15:00</th>\n",
       "      <td>BNBUSDT</td>\n",
       "      <td>213.10</td>\n",
       "      <td>213.10</td>\n",
       "      <td>212.90</td>\n",
       "      <td>213.00</td>\n",
       "      <td>385.15700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-10 12:15:00</th>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>25815.18</td>\n",
       "      <td>25815.19</td>\n",
       "      <td>25801.86</td>\n",
       "      <td>25803.16</td>\n",
       "      <td>25.21043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-10 12:15:00</th>\n",
       "      <td>ETHUSDT</td>\n",
       "      <td>1625.90</td>\n",
       "      <td>1625.91</td>\n",
       "      <td>1624.86</td>\n",
       "      <td>1624.86</td>\n",
       "      <td>239.22490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-10 12:20:00</th>\n",
       "      <td>BNBUSDT</td>\n",
       "      <td>213.00</td>\n",
       "      <td>213.10</td>\n",
       "      <td>212.90</td>\n",
       "      <td>213.10</td>\n",
       "      <td>375.34700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-10 12:20:00</th>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>25803.17</td>\n",
       "      <td>25816.67</td>\n",
       "      <td>25803.16</td>\n",
       "      <td>25812.27</td>\n",
       "      <td>18.98413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         tic      open      high       low     close  \\\n",
       "date                                                                   \n",
       "2023-09-10 12:15:00  BNBUSDT    213.10    213.10    212.90    213.00   \n",
       "2023-09-10 12:15:00  BTCUSDT  25815.18  25815.19  25801.86  25803.16   \n",
       "2023-09-10 12:15:00  ETHUSDT   1625.90   1625.91   1624.86   1624.86   \n",
       "2023-09-10 12:20:00  BNBUSDT    213.00    213.10    212.90    213.10   \n",
       "2023-09-10 12:20:00  BTCUSDT  25803.17  25816.67  25803.16  25812.27   \n",
       "\n",
       "                        volume  \n",
       "date                            \n",
       "2023-09-10 12:15:00  385.15700  \n",
       "2023-09-10 12:15:00   25.21043  \n",
       "2023-09-10 12:15:00  239.22490  \n",
       "2023-09-10 12:20:00  375.34700  \n",
       "2023-09-10 12:20:00   18.98413  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 630721 entries, 2023-09-10 12:15:00 to 2025-09-09 12:15:00\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   tic     630721 non-null  object \n",
      " 1   open    630721 non-null  float64\n",
      " 2   high    630721 non-null  float64\n",
      " 3   low     630721 non-null  float64\n",
      " 4   close   630721 non-null  float64\n",
      " 5   volume  630721 non-null  float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 33.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Load cryptocurrency data\n",
    "def load_crypto_data(csv_path='crypto_5min_2years.csv'):\n",
    "    \"\"\"Load and preprocess cryptocurrency data\"\"\"\n",
    "    print(f\"📊 Loading data from {csv_path}...\")\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"❌ File {csv_path} not found!\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"✅ Raw data shape: {df.shape}\")\n",
    "    \n",
    "    # Handle datetime index\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "    elif 'timestamp' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['timestamp'])\n",
    "        df.set_index('date', inplace=True)\n",
    "    else:\n",
    "        # Create datetime index for sample data\n",
    "        if len(df) > 0:\n",
    "            dates = pd.date_range(start='2024-01-01', periods=len(df), freq='5T')\n",
    "            df.index = dates\n",
    "    \n",
    "    print(f\"📅 Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"💰 Symbols: {df['symbol'].unique() if 'symbol' in df.columns else 'Unknown'}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_crypto_data()\n",
    "\n",
    "if df is not None:\n",
    "    display(df.head())\n",
    "    print(f\"\\n📋 Data info:\")\n",
    "    display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Enhanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ TA-Lib not available, using manual calculations\n",
      "🔧 Calculating enhanced features...\n",
      "🔧 Calculating enhanced trading features...\n",
      "  📊 Calculating core technical indicators...\n",
      "  💰 Calculating order flow indicators...\n",
      "  🏛️ Calculating market microstructure features...\n",
      "  📈 Calculating volatility features...\n",
      "  🚀 Calculating momentum and trend features...\n",
      "  📊 Calculating support and resistance levels...\n",
      "  💹 Calculating price-based features...\n",
      "  📦 Calculating volume-based features...\n",
      "  ⏰ Calculating time-based features...\n",
      "  ⏪ Calculating lagged features...\n",
      "✅ Enhanced features calculated: 66 features\n",
      "✅ Enhanced features shape: (630721, 66)\n",
      "🎯 Selecting important features...\n",
      "✅ Selected 29 important features\n",
      "🎯 Selected features shape: (630721, 29)\n",
      "✅ Final processed features shape: (630721, 29)\n",
      "📋 Feature columns: ['open', 'volume', 'rsi', 'rsi_7', 'macd', 'macd_signal', 'bb_upper', 'stoch_k', 'stoch_d', 'vwap']...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>rsi_7</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>bb_upper</th>\n",
       "      <th>stoch_k</th>\n",
       "      <th>stoch_d</th>\n",
       "      <th>vwap</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_change</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_session_open</th>\n",
       "      <th>close_lag_1</th>\n",
       "      <th>volume_lag_1</th>\n",
       "      <th>volume_lag_2</th>\n",
       "      <th>volume_lag_3</th>\n",
       "      <th>volume_lag_5</th>\n",
       "      <th>volume_lag_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-09-10 12:15:00</th>\n",
       "      <td>213.10</td>\n",
       "      <td>385.15700</td>\n",
       "      <td>55.555415</td>\n",
       "      <td>50.000098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34033.495843</td>\n",
       "      <td>99.954147</td>\n",
       "      <td>35.158169</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.934545</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>213.00</td>\n",
       "      <td>385.15700</td>\n",
       "      <td>385.15700</td>\n",
       "      <td>385.15700</td>\n",
       "      <td>385.157</td>\n",
       "      <td>385.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-10 12:15:00</th>\n",
       "      <td>25815.18</td>\n",
       "      <td>25.21043</td>\n",
       "      <td>55.555415</td>\n",
       "      <td>50.000098</td>\n",
       "      <td>574.138205</td>\n",
       "      <td>318.965670</td>\n",
       "      <td>34033.495843</td>\n",
       "      <td>99.954147</td>\n",
       "      <td>35.158169</td>\n",
       "      <td>1785.320461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.934545</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>213.00</td>\n",
       "      <td>385.15700</td>\n",
       "      <td>385.15700</td>\n",
       "      <td>385.15700</td>\n",
       "      <td>385.157</td>\n",
       "      <td>385.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-10 12:15:00</th>\n",
       "      <td>1625.90</td>\n",
       "      <td>239.22490</td>\n",
       "      <td>55.555415</td>\n",
       "      <td>50.000098</td>\n",
       "      <td>-18.201604</td>\n",
       "      <td>180.782361</td>\n",
       "      <td>34033.495843</td>\n",
       "      <td>99.954147</td>\n",
       "      <td>35.158169</td>\n",
       "      <td>1726.356697</td>\n",
       "      <td>...</td>\n",
       "      <td>8.489124</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>25803.16</td>\n",
       "      <td>25.21043</td>\n",
       "      <td>385.15700</td>\n",
       "      <td>385.15700</td>\n",
       "      <td>385.157</td>\n",
       "      <td>385.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-10 12:20:00</th>\n",
       "      <td>213.00</td>\n",
       "      <td>375.34700</td>\n",
       "      <td>55.555415</td>\n",
       "      <td>50.000098</td>\n",
       "      <td>-338.134343</td>\n",
       "      <td>4.997569</td>\n",
       "      <td>34033.495843</td>\n",
       "      <td>99.954147</td>\n",
       "      <td>35.158169</td>\n",
       "      <td>1172.156689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569013</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1624.86</td>\n",
       "      <td>239.22490</td>\n",
       "      <td>25.21043</td>\n",
       "      <td>385.15700</td>\n",
       "      <td>385.157</td>\n",
       "      <td>385.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-10 12:20:00</th>\n",
       "      <td>25803.17</td>\n",
       "      <td>18.98413</td>\n",
       "      <td>55.555415</td>\n",
       "      <td>50.000098</td>\n",
       "      <td>513.787460</td>\n",
       "      <td>156.351059</td>\n",
       "      <td>34033.495843</td>\n",
       "      <td>99.954147</td>\n",
       "      <td>35.158169</td>\n",
       "      <td>1620.217612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.949422</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>213.10</td>\n",
       "      <td>375.34700</td>\n",
       "      <td>239.22490</td>\n",
       "      <td>25.21043</td>\n",
       "      <td>385.157</td>\n",
       "      <td>385.157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open     volume        rsi      rsi_7        macd  \\\n",
       "date                                                                         \n",
       "2023-09-10 12:15:00    213.10  385.15700  55.555415  50.000098    0.000000   \n",
       "2023-09-10 12:15:00  25815.18   25.21043  55.555415  50.000098  574.138205   \n",
       "2023-09-10 12:15:00   1625.90  239.22490  55.555415  50.000098  -18.201604   \n",
       "2023-09-10 12:20:00    213.00  375.34700  55.555415  50.000098 -338.134343   \n",
       "2023-09-10 12:20:00  25803.17   18.98413  55.555415  50.000098  513.787460   \n",
       "\n",
       "                     macd_signal      bb_upper    stoch_k    stoch_d  \\\n",
       "date                                                                   \n",
       "2023-09-10 12:15:00     0.000000  34033.495843  99.954147  35.158169   \n",
       "2023-09-10 12:15:00   318.965670  34033.495843  99.954147  35.158169   \n",
       "2023-09-10 12:15:00   180.782361  34033.495843  99.954147  35.158169   \n",
       "2023-09-10 12:20:00     4.997569  34033.495843  99.954147  35.158169   \n",
       "2023-09-10 12:20:00   156.351059  34033.495843  99.954147  35.158169   \n",
       "\n",
       "                            vwap  ...  volume_change  hour  day_of_week  \\\n",
       "date                              ...                                     \n",
       "2023-09-10 12:15:00   213.000000  ...      -0.934545    12            6   \n",
       "2023-09-10 12:15:00  1785.320461  ...      -0.934545    12            6   \n",
       "2023-09-10 12:15:00  1726.356697  ...       8.489124    12            6   \n",
       "2023-09-10 12:20:00  1172.156689  ...       0.569013    12            6   \n",
       "2023-09-10 12:20:00  1620.217612  ...      -0.949422    12            6   \n",
       "\n",
       "                     is_session_open  close_lag_1  volume_lag_1  volume_lag_2  \\\n",
       "date                                                                            \n",
       "2023-09-10 12:15:00                1       213.00     385.15700     385.15700   \n",
       "2023-09-10 12:15:00                1       213.00     385.15700     385.15700   \n",
       "2023-09-10 12:15:00                1     25803.16      25.21043     385.15700   \n",
       "2023-09-10 12:20:00                1      1624.86     239.22490      25.21043   \n",
       "2023-09-10 12:20:00                1       213.10     375.34700     239.22490   \n",
       "\n",
       "                     volume_lag_3  volume_lag_5  volume_lag_10  \n",
       "date                                                            \n",
       "2023-09-10 12:15:00     385.15700       385.157        385.157  \n",
       "2023-09-10 12:15:00     385.15700       385.157        385.157  \n",
       "2023-09-10 12:15:00     385.15700       385.157        385.157  \n",
       "2023-09-10 12:20:00     385.15700       385.157        385.157  \n",
       "2023-09-10 12:20:00      25.21043       385.157        385.157  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import enhanced features module\n",
    "from enhanced_features import calculate_enhanced_features, select_important_features\n",
    "\n",
    "# Calculate enhanced features\n",
    "def process_features(df):\n",
    "    \"\"\"Process and enhance features for training\"\"\"\n",
    "    print(\"🔧 Calculating enhanced features...\")\n",
    "    \n",
    "    # Calculate enhanced features\n",
    "    enhanced_df = calculate_enhanced_features(df)\n",
    "    print(f\"✅ Enhanced features shape: {enhanced_df.shape}\")\n",
    "    \n",
    "    # Select important features\n",
    "    selected_features = select_important_features(enhanced_df, n_features=40)\n",
    "    print(f\"🎯 Selected features shape: {selected_features.shape}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    selected_features = selected_features.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    print(f\"✅ Final processed features shape: {selected_features.shape}\")\n",
    "    print(f\"📋 Feature columns: {list(selected_features.columns[:10])}...\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "# Process features\n",
    "if df is not None:\n",
    "    features_df = process_features(df)\n",
    "    display(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Enhanced Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Model configuration:\n",
      "   model_params: {'d_model': 512, 'n_heads': 16, 'n_layers': 8, 'd_ff': 2048, 'dropout': 0.15, 'max_seq_len': 250, 'use_multi_scale': True, 'scales': [5, 15, 30, 60]}\n",
      "   training_params: {'learning_rate': 5e-05, 'batch_size': 32, 'n_epochs': 150, 'warmup_steps': 2000, 'weight_decay': 1e-05, 'gradient_clipping': 1.0}\n",
      "   environment_params: {'initial_amount': 100000, 'transaction_cost_pct': 0.001, 'sequence_length': 250, 'use_multi_scale': True}\n",
      "\n",
      "🧠 Model created successfully!\n",
      "📊 Input dimension: 29\n",
      "🔧 Model parameters: 27,374,350\n",
      "💾 Model size: 104.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Import enhanced transformer\n",
    "from transformer_enhanced_v2 import EnhancedCryptoTransformer, create_enhanced_transformer_config\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create model configuration\n",
    "config = create_enhanced_transformer_config()\n",
    "print(\"📋 Model configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Create model\n",
    "if 'features_df' in locals():\n",
    "    input_dim = features_df.shape[1]\n",
    "    model = EnhancedCryptoTransformer(\n",
    "        input_dim=input_dim,\n",
    "        **config['model_params']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"\\n🧠 Model created successfully!\")\n",
    "    print(f\"📊 Input dimension: {input_dim}\")\n",
    "    print(f\"🔧 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"💾 Model size: {sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024:.1f} MB\")\n",
    "else:\n",
    "    print(\"⚠️ Features not available, creating test model\")\n",
    "    model = EnhancedCryptoTransformer(\n",
    "        input_dim=25,\n",
    "        **config['model_params']\n",
    "    ).to(device)\n",
    "    print(f\"🧠 Test model created with 25 input dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing model forward pass...\n",
      "✅ Model forward pass successful!\n",
      "📊 Output shapes:\n",
      "   action: torch.Size([4, 1])\n",
      "   market_regime: torch.Size([4, 4])\n",
      "   confidence: torch.Size([4, 1])\n",
      "   volatility: torch.Size([4, 1])\n",
      "   risk_assessment: torch.Size([4, 3])\n",
      "   hidden_state: torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test model forward pass\n",
    "def test_model(model, input_dim=25):\n",
    "    \"\"\"Test model forward pass\"\"\"\n",
    "    print(\"🧪 Testing model forward pass...\")\n",
    "    \n",
    "    # Create test input\n",
    "    batch_size = 4\n",
    "    seq_len = config['model_params']['max_seq_len']\n",
    "    test_input = torch.randn(batch_size, seq_len, input_dim).to(device)\n",
    "    \n",
    "    # Create multi-scale inputs\n",
    "    scale_inputs = {\n",
    "        5: torch.randn(batch_size, seq_len, input_dim).to(device),\n",
    "        15: torch.randn(batch_size, seq_len//3, input_dim).to(device),\n",
    "        30: torch.randn(batch_size, seq_len//6, input_dim).to(device),\n",
    "    }\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_input, scale_inputs)\n",
    "    \n",
    "    print(\"✅ Model forward pass successful!\")\n",
    "    print(\"📊 Output shapes:\")\n",
    "    for key, value in outputs.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"   {key}: {value.shape}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Test model\n",
    "test_outputs = test_model(model, input_dim if 'features_df' in locals() else 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏋️‍♂️ Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Creating datasets...\n",
      "📊 Training samples: 504372\n",
      "📊 Validation samples: 126094\n",
      "🔧 Batch size: 32\n",
      "🔧 Training batches: 15762\n",
      "🔧 Validation batches: 3941\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "class CryptoDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for cryptocurrency trading\"\"\"\n",
    "    def __init__(self, features_df, sequence_length=250, prediction_horizon=5):\n",
    "        self.features = features_df.values\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.close_prices = features_df['close'].values if 'close' in features_df.columns else self.features[:, 0]\n",
    "        \n",
    "        self.sequences, self.targets = self._prepare_sequences()\n",
    "    \n",
    "    def _prepare_sequences(self):\n",
    "        \"\"\"Prepare training sequences\"\"\"\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(self.features) - self.sequence_length - self.prediction_horizon):\n",
    "            # Input sequence\n",
    "            seq = self.features[i:i + self.sequence_length]\n",
    "            sequences.append(seq)\n",
    "            \n",
    "            # Target (future return)\n",
    "            current_price = self.close_prices[i + self.sequence_length - 1]\n",
    "            future_price = self.close_prices[i + self.sequence_length + self.prediction_horizon - 1]\n",
    "            target_return = (future_price - current_price) / current_price\n",
    "            targets.append(target_return)\n",
    "        \n",
    "        return np.array(sequences), np.array(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.FloatTensor(self.sequences[idx])\n",
    "        target = torch.FloatTensor([self.targets[idx]])\n",
    "        return sequence, target\n",
    "\n",
    "# Create datasets\n",
    "if 'features_df' in locals():\n",
    "    print(\"🔧 Creating datasets...\")\n",
    "    \n",
    "    # Create dataset\n",
    "    full_dataset = CryptoDataset(features_df, sequence_length=config['model_params']['max_seq_len'])\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    print(f\"📊 Training samples: {len(train_dataset)}\")\n",
    "    print(f\"📊 Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    batch_size = config['training_params']['batch_size']\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    print(f\"🔧 Batch size: {batch_size}\")\n",
    "    print(f\"🔧 Training batches: {len(train_loader)}\")\n",
    "    print(f\"🔧 Validation batches: {len(val_loader)}\")\n",
    "else:\n",
    "    print(\"⚠️ Features not available, skipping dataset creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training setup completed!\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress PyTorch warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Initialize training components\n",
    "try:\n",
    "    if 'train_loader' in locals():\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['training_params']['learning_rate'],\n",
    "            weight_decay=config['training_params']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config['training_params']['n_epochs'],\n",
    "            eta_min=config['training_params']['learning_rate'] * 0.1\n",
    "        )\n",
    "        \n",
    "        # Loss functions\n",
    "        action_loss_fn = nn.MSELoss()\n",
    "        confidence_loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # Training history\n",
    "        training_history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'learning_rate': [],\n",
    "            'epoch_time': [],\n",
    "            'gpu_memory': []\n",
    "        }\n",
    "        \n",
    "        print(\"🚀 Training setup completed!\")\n",
    "    else:\n",
    "        print(\"⚠️ Training setup skipped - no datasets available\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error setting up training: {str(e)}\")\n",
    "    print(\"This might be due to PyTorch compatibility issues. Please restart the kernel and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    action_loss = 0\n",
    "    confidence_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "        sequences = sequences.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        \n",
    "        # Calculate losses\n",
    "        action_loss_batch = action_loss_fn(outputs['action'], targets)\n",
    "        confidence_loss_batch = confidence_loss_fn(outputs['confidence'], torch.ones_like(outputs['confidence']) * 0.8)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss_batch = action_loss_batch + 0.2 * confidence_loss_batch\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss_batch.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        total_loss += total_loss_batch.item()\n",
    "        action_loss += action_loss_batch.item()\n",
    "        confidence_loss += confidence_loss_batch.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(train_loader)}: Loss = {total_loss_batch.item():.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss / num_batches,\n",
    "        'action_loss': action_loss / num_batches,\n",
    "        'confidence_loss': confidence_loss / num_batches\n",
    "    }\n",
    "\n",
    "def validate_epoch(model, val_loader, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in val_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            \n",
    "            loss = action_loss_fn(outputs['action'], targets)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "print(\"🔧 Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting enhanced transformer training...\n",
      "📊 Training samples: 504372\n",
      "📊 Validation samples: 126094\n",
      "🧠 Model parameters: 28,473,102\n",
      "🔧 Epochs: 150\n",
      "  Batch 0/15762: Loss = 415.2520\n",
      "  Batch 10/15762: Loss = 279.0759\n",
      "  Batch 20/15762: Loss = 443.4774\n",
      "  Batch 30/15762: Loss = 93.4017\n",
      "  Batch 40/15762: Loss = 259.8585\n",
      "  Batch 50/15762: Loss = 297.1124\n",
      "  Batch 60/15762: Loss = 553.0376\n",
      "  Batch 70/15762: Loss = 85.2219\n",
      "  Batch 80/15762: Loss = 238.8176\n",
      "  Batch 90/15762: Loss = 176.4643\n",
      "  Batch 100/15762: Loss = 194.6840\n",
      "  Batch 110/15762: Loss = 198.8205\n",
      "  Batch 120/15762: Loss = 151.9159\n",
      "  Batch 130/15762: Loss = 231.3333\n",
      "  Batch 140/15762: Loss = 306.1883\n",
      "  Batch 150/15762: Loss = 289.8451\n",
      "  Batch 160/15762: Loss = 124.8691\n",
      "  Batch 170/15762: Loss = 243.0524\n",
      "  Batch 180/15762: Loss = 291.3756\n",
      "  Batch 190/15762: Loss = 146.4964\n",
      "  Batch 200/15762: Loss = 324.4935\n",
      "  Batch 210/15762: Loss = 199.5320\n",
      "  Batch 220/15762: Loss = 225.0054\n",
      "  Batch 230/15762: Loss = 196.0229\n",
      "  Batch 240/15762: Loss = 139.8135\n",
      "  Batch 250/15762: Loss = 262.9635\n",
      "  Batch 260/15762: Loss = 120.1466\n",
      "  Batch 270/15762: Loss = 341.1950\n",
      "  Batch 280/15762: Loss = 217.2810\n",
      "  Batch 290/15762: Loss = 312.9293\n",
      "  Batch 300/15762: Loss = 312.4035\n",
      "  Batch 310/15762: Loss = 300.7779\n",
      "  Batch 320/15762: Loss = 387.0700\n",
      "  Batch 330/15762: Loss = 92.7083\n",
      "  Batch 340/15762: Loss = 156.6255\n",
      "  Batch 350/15762: Loss = 155.4100\n",
      "  Batch 360/15762: Loss = 311.4955\n",
      "  Batch 370/15762: Loss = 227.6573\n",
      "  Batch 380/15762: Loss = 128.8182\n",
      "  Batch 390/15762: Loss = 353.7994\n",
      "  Batch 400/15762: Loss = 250.8913\n",
      "  Batch 410/15762: Loss = 231.9559\n",
      "  Batch 420/15762: Loss = 189.3152\n",
      "  Batch 430/15762: Loss = 213.0771\n",
      "  Batch 440/15762: Loss = 253.5129\n",
      "  Batch 450/15762: Loss = 392.3602\n",
      "  Batch 460/15762: Loss = 276.2674\n",
      "  Batch 470/15762: Loss = 138.3150\n",
      "  Batch 480/15762: Loss = 307.4576\n",
      "  Batch 490/15762: Loss = 309.9413\n",
      "  Batch 500/15762: Loss = 193.2525\n",
      "  Batch 510/15762: Loss = 87.7302\n",
      "  Batch 520/15762: Loss = 208.3815\n",
      "  Batch 530/15762: Loss = 226.8636\n",
      "  Batch 540/15762: Loss = 250.4038\n",
      "  Batch 550/15762: Loss = 142.8297\n",
      "  Batch 560/15762: Loss = 172.8710\n",
      "  Batch 570/15762: Loss = 355.0274\n",
      "  Batch 580/15762: Loss = 170.8740\n",
      "  Batch 590/15762: Loss = 229.4347\n",
      "  Batch 600/15762: Loss = 109.9420\n",
      "  Batch 610/15762: Loss = 222.8938\n",
      "  Batch 620/15762: Loss = 344.1990\n",
      "  Batch 630/15762: Loss = 190.9972\n",
      "  Batch 640/15762: Loss = 387.6223\n",
      "  Batch 650/15762: Loss = 282.8525\n",
      "  Batch 660/15762: Loss = 220.4503\n",
      "  Batch 670/15762: Loss = 237.5200\n",
      "  Batch 680/15762: Loss = 182.6086\n",
      "  Batch 690/15762: Loss = 354.2045\n",
      "  Batch 700/15762: Loss = 120.3921\n",
      "  Batch 710/15762: Loss = 335.5979\n",
      "  Batch 720/15762: Loss = 127.8442\n",
      "  Batch 730/15762: Loss = 172.4137\n",
      "  Batch 740/15762: Loss = 264.4534\n",
      "  Batch 750/15762: Loss = 209.0436\n",
      "  Batch 760/15762: Loss = 178.2645\n",
      "  Batch 770/15762: Loss = 326.5320\n",
      "  Batch 780/15762: Loss = 200.2042\n",
      "  Batch 790/15762: Loss = 371.6530\n",
      "  Batch 800/15762: Loss = 305.8605\n",
      "  Batch 810/15762: Loss = 146.1084\n",
      "  Batch 820/15762: Loss = 173.0711\n",
      "  Batch 830/15762: Loss = 379.9413\n",
      "  Batch 840/15762: Loss = 152.0175\n",
      "  Batch 850/15762: Loss = 140.8421\n",
      "  Batch 860/15762: Loss = 144.2327\n",
      "  Batch 870/15762: Loss = 152.0663\n",
      "  Batch 880/15762: Loss = 296.1758\n",
      "  Batch 890/15762: Loss = 301.2304\n",
      "  Batch 900/15762: Loss = 387.8641\n",
      "  Batch 910/15762: Loss = 147.4164\n",
      "  Batch 920/15762: Loss = 341.4849\n",
      "  Batch 930/15762: Loss = 213.3384\n",
      "  Batch 940/15762: Loss = 367.6825\n",
      "  Batch 950/15762: Loss = 255.6406\n",
      "  Batch 960/15762: Loss = 137.7421\n",
      "  Batch 970/15762: Loss = 353.3917\n",
      "  Batch 980/15762: Loss = 120.5864\n",
      "  Batch 990/15762: Loss = 206.0457\n",
      "  Batch 1000/15762: Loss = 254.4393\n",
      "  Batch 1010/15762: Loss = 159.6405\n",
      "  Batch 1020/15762: Loss = 319.6226\n",
      "  Batch 1030/15762: Loss = 103.7771\n",
      "  Batch 1040/15762: Loss = 94.1382\n",
      "  Batch 1050/15762: Loss = 280.0542\n",
      "  Batch 1060/15762: Loss = 145.7089\n",
      "  Batch 1070/15762: Loss = 344.8710\n",
      "  Batch 1080/15762: Loss = 411.8235\n",
      "  Batch 1090/15762: Loss = 185.8363\n",
      "  Batch 1100/15762: Loss = 197.5310\n",
      "  Batch 1110/15762: Loss = 146.0559\n",
      "  Batch 1120/15762: Loss = 197.9639\n",
      "  Batch 1130/15762: Loss = 162.5434\n",
      "  Batch 1140/15762: Loss = 209.5212\n",
      "  Batch 1150/15762: Loss = 260.6601\n",
      "  Batch 1160/15762: Loss = 326.9968\n",
      "  Batch 1170/15762: Loss = 219.1323\n",
      "  Batch 1180/15762: Loss = 305.5674\n",
      "  Batch 1190/15762: Loss = 197.1686\n",
      "  Batch 1200/15762: Loss = 200.8380\n",
      "  Batch 1210/15762: Loss = 268.0695\n",
      "  Batch 1220/15762: Loss = 280.3775\n",
      "  Batch 1230/15762: Loss = 320.5701\n",
      "  Batch 1240/15762: Loss = 378.5771\n",
      "  Batch 1250/15762: Loss = 226.8223\n",
      "  Batch 1260/15762: Loss = 220.3460\n",
      "  Batch 1270/15762: Loss = 132.5354\n",
      "  Batch 1280/15762: Loss = 257.9575\n",
      "  Batch 1290/15762: Loss = 213.1222\n",
      "  Batch 1300/15762: Loss = 324.8331\n",
      "  Batch 1310/15762: Loss = 139.6710\n",
      "  Batch 1320/15762: Loss = 223.2718\n",
      "  Batch 1330/15762: Loss = 271.3499\n",
      "  Batch 1340/15762: Loss = 296.6789\n",
      "  Batch 1350/15762: Loss = 365.0215\n",
      "  Batch 1360/15762: Loss = 237.8756\n",
      "  Batch 1370/15762: Loss = 395.6440\n",
      "  Batch 1380/15762: Loss = 317.4414\n",
      "  Batch 1390/15762: Loss = 383.0555\n",
      "  Batch 1400/15762: Loss = 221.7421\n",
      "  Batch 1410/15762: Loss = 354.5163\n",
      "  Batch 1420/15762: Loss = 201.1057\n",
      "  Batch 1430/15762: Loss = 275.7668\n",
      "  Batch 1440/15762: Loss = 138.4665\n",
      "  Batch 1450/15762: Loss = 295.1987\n",
      "  Batch 1460/15762: Loss = 344.7052\n",
      "  Batch 1470/15762: Loss = 398.3449\n",
      "  Batch 1480/15762: Loss = 368.0393\n",
      "  Batch 1490/15762: Loss = 177.7055\n",
      "  Batch 1500/15762: Loss = 396.9620\n",
      "  Batch 1510/15762: Loss = 111.8783\n",
      "  Batch 1520/15762: Loss = 189.6931\n",
      "  Batch 1530/15762: Loss = 260.9029\n",
      "  Batch 1540/15762: Loss = 258.8673\n",
      "  Batch 1550/15762: Loss = 507.4750\n",
      "  Batch 1560/15762: Loss = 220.0890\n",
      "  Batch 1570/15762: Loss = 278.5764\n",
      "  Batch 1580/15762: Loss = 173.0359\n",
      "  Batch 1590/15762: Loss = 199.1232\n",
      "  Batch 1600/15762: Loss = 237.9822\n",
      "  Batch 1610/15762: Loss = 218.9972\n",
      "  Batch 1620/15762: Loss = 336.0832\n",
      "  Batch 1630/15762: Loss = 177.2033\n",
      "  Batch 1640/15762: Loss = 216.4248\n",
      "  Batch 1650/15762: Loss = 253.6291\n",
      "  Batch 1660/15762: Loss = 265.2000\n",
      "  Batch 1670/15762: Loss = 287.1557\n",
      "  Batch 1680/15762: Loss = 298.1086\n",
      "  Batch 1690/15762: Loss = 360.1890\n",
      "  Batch 1700/15762: Loss = 284.3300\n",
      "  Batch 1710/15762: Loss = 250.1045\n",
      "  Batch 1720/15762: Loss = 384.5739\n",
      "  Batch 1730/15762: Loss = 242.2062\n",
      "  Batch 1740/15762: Loss = 300.7033\n",
      "  Batch 1750/15762: Loss = 290.1521\n",
      "  Batch 1760/15762: Loss = 200.9376\n",
      "  Batch 1770/15762: Loss = 178.2563\n",
      "  Batch 1780/15762: Loss = 413.9096\n",
      "  Batch 1790/15762: Loss = 226.7155\n",
      "  Batch 1800/15762: Loss = 127.2285\n",
      "  Batch 1810/15762: Loss = 211.8237\n",
      "  Batch 1820/15762: Loss = 289.9530\n",
      "  Batch 1830/15762: Loss = 344.5507\n",
      "  Batch 1840/15762: Loss = 161.5538\n",
      "  Batch 1850/15762: Loss = 387.6998\n",
      "  Batch 1860/15762: Loss = 254.2583\n",
      "  Batch 1870/15762: Loss = 239.1646\n",
      "  Batch 1880/15762: Loss = 186.5988\n",
      "  Batch 1890/15762: Loss = 267.7754\n",
      "  Batch 1900/15762: Loss = 307.6508\n",
      "  Batch 1910/15762: Loss = 205.3167\n",
      "  Batch 1920/15762: Loss = 257.1599\n",
      "  Batch 1930/15762: Loss = 292.3237\n",
      "  Batch 1940/15762: Loss = 182.4462\n",
      "  Batch 1950/15762: Loss = 334.4954\n",
      "  Batch 1960/15762: Loss = 216.9066\n",
      "  Batch 1970/15762: Loss = 392.5872\n",
      "  Batch 1980/15762: Loss = 167.4763\n",
      "  Batch 1990/15762: Loss = 220.4055\n",
      "  Batch 2000/15762: Loss = 256.1500\n",
      "  Batch 2010/15762: Loss = 188.2265\n",
      "  Batch 2020/15762: Loss = 138.6535\n",
      "  Batch 2030/15762: Loss = 313.7790\n",
      "  Batch 2040/15762: Loss = 159.6068\n",
      "  Batch 2050/15762: Loss = 202.0638\n",
      "  Batch 2060/15762: Loss = 205.8026\n",
      "  Batch 2070/15762: Loss = 220.3694\n",
      "  Batch 2080/15762: Loss = 222.9800\n",
      "  Batch 2090/15762: Loss = 332.7827\n",
      "  Batch 2100/15762: Loss = 285.3984\n",
      "  Batch 2110/15762: Loss = 119.9863\n",
      "  Batch 2120/15762: Loss = 273.2021\n",
      "  Batch 2130/15762: Loss = 416.0088\n",
      "  Batch 2140/15762: Loss = 299.0229\n",
      "  Batch 2150/15762: Loss = 149.3340\n",
      "  Batch 2160/15762: Loss = 228.1797\n",
      "  Batch 2170/15762: Loss = 148.6922\n",
      "  Batch 2180/15762: Loss = 176.4402\n",
      "  Batch 2190/15762: Loss = 164.5834\n",
      "  Batch 2200/15762: Loss = 173.9318\n",
      "  Batch 2210/15762: Loss = 400.3718\n",
      "  Batch 2220/15762: Loss = 155.9329\n",
      "  Batch 2230/15762: Loss = 292.4941\n",
      "  Batch 2240/15762: Loss = 268.9217\n",
      "  Batch 2250/15762: Loss = 186.6274\n",
      "  Batch 2260/15762: Loss = 372.6350\n",
      "  Batch 2270/15762: Loss = 248.9698\n",
      "  Batch 2280/15762: Loss = 204.2995\n",
      "  Batch 2290/15762: Loss = 88.1456\n",
      "  Batch 2300/15762: Loss = 114.3301\n",
      "  Batch 2310/15762: Loss = 315.0678\n",
      "  Batch 2320/15762: Loss = 168.3185\n",
      "  Batch 2330/15762: Loss = 305.2512\n",
      "  Batch 2340/15762: Loss = 261.7979\n",
      "  Batch 2350/15762: Loss = 191.6670\n",
      "  Batch 2360/15762: Loss = 222.0108\n",
      "  Batch 2370/15762: Loss = 286.0778\n",
      "  Batch 2380/15762: Loss = 230.6700\n",
      "  Batch 2390/15762: Loss = 209.2651\n",
      "  Batch 2400/15762: Loss = 193.4184\n",
      "  Batch 2410/15762: Loss = 581.1182\n",
      "  Batch 2420/15762: Loss = 388.8381\n",
      "  Batch 2430/15762: Loss = 238.3503\n",
      "  Batch 2440/15762: Loss = 345.1652\n",
      "  Batch 2450/15762: Loss = 313.8141\n",
      "  Batch 2460/15762: Loss = 145.1520\n",
      "  Batch 2470/15762: Loss = 408.3675\n",
      "  Batch 2480/15762: Loss = 264.6064\n",
      "  Batch 2490/15762: Loss = 172.1827\n",
      "  Batch 2500/15762: Loss = 275.7761\n",
      "  Batch 2510/15762: Loss = 315.2968\n",
      "  Batch 2520/15762: Loss = 270.6696\n",
      "  Batch 2530/15762: Loss = 256.5847\n",
      "  Batch 2540/15762: Loss = 299.3133\n",
      "  Batch 2550/15762: Loss = 235.5288\n",
      "  Batch 2560/15762: Loss = 347.0778\n",
      "  Batch 2570/15762: Loss = 348.5410\n",
      "  Batch 2580/15762: Loss = 208.4832\n",
      "  Batch 2590/15762: Loss = 301.3593\n",
      "  Batch 2600/15762: Loss = 197.9994\n",
      "  Batch 2610/15762: Loss = 166.2568\n",
      "  Batch 2620/15762: Loss = 174.7511\n",
      "  Batch 2630/15762: Loss = 165.6045\n",
      "  Batch 2640/15762: Loss = 178.9489\n",
      "  Batch 2650/15762: Loss = 368.6051\n",
      "  Batch 2660/15762: Loss = 173.4143\n",
      "  Batch 2670/15762: Loss = 213.2359\n",
      "  Batch 2680/15762: Loss = 114.8099\n",
      "  Batch 2690/15762: Loss = 383.2607\n",
      "  Batch 2700/15762: Loss = 124.1219\n",
      "  Batch 2710/15762: Loss = 403.7568\n",
      "  Batch 2720/15762: Loss = 242.1492\n",
      "  Batch 2730/15762: Loss = 260.7180\n",
      "  Batch 2740/15762: Loss = 210.4982\n",
      "  Batch 2750/15762: Loss = 279.7724\n",
      "  Batch 2760/15762: Loss = 283.2404\n",
      "  Batch 2770/15762: Loss = 217.7983\n",
      "  Batch 2780/15762: Loss = 164.0685\n",
      "  Batch 2790/15762: Loss = 169.1841\n",
      "  Batch 2800/15762: Loss = 264.3306\n",
      "  Batch 2810/15762: Loss = 113.5157\n",
      "  Batch 2820/15762: Loss = 288.3333\n",
      "  Batch 2830/15762: Loss = 239.9664\n",
      "  Batch 2840/15762: Loss = 300.0015\n",
      "  Batch 2850/15762: Loss = 351.8088\n",
      "  Batch 2860/15762: Loss = 190.9509\n",
      "  Batch 2870/15762: Loss = 164.7896\n",
      "  Batch 2880/15762: Loss = 263.6861\n",
      "  Batch 2890/15762: Loss = 383.0967\n",
      "  Batch 2900/15762: Loss = 197.4475\n",
      "  Batch 2910/15762: Loss = 147.9370\n",
      "  Batch 2920/15762: Loss = 238.6705\n",
      "  Batch 2930/15762: Loss = 230.2726\n",
      "  Batch 2940/15762: Loss = 347.6093\n",
      "  Batch 2950/15762: Loss = 307.4298\n",
      "  Batch 2960/15762: Loss = 194.4641\n",
      "  Batch 2970/15762: Loss = 322.4535\n",
      "  Batch 2980/15762: Loss = 78.0371\n",
      "  Batch 2990/15762: Loss = 141.1393\n",
      "  Batch 3000/15762: Loss = 143.6624\n",
      "  Batch 3010/15762: Loss = 252.8926\n",
      "  Batch 3020/15762: Loss = 194.9631\n",
      "  Batch 3030/15762: Loss = 113.7007\n",
      "  Batch 3040/15762: Loss = 209.6555\n",
      "  Batch 3050/15762: Loss = 178.1142\n",
      "  Batch 3060/15762: Loss = 113.5794\n",
      "  Batch 3070/15762: Loss = 404.6808\n",
      "  Batch 3080/15762: Loss = 261.0807\n",
      "  Batch 3090/15762: Loss = 186.2420\n",
      "  Batch 3100/15762: Loss = 249.3505\n",
      "  Batch 3110/15762: Loss = 266.3047\n",
      "  Batch 3120/15762: Loss = 200.9245\n",
      "  Batch 3130/15762: Loss = 193.5824\n",
      "  Batch 3140/15762: Loss = 257.8041\n",
      "  Batch 3150/15762: Loss = 305.0295\n",
      "  Batch 3160/15762: Loss = 195.9303\n",
      "  Batch 3170/15762: Loss = 230.9236\n",
      "  Batch 3180/15762: Loss = 100.4124\n",
      "  Batch 3190/15762: Loss = 277.2645\n",
      "  Batch 3200/15762: Loss = 106.6040\n",
      "  Batch 3210/15762: Loss = 148.7998\n",
      "  Batch 3220/15762: Loss = 285.6428\n",
      "  Batch 3230/15762: Loss = 267.7028\n",
      "  Batch 3240/15762: Loss = 468.1355\n",
      "  Batch 3250/15762: Loss = 346.5973\n",
      "  Batch 3260/15762: Loss = 284.8777\n",
      "  Batch 3270/15762: Loss = 232.1066\n",
      "  Batch 3280/15762: Loss = 193.8680\n",
      "  Batch 3290/15762: Loss = 277.6315\n",
      "  Batch 3300/15762: Loss = 180.9839\n",
      "  Batch 3310/15762: Loss = 281.7022\n",
      "  Batch 3320/15762: Loss = 244.8711\n",
      "  Batch 3330/15762: Loss = 291.3966\n",
      "  Batch 3340/15762: Loss = 220.2152\n",
      "  Batch 3350/15762: Loss = 359.6294\n",
      "  Batch 3360/15762: Loss = 224.0925\n",
      "  Batch 3370/15762: Loss = 196.8661\n",
      "  Batch 3380/15762: Loss = 197.1047\n",
      "  Batch 3390/15762: Loss = 288.7133\n",
      "  Batch 3400/15762: Loss = 165.9596\n",
      "  Batch 3410/15762: Loss = 274.6644\n",
      "  Batch 3420/15762: Loss = 157.3620\n",
      "  Batch 3430/15762: Loss = 120.7673\n",
      "  Batch 3440/15762: Loss = 372.4786\n",
      "  Batch 3450/15762: Loss = 142.9594\n",
      "  Batch 3460/15762: Loss = 368.0435\n",
      "  Batch 3470/15762: Loss = 284.2870\n",
      "  Batch 3480/15762: Loss = 149.2614\n",
      "  Batch 3490/15762: Loss = 185.5572\n",
      "  Batch 3500/15762: Loss = 289.6481\n",
      "  Batch 3510/15762: Loss = 325.2783\n",
      "  Batch 3520/15762: Loss = 346.6848\n",
      "  Batch 3530/15762: Loss = 138.5289\n",
      "  Batch 3540/15762: Loss = 158.6233\n",
      "  Batch 3550/15762: Loss = 211.9202\n",
      "  Batch 3560/15762: Loss = 204.0635\n",
      "  Batch 3570/15762: Loss = 211.9359\n",
      "  Batch 3580/15762: Loss = 244.8524\n",
      "  Batch 3590/15762: Loss = 213.1854\n",
      "  Batch 3600/15762: Loss = 206.1350\n",
      "  Batch 3610/15762: Loss = 224.6526\n",
      "  Batch 3620/15762: Loss = 501.5214\n",
      "  Batch 3630/15762: Loss = 237.3492\n",
      "  Batch 3640/15762: Loss = 392.8702\n",
      "  Batch 3650/15762: Loss = 131.1880\n",
      "  Batch 3660/15762: Loss = 319.8276\n",
      "  Batch 3670/15762: Loss = 74.3313\n",
      "  Batch 3680/15762: Loss = 197.5324\n",
      "  Batch 3690/15762: Loss = 258.3173\n",
      "  Batch 3700/15762: Loss = 278.9609\n",
      "  Batch 3710/15762: Loss = 295.5536\n",
      "  Batch 3720/15762: Loss = 295.3761\n",
      "  Batch 3730/15762: Loss = 301.4474\n",
      "  Batch 3740/15762: Loss = 383.7853\n",
      "  Batch 3750/15762: Loss = 195.0106\n",
      "  Batch 3760/15762: Loss = 417.9201\n",
      "  Batch 3770/15762: Loss = 273.8987\n",
      "  Batch 3780/15762: Loss = 377.3674\n",
      "  Batch 3790/15762: Loss = 247.7220\n",
      "  Batch 3800/15762: Loss = 198.9342\n",
      "  Batch 3810/15762: Loss = 240.6766\n",
      "  Batch 3820/15762: Loss = 220.6397\n",
      "  Batch 3830/15762: Loss = 136.3013\n",
      "  Batch 3840/15762: Loss = 339.7403\n",
      "  Batch 3850/15762: Loss = 271.4189\n",
      "  Batch 3860/15762: Loss = 315.3209\n",
      "  Batch 3870/15762: Loss = 357.6184\n",
      "  Batch 3880/15762: Loss = 231.6504\n",
      "  Batch 3890/15762: Loss = 132.9788\n",
      "  Batch 3900/15762: Loss = 148.6461\n",
      "  Batch 3910/15762: Loss = 355.9046\n",
      "  Batch 3920/15762: Loss = 165.4051\n",
      "  Batch 3930/15762: Loss = 182.2809\n",
      "  Batch 3940/15762: Loss = 253.9702\n",
      "  Batch 3950/15762: Loss = 235.8532\n",
      "  Batch 3960/15762: Loss = 231.2327\n",
      "  Batch 3970/15762: Loss = 275.9811\n",
      "  Batch 3980/15762: Loss = 71.5448\n",
      "  Batch 3990/15762: Loss = 203.4048\n",
      "  Batch 4000/15762: Loss = 289.2076\n",
      "  Batch 4010/15762: Loss = 206.2095\n",
      "  Batch 4020/15762: Loss = 159.1121\n",
      "  Batch 4030/15762: Loss = 198.5003\n",
      "  Batch 4040/15762: Loss = 246.6171\n",
      "  Batch 4050/15762: Loss = 121.6039\n",
      "  Batch 4060/15762: Loss = 260.0472\n",
      "  Batch 4070/15762: Loss = 417.4676\n",
      "  Batch 4080/15762: Loss = 362.8123\n",
      "  Batch 4090/15762: Loss = 304.8777\n",
      "  Batch 4100/15762: Loss = 353.4728\n",
      "  Batch 4110/15762: Loss = 336.6887\n",
      "  Batch 4120/15762: Loss = 168.7450\n",
      "  Batch 4130/15762: Loss = 292.0406\n",
      "  Batch 4140/15762: Loss = 211.3693\n",
      "  Batch 4150/15762: Loss = 347.9085\n",
      "  Batch 4160/15762: Loss = 97.4542\n",
      "  Batch 4170/15762: Loss = 231.7169\n",
      "  Batch 4180/15762: Loss = 153.5091\n",
      "  Batch 4190/15762: Loss = 267.8181\n",
      "  Batch 4200/15762: Loss = 249.0555\n",
      "  Batch 4210/15762: Loss = 144.0346\n",
      "  Batch 4220/15762: Loss = 318.1514\n",
      "  Batch 4230/15762: Loss = 262.0812\n",
      "  Batch 4240/15762: Loss = 262.3008\n",
      "  Batch 4250/15762: Loss = 204.4928\n",
      "  Batch 4260/15762: Loss = 161.9174\n",
      "  Batch 4270/15762: Loss = 141.9765\n",
      "  Batch 4280/15762: Loss = 428.1199\n",
      "  Batch 4290/15762: Loss = 285.0045\n",
      "  Batch 4300/15762: Loss = 220.9125\n",
      "  Batch 4310/15762: Loss = 324.2071\n",
      "  Batch 4320/15762: Loss = 123.8591\n",
      "  Batch 4330/15762: Loss = 152.3976\n",
      "  Batch 4340/15762: Loss = 210.4937\n",
      "  Batch 4350/15762: Loss = 236.5131\n",
      "  Batch 4360/15762: Loss = 356.5760\n",
      "  Batch 4370/15762: Loss = 180.5334\n",
      "  Batch 4380/15762: Loss = 191.1104\n",
      "  Batch 4390/15762: Loss = 412.7012\n",
      "  Batch 4400/15762: Loss = 380.8239\n",
      "  Batch 4410/15762: Loss = 123.1322\n",
      "  Batch 4420/15762: Loss = 203.0464\n",
      "  Batch 4430/15762: Loss = 176.1000\n",
      "  Batch 4440/15762: Loss = 327.2800\n",
      "  Batch 4450/15762: Loss = 234.6860\n",
      "  Batch 4460/15762: Loss = 237.2160\n",
      "  Batch 4470/15762: Loss = 311.5513\n",
      "  Batch 4480/15762: Loss = 267.4389\n",
      "  Batch 4490/15762: Loss = 339.3387\n",
      "  Batch 4500/15762: Loss = 249.1612\n",
      "  Batch 4510/15762: Loss = 176.0966\n",
      "  Batch 4520/15762: Loss = 153.9874\n",
      "  Batch 4530/15762: Loss = 370.7084\n",
      "  Batch 4540/15762: Loss = 116.4627\n",
      "  Batch 4550/15762: Loss = 202.6442\n",
      "  Batch 4560/15762: Loss = 241.6374\n",
      "  Batch 4570/15762: Loss = 319.0099\n",
      "  Batch 4580/15762: Loss = 165.4928\n",
      "  Batch 4590/15762: Loss = 246.1533\n",
      "  Batch 4600/15762: Loss = 296.3530\n",
      "  Batch 4610/15762: Loss = 189.2078\n",
      "  Batch 4620/15762: Loss = 274.7761\n",
      "  Batch 4630/15762: Loss = 258.4675\n",
      "  Batch 4640/15762: Loss = 192.6915\n",
      "  Batch 4650/15762: Loss = 265.1938\n",
      "  Batch 4660/15762: Loss = 168.5625\n",
      "  Batch 4670/15762: Loss = 201.9103\n",
      "  Batch 4680/15762: Loss = 177.8587\n",
      "  Batch 4690/15762: Loss = 80.5161\n",
      "  Batch 4700/15762: Loss = 155.3341\n",
      "  Batch 4710/15762: Loss = 263.4649\n",
      "  Batch 4720/15762: Loss = 182.7724\n",
      "  Batch 4730/15762: Loss = 284.2366\n",
      "  Batch 4740/15762: Loss = 266.0383\n",
      "  Batch 4750/15762: Loss = 264.4267\n",
      "  Batch 4760/15762: Loss = 338.5998\n",
      "  Batch 4770/15762: Loss = 231.5410\n",
      "  Batch 4780/15762: Loss = 179.1859\n",
      "  Batch 4790/15762: Loss = 347.5465\n",
      "  Batch 4800/15762: Loss = 185.9427\n",
      "  Batch 4810/15762: Loss = 209.4303\n",
      "  Batch 4820/15762: Loss = 270.8572\n",
      "  Batch 4830/15762: Loss = 318.1552\n",
      "  Batch 4840/15762: Loss = 266.1515\n",
      "  Batch 4850/15762: Loss = 103.4130\n",
      "  Batch 4860/15762: Loss = 154.9922\n",
      "  Batch 4870/15762: Loss = 170.9036\n",
      "  Batch 4880/15762: Loss = 256.4688\n",
      "  Batch 4890/15762: Loss = 197.4149\n",
      "  Batch 4900/15762: Loss = 187.2193\n",
      "  Batch 4910/15762: Loss = 122.5398\n",
      "  Batch 4920/15762: Loss = 293.7424\n",
      "  Batch 4930/15762: Loss = 126.9913\n",
      "  Batch 4940/15762: Loss = 118.5141\n",
      "  Batch 4950/15762: Loss = 154.4548\n",
      "  Batch 4960/15762: Loss = 325.2461\n",
      "  Batch 4970/15762: Loss = 212.3025\n",
      "  Batch 4980/15762: Loss = 164.6806\n",
      "  Batch 4990/15762: Loss = 211.4452\n",
      "  Batch 5000/15762: Loss = 210.1814\n",
      "  Batch 5010/15762: Loss = 270.0825\n",
      "  Batch 5020/15762: Loss = 221.3028\n",
      "  Batch 5030/15762: Loss = 267.2471\n",
      "  Batch 5040/15762: Loss = 435.7880\n",
      "  Batch 5050/15762: Loss = 232.3319\n",
      "  Batch 5060/15762: Loss = 291.4479\n",
      "  Batch 5070/15762: Loss = 199.1956\n",
      "  Batch 5080/15762: Loss = 297.1815\n",
      "  Batch 5090/15762: Loss = 251.3018\n",
      "  Batch 5100/15762: Loss = 202.2727\n",
      "  Batch 5110/15762: Loss = 272.1925\n",
      "  Batch 5120/15762: Loss = 179.8824\n",
      "  Batch 5130/15762: Loss = 194.1468\n",
      "  Batch 5140/15762: Loss = 261.1600\n",
      "  Batch 5150/15762: Loss = 304.0947\n",
      "  Batch 5160/15762: Loss = 223.2655\n",
      "  Batch 5170/15762: Loss = 154.6935\n",
      "  Batch 5180/15762: Loss = 205.3808\n",
      "  Batch 5190/15762: Loss = 154.7355\n",
      "  Batch 5200/15762: Loss = 199.7215\n",
      "  Batch 5210/15762: Loss = 177.1042\n",
      "  Batch 5220/15762: Loss = 360.8592\n",
      "  Batch 5230/15762: Loss = 329.7497\n",
      "  Batch 5240/15762: Loss = 426.3274\n",
      "  Batch 5250/15762: Loss = 250.5740\n",
      "  Batch 5260/15762: Loss = 241.0005\n",
      "  Batch 5270/15762: Loss = 253.6237\n",
      "  Batch 5280/15762: Loss = 297.3705\n",
      "  Batch 5290/15762: Loss = 375.5971\n",
      "  Batch 5300/15762: Loss = 204.1489\n",
      "  Batch 5310/15762: Loss = 358.6495\n",
      "  Batch 5320/15762: Loss = 484.5807\n",
      "  Batch 5330/15762: Loss = 66.7574\n",
      "  Batch 5340/15762: Loss = 312.5599\n",
      "  Batch 5350/15762: Loss = 218.6592\n",
      "  Batch 5360/15762: Loss = 264.0099\n",
      "  Batch 5370/15762: Loss = 513.4061\n",
      "  Batch 5380/15762: Loss = 237.2894\n",
      "  Batch 5390/15762: Loss = 190.7113\n",
      "  Batch 5400/15762: Loss = 89.8317\n",
      "  Batch 5410/15762: Loss = 218.8324\n",
      "  Batch 5420/15762: Loss = 211.4001\n",
      "  Batch 5430/15762: Loss = 109.1600\n",
      "  Batch 5440/15762: Loss = 267.6927\n",
      "  Batch 5450/15762: Loss = 195.7768\n",
      "  Batch 5460/15762: Loss = 239.3130\n",
      "  Batch 5470/15762: Loss = 286.2790\n",
      "  Batch 5480/15762: Loss = 160.6371\n",
      "  Batch 5490/15762: Loss = 189.0045\n",
      "  Batch 5500/15762: Loss = 188.2508\n",
      "  Batch 5510/15762: Loss = 281.2755\n",
      "  Batch 5520/15762: Loss = 167.8486\n",
      "  Batch 5530/15762: Loss = 108.0993\n",
      "  Batch 5540/15762: Loss = 190.7342\n",
      "  Batch 5550/15762: Loss = 290.8897\n",
      "  Batch 5560/15762: Loss = 208.6169\n",
      "  Batch 5570/15762: Loss = 199.8652\n",
      "  Batch 5580/15762: Loss = 163.9127\n",
      "  Batch 5590/15762: Loss = 416.1122\n",
      "  Batch 5600/15762: Loss = 325.4429\n",
      "  Batch 5610/15762: Loss = 254.7362\n",
      "  Batch 5620/15762: Loss = 275.9187\n",
      "  Batch 5630/15762: Loss = 328.6755\n",
      "  Batch 5640/15762: Loss = 293.2628\n",
      "  Batch 5650/15762: Loss = 217.3382\n",
      "  Batch 5660/15762: Loss = 87.9803\n",
      "  Batch 5670/15762: Loss = 213.0408\n",
      "  Batch 5680/15762: Loss = 502.1606\n",
      "  Batch 5690/15762: Loss = 325.0042\n",
      "  Batch 5700/15762: Loss = 227.8681\n",
      "  Batch 5710/15762: Loss = 218.3593\n",
      "  Batch 5720/15762: Loss = 256.2534\n",
      "  Batch 5730/15762: Loss = 383.0714\n",
      "  Batch 5740/15762: Loss = 179.7897\n",
      "  Batch 5750/15762: Loss = 292.7525\n",
      "  Batch 5760/15762: Loss = 210.3175\n",
      "  Batch 5770/15762: Loss = 217.1817\n",
      "  Batch 5780/15762: Loss = 216.7426\n",
      "  Batch 5790/15762: Loss = 259.6386\n",
      "  Batch 5800/15762: Loss = 404.2841\n",
      "  Batch 5810/15762: Loss = 149.2460\n",
      "  Batch 5820/15762: Loss = 154.5284\n",
      "  Batch 5830/15762: Loss = 329.0313\n",
      "  Batch 5840/15762: Loss = 222.4588\n",
      "  Batch 5850/15762: Loss = 258.9461\n",
      "  Batch 5860/15762: Loss = 238.4994\n",
      "  Batch 5870/15762: Loss = 114.9210\n",
      "  Batch 5880/15762: Loss = 168.8392\n",
      "  Batch 5890/15762: Loss = 246.7796\n",
      "  Batch 5900/15762: Loss = 148.7078\n",
      "  Batch 5910/15762: Loss = 308.6624\n",
      "  Batch 5920/15762: Loss = 246.2569\n",
      "  Batch 5930/15762: Loss = 106.0540\n",
      "  Batch 5940/15762: Loss = 342.7931\n",
      "  Batch 5950/15762: Loss = 126.2405\n",
      "  Batch 5960/15762: Loss = 314.7619\n",
      "  Batch 5970/15762: Loss = 214.3906\n",
      "  Batch 5980/15762: Loss = 266.5650\n",
      "  Batch 5990/15762: Loss = 312.2204\n",
      "  Batch 6000/15762: Loss = 318.6473\n",
      "  Batch 6010/15762: Loss = 74.4572\n",
      "  Batch 6020/15762: Loss = 337.1239\n",
      "  Batch 6030/15762: Loss = 191.7514\n",
      "  Batch 6040/15762: Loss = 149.6037\n",
      "  Batch 6050/15762: Loss = 162.9042\n",
      "  Batch 6060/15762: Loss = 126.7125\n",
      "  Batch 6070/15762: Loss = 316.0031\n",
      "  Batch 6080/15762: Loss = 202.6280\n",
      "  Batch 6090/15762: Loss = 243.5769\n",
      "  Batch 6100/15762: Loss = 263.3086\n",
      "  Batch 6110/15762: Loss = 101.1188\n",
      "  Batch 6120/15762: Loss = 185.7650\n",
      "  Batch 6130/15762: Loss = 219.7770\n",
      "  Batch 6140/15762: Loss = 335.6048\n",
      "  Batch 6150/15762: Loss = 289.1393\n",
      "  Batch 6160/15762: Loss = 292.6409\n",
      "  Batch 6170/15762: Loss = 279.0068\n",
      "  Batch 6180/15762: Loss = 244.6843\n",
      "  Batch 6190/15762: Loss = 216.2187\n",
      "  Batch 6200/15762: Loss = 244.5740\n",
      "  Batch 6210/15762: Loss = 300.8098\n",
      "  Batch 6220/15762: Loss = 141.1626\n",
      "  Batch 6230/15762: Loss = 275.2766\n",
      "  Batch 6240/15762: Loss = 335.5910\n",
      "  Batch 6250/15762: Loss = 253.8270\n",
      "  Batch 6260/15762: Loss = 293.3903\n",
      "  Batch 6270/15762: Loss = 265.0826\n",
      "  Batch 6280/15762: Loss = 196.0321\n",
      "  Batch 6290/15762: Loss = 402.5677\n",
      "  Batch 6300/15762: Loss = 246.6532\n",
      "  Batch 6310/15762: Loss = 274.1734\n",
      "  Batch 6320/15762: Loss = 354.1525\n",
      "  Batch 6330/15762: Loss = 228.5491\n",
      "  Batch 6340/15762: Loss = 162.1620\n",
      "  Batch 6350/15762: Loss = 380.2470\n",
      "  Batch 6360/15762: Loss = 224.4620\n",
      "  Batch 6370/15762: Loss = 222.6652\n",
      "  Batch 6380/15762: Loss = 245.4382\n",
      "  Batch 6390/15762: Loss = 263.3600\n",
      "  Batch 6400/15762: Loss = 219.6196\n",
      "  Batch 6410/15762: Loss = 400.9858\n",
      "  Batch 6420/15762: Loss = 373.0293\n",
      "  Batch 6430/15762: Loss = 149.2182\n",
      "  Batch 6440/15762: Loss = 243.0087\n",
      "  Batch 6450/15762: Loss = 230.9353\n",
      "  Batch 6460/15762: Loss = 231.8420\n",
      "  Batch 6470/15762: Loss = 174.6529\n",
      "  Batch 6480/15762: Loss = 268.9001\n",
      "  Batch 6490/15762: Loss = 183.3888\n",
      "  Batch 6500/15762: Loss = 302.0383\n",
      "  Batch 6510/15762: Loss = 217.0766\n",
      "  Batch 6520/15762: Loss = 317.5001\n",
      "  Batch 6530/15762: Loss = 330.9402\n",
      "  Batch 6540/15762: Loss = 298.4722\n",
      "  Batch 6550/15762: Loss = 474.6099\n",
      "  Batch 6560/15762: Loss = 178.6295\n",
      "  Batch 6570/15762: Loss = 210.9353\n",
      "  Batch 6580/15762: Loss = 156.0869\n",
      "  Batch 6590/15762: Loss = 296.2165\n",
      "  Batch 6600/15762: Loss = 239.1643\n",
      "  Batch 6610/15762: Loss = 255.0492\n",
      "  Batch 6620/15762: Loss = 291.9382\n",
      "  Batch 6630/15762: Loss = 194.9531\n",
      "  Batch 6640/15762: Loss = 211.2373\n",
      "  Batch 6650/15762: Loss = 138.5734\n",
      "  Batch 6660/15762: Loss = 96.7310\n",
      "  Batch 6670/15762: Loss = 354.6385\n",
      "  Batch 6680/15762: Loss = 247.6046\n",
      "  Batch 6690/15762: Loss = 340.3865\n",
      "  Batch 6700/15762: Loss = 415.9204\n",
      "  Batch 6710/15762: Loss = 157.6098\n",
      "  Batch 6720/15762: Loss = 135.7890\n",
      "  Batch 6730/15762: Loss = 225.5201\n",
      "  Batch 6740/15762: Loss = 129.7568\n",
      "  Batch 6750/15762: Loss = 100.1776\n",
      "  Batch 6760/15762: Loss = 386.4500\n",
      "  Batch 6770/15762: Loss = 223.4797\n",
      "  Batch 6780/15762: Loss = 182.1938\n",
      "  Batch 6790/15762: Loss = 366.6634\n",
      "  Batch 6800/15762: Loss = 268.3918\n",
      "  Batch 6810/15762: Loss = 204.6470\n",
      "  Batch 6820/15762: Loss = 289.1345\n",
      "  Batch 6830/15762: Loss = 307.2216\n",
      "  Batch 6840/15762: Loss = 187.0925\n",
      "  Batch 6850/15762: Loss = 51.9146\n",
      "  Batch 6860/15762: Loss = 271.1012\n",
      "  Batch 6870/15762: Loss = 187.2129\n",
      "  Batch 6880/15762: Loss = 373.9075\n",
      "  Batch 6890/15762: Loss = 258.3984\n",
      "  Batch 6900/15762: Loss = 281.1078\n",
      "  Batch 6910/15762: Loss = 367.7542\n",
      "  Batch 6920/15762: Loss = 335.8690\n",
      "  Batch 6930/15762: Loss = 107.1296\n",
      "  Batch 6940/15762: Loss = 214.2516\n",
      "  Batch 6950/15762: Loss = 162.3473\n",
      "  Batch 6960/15762: Loss = 233.4322\n",
      "  Batch 6970/15762: Loss = 91.9973\n",
      "  Batch 6980/15762: Loss = 253.4005\n",
      "  Batch 6990/15762: Loss = 231.8303\n",
      "  Batch 7000/15762: Loss = 149.4591\n",
      "  Batch 7010/15762: Loss = 120.8804\n",
      "  Batch 7020/15762: Loss = 192.4382\n",
      "  Batch 7030/15762: Loss = 444.8811\n",
      "  Batch 7040/15762: Loss = 478.8485\n",
      "  Batch 7050/15762: Loss = 131.0874\n",
      "  Batch 7060/15762: Loss = 256.4694\n",
      "  Batch 7070/15762: Loss = 222.1624\n",
      "  Batch 7080/15762: Loss = 430.2166\n",
      "  Batch 7090/15762: Loss = 212.6465\n",
      "  Batch 7100/15762: Loss = 182.4059\n",
      "  Batch 7110/15762: Loss = 233.1447\n",
      "  Batch 7120/15762: Loss = 137.8841\n",
      "  Batch 7130/15762: Loss = 367.6166\n",
      "  Batch 7140/15762: Loss = 303.5519\n",
      "  Batch 7150/15762: Loss = 297.7482\n",
      "  Batch 7160/15762: Loss = 253.0106\n",
      "  Batch 7170/15762: Loss = 169.7722\n",
      "  Batch 7180/15762: Loss = 258.3470\n",
      "  Batch 7190/15762: Loss = 258.7861\n",
      "  Batch 7200/15762: Loss = 158.7790\n",
      "  Batch 7210/15762: Loss = 319.3690\n",
      "  Batch 7220/15762: Loss = 297.3730\n",
      "  Batch 7230/15762: Loss = 220.2723\n",
      "  Batch 7240/15762: Loss = 312.6320\n",
      "  Batch 7250/15762: Loss = 261.5771\n",
      "  Batch 7260/15762: Loss = 284.6404\n",
      "  Batch 7270/15762: Loss = 335.2337\n",
      "  Batch 7280/15762: Loss = 209.5187\n",
      "  Batch 7290/15762: Loss = 186.7929\n",
      "  Batch 7300/15762: Loss = 303.9443\n",
      "  Batch 7310/15762: Loss = 140.0788\n",
      "  Batch 7320/15762: Loss = 224.8181\n",
      "  Batch 7330/15762: Loss = 98.0716\n",
      "  Batch 7340/15762: Loss = 187.7967\n",
      "  Batch 7350/15762: Loss = 235.6222\n",
      "  Batch 7360/15762: Loss = 181.3348\n",
      "  Batch 7370/15762: Loss = 243.8210\n",
      "  Batch 7380/15762: Loss = 279.4934\n",
      "  Batch 7390/15762: Loss = 76.8520\n",
      "  Batch 7400/15762: Loss = 231.1311\n",
      "  Batch 7410/15762: Loss = 187.1383\n",
      "  Batch 7420/15762: Loss = 307.4203\n",
      "  Batch 7430/15762: Loss = 245.9374\n",
      "  Batch 7440/15762: Loss = 233.0756\n",
      "  Batch 7450/15762: Loss = 411.7620\n",
      "  Batch 7460/15762: Loss = 74.8188\n",
      "  Batch 7470/15762: Loss = 293.4095\n",
      "  Batch 7480/15762: Loss = 265.3892\n",
      "  Batch 7490/15762: Loss = 88.3508\n",
      "  Batch 7500/15762: Loss = 331.8110\n",
      "  Batch 7510/15762: Loss = 496.8116\n",
      "  Batch 7520/15762: Loss = 170.5733\n",
      "  Batch 7530/15762: Loss = 136.2096\n",
      "  Batch 7540/15762: Loss = 247.8665\n",
      "  Batch 7550/15762: Loss = 230.7859\n",
      "  Batch 7560/15762: Loss = 232.3839\n",
      "  Batch 7570/15762: Loss = 385.4504\n",
      "  Batch 7580/15762: Loss = 187.9883\n",
      "  Batch 7590/15762: Loss = 487.3152\n",
      "  Batch 7600/15762: Loss = 199.3276\n",
      "  Batch 7610/15762: Loss = 106.0367\n",
      "  Batch 7620/15762: Loss = 326.5783\n",
      "  Batch 7630/15762: Loss = 227.7419\n",
      "  Batch 7640/15762: Loss = 265.3203\n",
      "  Batch 7650/15762: Loss = 161.6497\n",
      "  Batch 7660/15762: Loss = 245.3617\n",
      "  Batch 7670/15762: Loss = 249.0122\n",
      "  Batch 7680/15762: Loss = 183.9427\n",
      "  Batch 7690/15762: Loss = 175.7965\n",
      "  Batch 7700/15762: Loss = 127.1410\n",
      "  Batch 7710/15762: Loss = 328.9254\n",
      "  Batch 7720/15762: Loss = 254.0124\n",
      "  Batch 7730/15762: Loss = 368.9685\n",
      "  Batch 7740/15762: Loss = 241.9633\n",
      "  Batch 7750/15762: Loss = 270.6009\n",
      "  Batch 7760/15762: Loss = 275.9378\n",
      "  Batch 7770/15762: Loss = 199.3834\n",
      "  Batch 7780/15762: Loss = 275.9752\n",
      "  Batch 7790/15762: Loss = 258.9940\n",
      "  Batch 7800/15762: Loss = 136.1842\n",
      "  Batch 7810/15762: Loss = 196.1275\n",
      "  Batch 7820/15762: Loss = 223.7625\n",
      "  Batch 7830/15762: Loss = 232.7817\n",
      "  Batch 7840/15762: Loss = 233.9594\n",
      "  Batch 7850/15762: Loss = 279.4427\n",
      "  Batch 7860/15762: Loss = 67.1328\n",
      "  Batch 7870/15762: Loss = 337.1136\n",
      "  Batch 7880/15762: Loss = 271.2968\n",
      "  Batch 7890/15762: Loss = 179.8847\n",
      "  Batch 7900/15762: Loss = 238.0857\n",
      "  Batch 7910/15762: Loss = 273.0130\n",
      "  Batch 7920/15762: Loss = 253.5367\n",
      "  Batch 7930/15762: Loss = 189.7069\n",
      "  Batch 7940/15762: Loss = 245.6797\n",
      "  Batch 7950/15762: Loss = 216.1531\n",
      "  Batch 7960/15762: Loss = 134.1577\n",
      "  Batch 7970/15762: Loss = 244.0070\n",
      "  Batch 7980/15762: Loss = 124.2850\n",
      "  Batch 7990/15762: Loss = 283.6603\n",
      "  Batch 8000/15762: Loss = 132.7281\n",
      "  Batch 8010/15762: Loss = 252.8759\n",
      "  Batch 8020/15762: Loss = 264.5493\n",
      "  Batch 8030/15762: Loss = 291.0951\n",
      "  Batch 8040/15762: Loss = 227.0265\n",
      "  Batch 8050/15762: Loss = 135.0139\n",
      "  Batch 8060/15762: Loss = 265.4879\n",
      "  Batch 8070/15762: Loss = 289.7546\n",
      "  Batch 8080/15762: Loss = 273.0203\n",
      "  Batch 8090/15762: Loss = 255.5252\n",
      "  Batch 8100/15762: Loss = 312.7526\n",
      "  Batch 8110/15762: Loss = 316.5176\n",
      "  Batch 8120/15762: Loss = 343.3065\n",
      "  Batch 8130/15762: Loss = 286.4088\n",
      "  Batch 8140/15762: Loss = 221.2952\n",
      "  Batch 8150/15762: Loss = 240.9018\n",
      "  Batch 8160/15762: Loss = 196.5753\n",
      "  Batch 8170/15762: Loss = 443.7456\n",
      "  Batch 8180/15762: Loss = 366.6054\n",
      "  Batch 8190/15762: Loss = 102.1202\n",
      "  Batch 8200/15762: Loss = 358.1801\n",
      "  Batch 8210/15762: Loss = 331.7368\n",
      "  Batch 8220/15762: Loss = 225.4424\n",
      "  Batch 8230/15762: Loss = 336.1025\n",
      "  Batch 8240/15762: Loss = 198.4534\n",
      "  Batch 8250/15762: Loss = 263.2843\n",
      "  Batch 8260/15762: Loss = 207.4104\n",
      "  Batch 8270/15762: Loss = 295.6755\n",
      "  Batch 8280/15762: Loss = 320.9230\n",
      "  Batch 8290/15762: Loss = 154.5386\n",
      "  Batch 8300/15762: Loss = 411.1252\n",
      "  Batch 8310/15762: Loss = 285.3460\n",
      "  Batch 8320/15762: Loss = 384.1674\n",
      "  Batch 8330/15762: Loss = 268.6702\n",
      "  Batch 8340/15762: Loss = 260.0713\n",
      "  Batch 8350/15762: Loss = 255.2395\n",
      "  Batch 8360/15762: Loss = 138.2880\n",
      "  Batch 8370/15762: Loss = 208.1116\n",
      "  Batch 8380/15762: Loss = 225.5440\n",
      "  Batch 8390/15762: Loss = 164.6252\n",
      "  Batch 8400/15762: Loss = 223.9228\n",
      "  Batch 8410/15762: Loss = 245.3792\n",
      "  Batch 8420/15762: Loss = 322.3995\n",
      "  Batch 8430/15762: Loss = 190.5539\n",
      "  Batch 8440/15762: Loss = 182.3707\n",
      "  Batch 8450/15762: Loss = 224.7661\n",
      "  Batch 8460/15762: Loss = 151.6313\n",
      "  Batch 8470/15762: Loss = 105.3228\n",
      "  Batch 8480/15762: Loss = 193.9757\n",
      "  Batch 8490/15762: Loss = 228.6896\n",
      "  Batch 8500/15762: Loss = 306.6215\n",
      "  Batch 8510/15762: Loss = 141.9872\n",
      "  Batch 8520/15762: Loss = 216.2608\n",
      "  Batch 8530/15762: Loss = 192.0798\n",
      "  Batch 8540/15762: Loss = 189.6163\n",
      "  Batch 8550/15762: Loss = 237.8128\n",
      "  Batch 8560/15762: Loss = 100.9993\n",
      "  Batch 8570/15762: Loss = 106.6720\n",
      "  Batch 8580/15762: Loss = 150.5587\n",
      "  Batch 8590/15762: Loss = 155.9084\n",
      "  Batch 8600/15762: Loss = 262.1197\n",
      "  Batch 8610/15762: Loss = 238.7764\n",
      "  Batch 8620/15762: Loss = 100.4982\n",
      "  Batch 8630/15762: Loss = 123.4133\n",
      "  Batch 8640/15762: Loss = 433.2313\n",
      "  Batch 8650/15762: Loss = 399.5177\n",
      "  Batch 8660/15762: Loss = 127.7207\n",
      "  Batch 8670/15762: Loss = 297.3131\n",
      "  Batch 8680/15762: Loss = 175.8756\n",
      "  Batch 8690/15762: Loss = 210.6541\n",
      "  Batch 8700/15762: Loss = 282.7482\n",
      "  Batch 8710/15762: Loss = 413.6689\n",
      "  Batch 8720/15762: Loss = 122.8337\n",
      "  Batch 8730/15762: Loss = 268.0327\n",
      "  Batch 8740/15762: Loss = 212.9918\n",
      "  Batch 8750/15762: Loss = 268.5965\n",
      "  Batch 8760/15762: Loss = 269.6568\n",
      "  Batch 8770/15762: Loss = 370.4709\n",
      "  Batch 8780/15762: Loss = 350.4706\n",
      "  Batch 8790/15762: Loss = 235.1561\n",
      "  Batch 8800/15762: Loss = 245.4351\n",
      "  Batch 8810/15762: Loss = 306.2451\n",
      "  Batch 8820/15762: Loss = 237.8318\n",
      "  Batch 8830/15762: Loss = 293.4246\n",
      "  Batch 8840/15762: Loss = 346.7955\n",
      "  Batch 8850/15762: Loss = 98.1635\n",
      "  Batch 8860/15762: Loss = 329.3093\n",
      "  Batch 8870/15762: Loss = 207.1743\n",
      "  Batch 8880/15762: Loss = 226.8488\n",
      "  Batch 8890/15762: Loss = 192.1812\n",
      "  Batch 8900/15762: Loss = 298.8034\n",
      "  Batch 8910/15762: Loss = 310.7932\n",
      "  Batch 8920/15762: Loss = 309.9575\n",
      "  Batch 8930/15762: Loss = 182.9748\n",
      "  Batch 8940/15762: Loss = 155.5935\n",
      "  Batch 8950/15762: Loss = 176.4218\n",
      "  Batch 8960/15762: Loss = 224.9504\n",
      "  Batch 8970/15762: Loss = 255.4214\n",
      "  Batch 8980/15762: Loss = 289.3647\n",
      "  Batch 8990/15762: Loss = 52.5570\n",
      "  Batch 9000/15762: Loss = 161.4185\n",
      "  Batch 9010/15762: Loss = 171.3341\n",
      "  Batch 9020/15762: Loss = 248.5988\n",
      "  Batch 9030/15762: Loss = 524.6581\n",
      "  Batch 9040/15762: Loss = 226.9049\n",
      "  Batch 9050/15762: Loss = 308.4162\n",
      "  Batch 9060/15762: Loss = 149.7628\n",
      "  Batch 9070/15762: Loss = 262.3841\n",
      "  Batch 9080/15762: Loss = 290.5038\n",
      "  Batch 9090/15762: Loss = 255.4726\n",
      "  Batch 9100/15762: Loss = 278.1041\n",
      "  Batch 9110/15762: Loss = 226.8657\n",
      "  Batch 9120/15762: Loss = 251.4093\n",
      "  Batch 9130/15762: Loss = 87.3187\n",
      "  Batch 9140/15762: Loss = 252.6200\n",
      "  Batch 9150/15762: Loss = 171.1265\n",
      "  Batch 9160/15762: Loss = 249.1313\n",
      "  Batch 9170/15762: Loss = 279.8351\n",
      "  Batch 9180/15762: Loss = 284.2808\n",
      "  Batch 9190/15762: Loss = 208.7636\n",
      "  Batch 9200/15762: Loss = 300.3457\n",
      "  Batch 9210/15762: Loss = 320.6763\n",
      "  Batch 9220/15762: Loss = 146.9599\n",
      "  Batch 9230/15762: Loss = 237.2647\n",
      "  Batch 9240/15762: Loss = 220.8406\n",
      "  Batch 9250/15762: Loss = 225.7896\n",
      "  Batch 9260/15762: Loss = 250.1465\n",
      "  Batch 9270/15762: Loss = 215.0791\n",
      "  Batch 9280/15762: Loss = 179.3646\n",
      "  Batch 9290/15762: Loss = 206.0426\n",
      "  Batch 9300/15762: Loss = 313.1089\n",
      "  Batch 9310/15762: Loss = 230.4953\n",
      "  Batch 9320/15762: Loss = 276.4604\n",
      "  Batch 9330/15762: Loss = 338.9734\n",
      "  Batch 9340/15762: Loss = 252.8088\n",
      "  Batch 9350/15762: Loss = 108.2749\n",
      "  Batch 9360/15762: Loss = 225.0443\n",
      "  Batch 9370/15762: Loss = 283.8987\n",
      "  Batch 9380/15762: Loss = 280.0735\n",
      "  Batch 9390/15762: Loss = 383.6775\n",
      "  Batch 9400/15762: Loss = 345.5326\n",
      "  Batch 9410/15762: Loss = 254.1317\n",
      "  Batch 9420/15762: Loss = 190.4774\n",
      "  Batch 9430/15762: Loss = 320.3182\n",
      "  Batch 9440/15762: Loss = 245.4363\n",
      "  Batch 9450/15762: Loss = 330.7731\n",
      "  Batch 9460/15762: Loss = 325.8293\n",
      "  Batch 9470/15762: Loss = 205.8710\n",
      "  Batch 9480/15762: Loss = 184.5483\n",
      "  Batch 9490/15762: Loss = 258.0470\n",
      "  Batch 9500/15762: Loss = 168.4532\n",
      "  Batch 9510/15762: Loss = 263.9246\n",
      "  Batch 9520/15762: Loss = 318.4304\n",
      "  Batch 9530/15762: Loss = 255.3760\n",
      "  Batch 9540/15762: Loss = 130.9845\n",
      "  Batch 9550/15762: Loss = 183.5550\n",
      "  Batch 9560/15762: Loss = 267.2183\n",
      "  Batch 9570/15762: Loss = 205.1872\n",
      "  Batch 9580/15762: Loss = 232.4828\n",
      "  Batch 9590/15762: Loss = 117.6696\n",
      "  Batch 9600/15762: Loss = 376.1839\n",
      "  Batch 9610/15762: Loss = 357.6329\n",
      "  Batch 9620/15762: Loss = 197.0090\n",
      "  Batch 9630/15762: Loss = 172.4021\n",
      "  Batch 9640/15762: Loss = 435.8113\n",
      "  Batch 9650/15762: Loss = 186.6918\n",
      "  Batch 9660/15762: Loss = 257.1143\n",
      "  Batch 9670/15762: Loss = 249.5765\n",
      "  Batch 9680/15762: Loss = 252.4998\n",
      "  Batch 9690/15762: Loss = 51.7848\n",
      "  Batch 9700/15762: Loss = 177.6469\n",
      "  Batch 9710/15762: Loss = 276.1367\n",
      "  Batch 9720/15762: Loss = 220.8583\n",
      "  Batch 9730/15762: Loss = 169.7606\n",
      "  Batch 9740/15762: Loss = 297.3662\n",
      "  Batch 9750/15762: Loss = 136.8656\n",
      "  Batch 9760/15762: Loss = 79.4250\n",
      "  Batch 9770/15762: Loss = 215.5117\n",
      "  Batch 9780/15762: Loss = 282.4582\n",
      "  Batch 9790/15762: Loss = 145.8530\n",
      "  Batch 9800/15762: Loss = 322.8284\n",
      "  Batch 9810/15762: Loss = 235.3135\n",
      "  Batch 9820/15762: Loss = 169.3656\n",
      "  Batch 9830/15762: Loss = 319.2645\n",
      "  Batch 9840/15762: Loss = 164.3755\n",
      "  Batch 9850/15762: Loss = 257.6002\n",
      "  Batch 9860/15762: Loss = 282.7067\n",
      "  Batch 9870/15762: Loss = 295.6721\n",
      "  Batch 9880/15762: Loss = 289.7873\n",
      "  Batch 9890/15762: Loss = 161.3485\n",
      "  Batch 9900/15762: Loss = 280.8558\n",
      "  Batch 9910/15762: Loss = 117.4460\n",
      "  Batch 9920/15762: Loss = 261.5631\n",
      "  Batch 9930/15762: Loss = 145.3416\n",
      "  Batch 9940/15762: Loss = 147.6524\n",
      "  Batch 9950/15762: Loss = 260.1379\n",
      "  Batch 9960/15762: Loss = 500.1945\n",
      "  Batch 9970/15762: Loss = 264.7918\n",
      "  Batch 9980/15762: Loss = 270.7403\n",
      "  Batch 9990/15762: Loss = 136.8493\n",
      "  Batch 10000/15762: Loss = 90.6445\n",
      "  Batch 10010/15762: Loss = 253.0862\n",
      "  Batch 10020/15762: Loss = 343.3690\n",
      "  Batch 10030/15762: Loss = 294.3130\n",
      "  Batch 10040/15762: Loss = 274.3766\n",
      "  Batch 10050/15762: Loss = 190.2078\n",
      "  Batch 10060/15762: Loss = 105.7885\n",
      "  Batch 10070/15762: Loss = 227.9678\n",
      "  Batch 10080/15762: Loss = 301.0358\n",
      "  Batch 10090/15762: Loss = 309.0388\n",
      "  Batch 10100/15762: Loss = 289.1865\n",
      "  Batch 10110/15762: Loss = 158.7203\n",
      "  Batch 10120/15762: Loss = 222.0384\n",
      "  Batch 10130/15762: Loss = 290.5179\n",
      "  Batch 10140/15762: Loss = 197.0288\n",
      "  Batch 10150/15762: Loss = 260.4641\n",
      "  Batch 10160/15762: Loss = 197.2038\n",
      "  Batch 10170/15762: Loss = 159.9757\n",
      "  Batch 10180/15762: Loss = 260.4490\n",
      "  Batch 10190/15762: Loss = 211.4679\n",
      "  Batch 10200/15762: Loss = 228.5957\n",
      "  Batch 10210/15762: Loss = 251.8838\n",
      "  Batch 10220/15762: Loss = 247.1115\n",
      "  Batch 10230/15762: Loss = 194.5622\n",
      "  Batch 10240/15762: Loss = 350.9927\n",
      "  Batch 10250/15762: Loss = 173.2572\n",
      "  Batch 10260/15762: Loss = 267.1716\n",
      "  Batch 10270/15762: Loss = 313.7010\n",
      "  Batch 10280/15762: Loss = 173.8080\n",
      "  Batch 10290/15762: Loss = 139.5499\n",
      "  Batch 10300/15762: Loss = 278.4178\n",
      "  Batch 10310/15762: Loss = 362.1645\n",
      "  Batch 10320/15762: Loss = 158.4281\n",
      "  Batch 10330/15762: Loss = 138.8429\n",
      "  Batch 10340/15762: Loss = 240.9554\n",
      "  Batch 10350/15762: Loss = 234.6021\n",
      "  Batch 10360/15762: Loss = 131.9992\n",
      "  Batch 10370/15762: Loss = 307.4445\n",
      "  Batch 10380/15762: Loss = 287.7821\n",
      "  Batch 10390/15762: Loss = 304.0849\n",
      "  Batch 10400/15762: Loss = 66.8168\n",
      "  Batch 10410/15762: Loss = 179.7199\n",
      "  Batch 10420/15762: Loss = 217.9412\n",
      "  Batch 10430/15762: Loss = 126.0309\n",
      "  Batch 10440/15762: Loss = 152.4724\n",
      "  Batch 10450/15762: Loss = 250.3331\n",
      "  Batch 10460/15762: Loss = 188.2157\n",
      "  Batch 10470/15762: Loss = 226.3266\n",
      "  Batch 10480/15762: Loss = 238.8937\n",
      "  Batch 10490/15762: Loss = 272.0585\n",
      "  Batch 10500/15762: Loss = 297.7956\n",
      "  Batch 10510/15762: Loss = 269.9979\n",
      "  Batch 10520/15762: Loss = 202.3094\n",
      "  Batch 10530/15762: Loss = 232.0060\n",
      "  Batch 10540/15762: Loss = 187.1533\n",
      "  Batch 10550/15762: Loss = 298.5208\n",
      "  Batch 10560/15762: Loss = 206.3165\n",
      "  Batch 10570/15762: Loss = 287.3617\n",
      "  Batch 10580/15762: Loss = 369.5613\n",
      "  Batch 10590/15762: Loss = 458.6719\n",
      "  Batch 10600/15762: Loss = 129.4164\n",
      "  Batch 10610/15762: Loss = 177.7001\n",
      "  Batch 10620/15762: Loss = 288.0505\n",
      "  Batch 10630/15762: Loss = 215.2482\n",
      "  Batch 10640/15762: Loss = 364.7440\n",
      "  Batch 10650/15762: Loss = 53.8633\n",
      "  Batch 10660/15762: Loss = 277.0059\n",
      "  Batch 10670/15762: Loss = 282.2587\n",
      "  Batch 10680/15762: Loss = 439.7134\n",
      "  Batch 10690/15762: Loss = 420.7697\n",
      "  Batch 10700/15762: Loss = 305.4697\n",
      "  Batch 10710/15762: Loss = 229.9544\n",
      "  Batch 10720/15762: Loss = 259.3594\n",
      "  Batch 10730/15762: Loss = 418.8474\n",
      "  Batch 10740/15762: Loss = 387.4055\n",
      "  Batch 10750/15762: Loss = 286.7167\n",
      "  Batch 10760/15762: Loss = 227.1439\n",
      "  Batch 10770/15762: Loss = 209.0815\n",
      "  Batch 10780/15762: Loss = 253.7504\n",
      "  Batch 10790/15762: Loss = 290.1219\n",
      "  Batch 10800/15762: Loss = 374.0658\n",
      "  Batch 10810/15762: Loss = 176.8971\n",
      "  Batch 10820/15762: Loss = 264.8882\n",
      "  Batch 10830/15762: Loss = 199.0823\n",
      "  Batch 10840/15762: Loss = 167.2517\n",
      "  Batch 10850/15762: Loss = 268.6330\n",
      "  Batch 10860/15762: Loss = 87.6212\n",
      "  Batch 10870/15762: Loss = 276.1475\n",
      "  Batch 10880/15762: Loss = 122.1280\n",
      "  Batch 10890/15762: Loss = 156.2847\n",
      "  Batch 10900/15762: Loss = 273.5183\n",
      "  Batch 10910/15762: Loss = 194.5875\n",
      "  Batch 10920/15762: Loss = 250.8354\n",
      "  Batch 10930/15762: Loss = 133.3798\n",
      "  Batch 10940/15762: Loss = 262.8285\n",
      "  Batch 10950/15762: Loss = 437.3523\n",
      "  Batch 10960/15762: Loss = 246.4274\n",
      "  Batch 10970/15762: Loss = 404.3228\n",
      "  Batch 10980/15762: Loss = 183.3419\n",
      "  Batch 10990/15762: Loss = 138.0958\n",
      "  Batch 11000/15762: Loss = 240.9494\n",
      "  Batch 11010/15762: Loss = 157.9346\n",
      "  Batch 11020/15762: Loss = 275.4515\n",
      "  Batch 11030/15762: Loss = 510.4413\n",
      "  Batch 11040/15762: Loss = 430.8248\n",
      "  Batch 11050/15762: Loss = 154.7711\n",
      "  Batch 11060/15762: Loss = 167.9911\n",
      "  Batch 11070/15762: Loss = 210.0405\n",
      "  Batch 11080/15762: Loss = 150.6751\n",
      "  Batch 11090/15762: Loss = 196.6188\n",
      "  Batch 11100/15762: Loss = 174.4227\n",
      "  Batch 11110/15762: Loss = 335.0210\n",
      "  Batch 11120/15762: Loss = 311.0984\n",
      "  Batch 11130/15762: Loss = 198.1048\n",
      "  Batch 11140/15762: Loss = 304.6374\n",
      "  Batch 11150/15762: Loss = 266.3026\n",
      "  Batch 11160/15762: Loss = 153.0448\n",
      "  Batch 11170/15762: Loss = 299.6253\n",
      "  Batch 11180/15762: Loss = 269.9864\n",
      "  Batch 11190/15762: Loss = 104.4831\n",
      "  Batch 11200/15762: Loss = 231.9292\n",
      "  Batch 11210/15762: Loss = 280.2144\n",
      "  Batch 11220/15762: Loss = 275.0285\n",
      "  Batch 11230/15762: Loss = 402.8121\n",
      "  Batch 11240/15762: Loss = 438.3258\n",
      "  Batch 11250/15762: Loss = 272.2578\n",
      "  Batch 11260/15762: Loss = 64.6981\n",
      "  Batch 11270/15762: Loss = 250.7039\n",
      "  Batch 11280/15762: Loss = 157.6010\n",
      "  Batch 11290/15762: Loss = 340.6716\n",
      "  Batch 11300/15762: Loss = 248.7489\n",
      "  Batch 11310/15762: Loss = 203.8081\n",
      "  Batch 11320/15762: Loss = 144.3914\n",
      "  Batch 11330/15762: Loss = 357.8908\n",
      "  Batch 11340/15762: Loss = 106.4875\n",
      "  Batch 11350/15762: Loss = 98.0073\n",
      "  Batch 11360/15762: Loss = 350.9719\n",
      "  Batch 11370/15762: Loss = 79.5658\n",
      "  Batch 11380/15762: Loss = 126.6106\n",
      "  Batch 11390/15762: Loss = 219.1249\n",
      "  Batch 11400/15762: Loss = 217.7826\n",
      "  Batch 11410/15762: Loss = 159.0456\n",
      "  Batch 11420/15762: Loss = 213.2020\n",
      "  Batch 11430/15762: Loss = 247.4940\n",
      "  Batch 11440/15762: Loss = 168.9514\n",
      "  Batch 11450/15762: Loss = 131.9026\n",
      "  Batch 11460/15762: Loss = 316.9568\n",
      "  Batch 11470/15762: Loss = 179.9296\n",
      "  Batch 11480/15762: Loss = 141.3947\n",
      "  Batch 11490/15762: Loss = 165.7103\n",
      "  Batch 11500/15762: Loss = 215.6902\n",
      "  Batch 11510/15762: Loss = 97.9642\n",
      "  Batch 11520/15762: Loss = 339.1885\n",
      "  Batch 11530/15762: Loss = 166.6612\n",
      "  Batch 11540/15762: Loss = 412.2182\n",
      "  Batch 11550/15762: Loss = 461.6803\n",
      "  Batch 11560/15762: Loss = 246.5226\n",
      "  Batch 11570/15762: Loss = 235.8365\n",
      "  Batch 11580/15762: Loss = 277.6434\n",
      "  Batch 11590/15762: Loss = 236.3108\n",
      "  Batch 11600/15762: Loss = 267.1583\n",
      "  Batch 11610/15762: Loss = 108.0179\n",
      "  Batch 11620/15762: Loss = 189.1869\n",
      "  Batch 11630/15762: Loss = 102.7430\n",
      "  Batch 11640/15762: Loss = 270.5692\n",
      "  Batch 11650/15762: Loss = 207.8521\n",
      "  Batch 11660/15762: Loss = 156.0361\n",
      "  Batch 11670/15762: Loss = 225.4088\n",
      "  Batch 11680/15762: Loss = 286.8084\n",
      "  Batch 11690/15762: Loss = 325.6682\n",
      "  Batch 11700/15762: Loss = 149.6332\n",
      "  Batch 11710/15762: Loss = 390.8282\n",
      "  Batch 11720/15762: Loss = 354.1192\n",
      "  Batch 11730/15762: Loss = 352.9324\n",
      "  Batch 11740/15762: Loss = 183.3034\n",
      "  Batch 11750/15762: Loss = 178.0069\n",
      "  Batch 11760/15762: Loss = 168.2132\n",
      "  Batch 11770/15762: Loss = 223.8494\n",
      "  Batch 11780/15762: Loss = 143.6244\n",
      "  Batch 11790/15762: Loss = 204.5880\n",
      "  Batch 11800/15762: Loss = 183.0391\n",
      "  Batch 11810/15762: Loss = 297.0797\n",
      "  Batch 11820/15762: Loss = 135.0125\n",
      "  Batch 11830/15762: Loss = 517.2565\n",
      "  Batch 11840/15762: Loss = 348.0408\n",
      "  Batch 11850/15762: Loss = 314.0641\n",
      "  Batch 11860/15762: Loss = 263.0095\n",
      "  Batch 11870/15762: Loss = 195.8748\n",
      "  Batch 11880/15762: Loss = 213.5721\n",
      "  Batch 11890/15762: Loss = 233.2282\n",
      "  Batch 11900/15762: Loss = 222.1944\n",
      "  Batch 11910/15762: Loss = 261.3019\n",
      "  Batch 11920/15762: Loss = 255.8624\n",
      "  Batch 11930/15762: Loss = 114.2340\n",
      "  Batch 11940/15762: Loss = 519.4902\n",
      "  Batch 11950/15762: Loss = 267.1742\n",
      "  Batch 11960/15762: Loss = 123.5780\n",
      "  Batch 11970/15762: Loss = 273.0660\n",
      "  Batch 11980/15762: Loss = 392.7911\n",
      "  Batch 11990/15762: Loss = 302.4308\n",
      "  Batch 12000/15762: Loss = 249.4911\n",
      "  Batch 12010/15762: Loss = 195.7619\n",
      "  Batch 12020/15762: Loss = 200.6593\n",
      "  Batch 12030/15762: Loss = 269.0920\n",
      "  Batch 12040/15762: Loss = 177.5365\n",
      "  Batch 12050/15762: Loss = 421.6970\n",
      "  Batch 12060/15762: Loss = 166.9335\n",
      "  Batch 12070/15762: Loss = 165.5089\n",
      "  Batch 12080/15762: Loss = 300.4297\n",
      "  Batch 12090/15762: Loss = 229.0606\n",
      "  Batch 12100/15762: Loss = 243.3528\n",
      "  Batch 12110/15762: Loss = 178.7173\n",
      "  Batch 12120/15762: Loss = 220.8129\n",
      "  Batch 12130/15762: Loss = 271.6145\n",
      "  Batch 12140/15762: Loss = 378.0507\n",
      "  Batch 12150/15762: Loss = 167.1048\n",
      "  Batch 12160/15762: Loss = 284.5603\n",
      "  Batch 12170/15762: Loss = 115.8077\n",
      "  Batch 12180/15762: Loss = 309.6126\n",
      "  Batch 12190/15762: Loss = 314.6288\n",
      "  Batch 12200/15762: Loss = 164.0107\n",
      "  Batch 12210/15762: Loss = 378.3523\n",
      "  Batch 12220/15762: Loss = 269.4118\n",
      "  Batch 12230/15762: Loss = 230.7590\n",
      "  Batch 12240/15762: Loss = 330.4311\n",
      "  Batch 12250/15762: Loss = 250.3889\n",
      "  Batch 12260/15762: Loss = 287.0140\n",
      "  Batch 12270/15762: Loss = 217.2373\n",
      "  Batch 12280/15762: Loss = 244.6611\n",
      "  Batch 12290/15762: Loss = 220.6199\n",
      "  Batch 12300/15762: Loss = 364.1791\n",
      "  Batch 12310/15762: Loss = 351.4839\n",
      "  Batch 12320/15762: Loss = 351.3592\n",
      "  Batch 12330/15762: Loss = 205.2743\n",
      "  Batch 12340/15762: Loss = 331.8769\n",
      "  Batch 12350/15762: Loss = 186.8448\n",
      "  Batch 12360/15762: Loss = 212.8000\n",
      "  Batch 12370/15762: Loss = 269.0055\n",
      "  Batch 12380/15762: Loss = 232.3366\n",
      "  Batch 12390/15762: Loss = 220.8395\n",
      "  Batch 12400/15762: Loss = 365.7020\n",
      "  Batch 12410/15762: Loss = 192.1044\n",
      "  Batch 12420/15762: Loss = 295.9291\n",
      "  Batch 12430/15762: Loss = 247.6874\n",
      "  Batch 12440/15762: Loss = 191.9622\n",
      "  Batch 12450/15762: Loss = 124.6851\n",
      "  Batch 12460/15762: Loss = 336.4229\n",
      "  Batch 12470/15762: Loss = 152.2502\n",
      "  Batch 12480/15762: Loss = 219.0504\n",
      "  Batch 12490/15762: Loss = 366.8595\n",
      "  Batch 12500/15762: Loss = 248.1573\n",
      "  Batch 12510/15762: Loss = 162.9295\n",
      "  Batch 12520/15762: Loss = 386.3609\n",
      "  Batch 12530/15762: Loss = 186.5794\n",
      "  Batch 12540/15762: Loss = 48.2012\n",
      "  Batch 12550/15762: Loss = 366.5902\n",
      "  Batch 12560/15762: Loss = 120.6440\n",
      "  Batch 12570/15762: Loss = 228.5283\n",
      "  Batch 12580/15762: Loss = 342.0217\n",
      "  Batch 12590/15762: Loss = 221.6018\n",
      "  Batch 12600/15762: Loss = 246.9485\n",
      "  Batch 12610/15762: Loss = 188.4216\n",
      "  Batch 12620/15762: Loss = 319.8354\n",
      "  Batch 12630/15762: Loss = 222.3802\n",
      "  Batch 12640/15762: Loss = 358.2018\n",
      "  Batch 12650/15762: Loss = 218.0952\n",
      "  Batch 12660/15762: Loss = 234.0945\n",
      "  Batch 12670/15762: Loss = 240.0291\n",
      "  Batch 12680/15762: Loss = 158.0274\n",
      "  Batch 12690/15762: Loss = 330.9719\n",
      "  Batch 12700/15762: Loss = 287.5701\n",
      "  Batch 12710/15762: Loss = 339.3561\n",
      "  Batch 12720/15762: Loss = 265.6089\n",
      "  Batch 12730/15762: Loss = 230.9083\n",
      "  Batch 12740/15762: Loss = 294.0978\n",
      "  Batch 12750/15762: Loss = 318.4737\n",
      "  Batch 12760/15762: Loss = 169.4686\n",
      "  Batch 12770/15762: Loss = 274.5659\n",
      "  Batch 12780/15762: Loss = 153.8150\n",
      "  Batch 12790/15762: Loss = 141.0401\n",
      "  Batch 12800/15762: Loss = 233.4523\n",
      "  Batch 12810/15762: Loss = 210.5611\n",
      "  Batch 12820/15762: Loss = 324.7264\n",
      "  Batch 12830/15762: Loss = 319.3725\n",
      "  Batch 12840/15762: Loss = 140.7569\n",
      "  Batch 12850/15762: Loss = 227.6595\n",
      "  Batch 12860/15762: Loss = 229.9212\n",
      "  Batch 12870/15762: Loss = 372.6245\n",
      "  Batch 12880/15762: Loss = 361.9378\n",
      "  Batch 12890/15762: Loss = 329.1772\n",
      "  Batch 12900/15762: Loss = 415.5550\n",
      "  Batch 12910/15762: Loss = 285.4938\n",
      "  Batch 12920/15762: Loss = 398.5667\n",
      "  Batch 12930/15762: Loss = 217.0394\n",
      "  Batch 12940/15762: Loss = 288.0117\n",
      "  Batch 12950/15762: Loss = 180.4860\n",
      "  Batch 12960/15762: Loss = 275.0885\n",
      "  Batch 12970/15762: Loss = 299.0884\n",
      "  Batch 12980/15762: Loss = 252.0386\n",
      "  Batch 12990/15762: Loss = 269.3555\n",
      "  Batch 13000/15762: Loss = 395.4819\n",
      "  Batch 13010/15762: Loss = 191.2846\n",
      "  Batch 13020/15762: Loss = 270.6893\n",
      "  Batch 13030/15762: Loss = 324.3158\n",
      "  Batch 13040/15762: Loss = 326.9884\n",
      "  Batch 13050/15762: Loss = 180.9784\n",
      "  Batch 13060/15762: Loss = 328.6808\n",
      "  Batch 13070/15762: Loss = 408.6396\n",
      "  Batch 13080/15762: Loss = 414.5074\n",
      "  Batch 13090/15762: Loss = 416.8231\n",
      "  Batch 13100/15762: Loss = 175.1052\n",
      "  Batch 13110/15762: Loss = 323.9384\n",
      "  Batch 13120/15762: Loss = 248.9464\n",
      "  Batch 13130/15762: Loss = 205.7627\n",
      "  Batch 13140/15762: Loss = 462.4278\n",
      "  Batch 13150/15762: Loss = 171.9634\n",
      "  Batch 13160/15762: Loss = 129.3315\n",
      "  Batch 13170/15762: Loss = 341.0874\n",
      "  Batch 13180/15762: Loss = 250.9165\n",
      "  Batch 13190/15762: Loss = 299.9398\n",
      "  Batch 13200/15762: Loss = 126.1205\n",
      "  Batch 13210/15762: Loss = 334.4056\n",
      "  Batch 13220/15762: Loss = 269.6547\n",
      "  Batch 13230/15762: Loss = 133.0787\n",
      "  Batch 13240/15762: Loss = 279.3077\n",
      "  Batch 13250/15762: Loss = 410.2128\n",
      "  Batch 13260/15762: Loss = 178.5404\n",
      "  Batch 13270/15762: Loss = 249.3192\n",
      "  Batch 13280/15762: Loss = 365.4536\n",
      "  Batch 13290/15762: Loss = 182.2243\n",
      "  Batch 13300/15762: Loss = 391.6669\n",
      "  Batch 13310/15762: Loss = 323.9844\n",
      "  Batch 13320/15762: Loss = 111.4423\n",
      "  Batch 13330/15762: Loss = 231.9313\n",
      "  Batch 13340/15762: Loss = 256.7397\n",
      "  Batch 13350/15762: Loss = 414.1603\n",
      "  Batch 13360/15762: Loss = 293.5237\n",
      "  Batch 13370/15762: Loss = 236.8394\n",
      "  Batch 13380/15762: Loss = 217.4453\n",
      "  Batch 13390/15762: Loss = 260.7463\n",
      "  Batch 13400/15762: Loss = 224.5908\n",
      "  Batch 13410/15762: Loss = 258.1340\n",
      "  Batch 13420/15762: Loss = 317.6476\n",
      "  Batch 13430/15762: Loss = 186.6180\n",
      "  Batch 13440/15762: Loss = 214.4458\n",
      "  Batch 13450/15762: Loss = 313.3015\n",
      "  Batch 13460/15762: Loss = 399.2505\n",
      "  Batch 13470/15762: Loss = 271.7616\n",
      "  Batch 13480/15762: Loss = 279.8782\n",
      "  Batch 13490/15762: Loss = 479.1881\n",
      "  Batch 13500/15762: Loss = 350.5784\n",
      "  Batch 13510/15762: Loss = 264.0414\n",
      "  Batch 13520/15762: Loss = 235.9667\n",
      "  Batch 13530/15762: Loss = 126.7550\n",
      "  Batch 13540/15762: Loss = 106.0434\n",
      "  Batch 13550/15762: Loss = 218.2622\n",
      "  Batch 13560/15762: Loss = 368.5229\n",
      "  Batch 13570/15762: Loss = 185.0040\n",
      "  Batch 13580/15762: Loss = 152.3731\n",
      "  Batch 13590/15762: Loss = 200.3975\n",
      "  Batch 13600/15762: Loss = 149.9156\n",
      "  Batch 13610/15762: Loss = 247.4509\n",
      "  Batch 13620/15762: Loss = 385.9282\n",
      "  Batch 13630/15762: Loss = 106.6020\n",
      "  Batch 13640/15762: Loss = 235.2684\n",
      "  Batch 13650/15762: Loss = 506.1188\n",
      "  Batch 13660/15762: Loss = 262.2857\n",
      "  Batch 13670/15762: Loss = 339.5652\n",
      "  Batch 13680/15762: Loss = 269.2130\n",
      "  Batch 13690/15762: Loss = 223.3392\n",
      "  Batch 13700/15762: Loss = 170.8006\n",
      "  Batch 13710/15762: Loss = 260.4399\n",
      "  Batch 13720/15762: Loss = 427.8015\n",
      "  Batch 13730/15762: Loss = 344.7481\n",
      "  Batch 13740/15762: Loss = 279.1520\n",
      "  Batch 13750/15762: Loss = 168.3713\n",
      "  Batch 13760/15762: Loss = 146.9088\n",
      "  Batch 13770/15762: Loss = 249.3138\n",
      "  Batch 13780/15762: Loss = 210.1276\n",
      "  Batch 13790/15762: Loss = 306.8570\n",
      "  Batch 13800/15762: Loss = 342.1479\n",
      "  Batch 13810/15762: Loss = 290.5775\n",
      "  Batch 13820/15762: Loss = 338.1209\n",
      "  Batch 13830/15762: Loss = 254.6312\n",
      "  Batch 13840/15762: Loss = 135.1570\n",
      "  Batch 13850/15762: Loss = 314.7930\n",
      "  Batch 13860/15762: Loss = 189.5491\n",
      "  Batch 13870/15762: Loss = 201.6773\n",
      "  Batch 13880/15762: Loss = 186.0300\n",
      "  Batch 13890/15762: Loss = 184.6401\n",
      "  Batch 13900/15762: Loss = 249.3536\n",
      "  Batch 13910/15762: Loss = 310.6252\n",
      "  Batch 13920/15762: Loss = 318.7271\n",
      "  Batch 13930/15762: Loss = 198.6816\n",
      "  Batch 13940/15762: Loss = 241.3848\n",
      "  Batch 13950/15762: Loss = 458.0722\n",
      "  Batch 13960/15762: Loss = 144.5052\n",
      "  Batch 13970/15762: Loss = 338.6363\n",
      "  Batch 13980/15762: Loss = 191.5943\n",
      "  Batch 13990/15762: Loss = 192.8556\n",
      "  Batch 14000/15762: Loss = 320.7765\n",
      "  Batch 14010/15762: Loss = 354.7253\n",
      "  Batch 14020/15762: Loss = 237.9152\n",
      "  Batch 14030/15762: Loss = 345.2600\n",
      "  Batch 14040/15762: Loss = 169.4113\n",
      "  Batch 14050/15762: Loss = 212.0947\n",
      "  Batch 14060/15762: Loss = 235.9034\n",
      "  Batch 14070/15762: Loss = 172.4739\n",
      "  Batch 14080/15762: Loss = 188.8260\n",
      "  Batch 14090/15762: Loss = 114.3002\n",
      "  Batch 14100/15762: Loss = 211.2160\n",
      "  Batch 14110/15762: Loss = 284.5292\n",
      "  Batch 14120/15762: Loss = 235.1021\n",
      "  Batch 14130/15762: Loss = 206.6662\n",
      "  Batch 14140/15762: Loss = 278.0476\n",
      "  Batch 14150/15762: Loss = 236.6745\n",
      "  Batch 14160/15762: Loss = 300.7328\n",
      "  Batch 14170/15762: Loss = 263.7681\n",
      "  Batch 14180/15762: Loss = 288.0379\n",
      "  Batch 14190/15762: Loss = 269.3845\n",
      "  Batch 14200/15762: Loss = 101.8871\n",
      "  Batch 14210/15762: Loss = 479.6582\n",
      "  Batch 14220/15762: Loss = 195.1427\n",
      "  Batch 14230/15762: Loss = 277.4380\n",
      "  Batch 14240/15762: Loss = 186.2175\n",
      "  Batch 14250/15762: Loss = 266.1088\n",
      "  Batch 14260/15762: Loss = 200.2501\n",
      "  Batch 14270/15762: Loss = 285.2045\n",
      "  Batch 14280/15762: Loss = 207.5256\n",
      "  Batch 14290/15762: Loss = 264.0502\n",
      "  Batch 14300/15762: Loss = 224.5400\n",
      "  Batch 14310/15762: Loss = 229.4985\n",
      "  Batch 14320/15762: Loss = 123.4786\n",
      "  Batch 14330/15762: Loss = 313.1917\n",
      "  Batch 14340/15762: Loss = 400.4235\n",
      "  Batch 14350/15762: Loss = 132.5031\n",
      "  Batch 14360/15762: Loss = 187.5811\n",
      "  Batch 14370/15762: Loss = 425.5485\n",
      "  Batch 14380/15762: Loss = 212.6189\n",
      "  Batch 14390/15762: Loss = 471.7609\n",
      "  Batch 14400/15762: Loss = 207.2985\n",
      "  Batch 14410/15762: Loss = 163.3384\n",
      "  Batch 14420/15762: Loss = 166.2694\n",
      "  Batch 14430/15762: Loss = 150.3557\n",
      "  Batch 14440/15762: Loss = 410.5039\n",
      "  Batch 14450/15762: Loss = 210.8047\n",
      "  Batch 14460/15762: Loss = 53.6169\n",
      "  Batch 14470/15762: Loss = 203.3345\n",
      "  Batch 14480/15762: Loss = 175.5245\n",
      "  Batch 14490/15762: Loss = 274.6521\n",
      "  Batch 14500/15762: Loss = 207.0900\n",
      "  Batch 14510/15762: Loss = 119.0214\n",
      "  Batch 14520/15762: Loss = 378.0788\n",
      "  Batch 14530/15762: Loss = 232.5064\n",
      "  Batch 14540/15762: Loss = 402.7715\n",
      "  Batch 14550/15762: Loss = 177.7228\n",
      "  Batch 14560/15762: Loss = 196.1463\n",
      "  Batch 14570/15762: Loss = 264.8236\n",
      "  Batch 14580/15762: Loss = 185.0746\n",
      "  Batch 14590/15762: Loss = 323.2760\n",
      "  Batch 14600/15762: Loss = 272.8175\n",
      "  Batch 14610/15762: Loss = 269.9777\n",
      "  Batch 14620/15762: Loss = 168.6533\n",
      "  Batch 14630/15762: Loss = 203.7349\n",
      "  Batch 14640/15762: Loss = 308.8572\n",
      "  Batch 14650/15762: Loss = 179.9365\n",
      "  Batch 14660/15762: Loss = 143.1683\n",
      "  Batch 14670/15762: Loss = 386.3769\n",
      "  Batch 14680/15762: Loss = 170.4264\n",
      "  Batch 14690/15762: Loss = 282.3652\n",
      "  Batch 14700/15762: Loss = 222.0897\n",
      "  Batch 14710/15762: Loss = 239.1950\n",
      "  Batch 14720/15762: Loss = 192.4306\n",
      "  Batch 14730/15762: Loss = 132.6913\n",
      "  Batch 14740/15762: Loss = 221.8872\n",
      "  Batch 14750/15762: Loss = 279.0485\n",
      "  Batch 14760/15762: Loss = 286.8310\n",
      "  Batch 14770/15762: Loss = 310.0154\n",
      "  Batch 14780/15762: Loss = 354.0143\n",
      "  Batch 14790/15762: Loss = 144.4837\n",
      "  Batch 14800/15762: Loss = 138.8912\n",
      "  Batch 14810/15762: Loss = 299.0814\n",
      "  Batch 14820/15762: Loss = 216.7036\n",
      "  Batch 14830/15762: Loss = 216.4892\n",
      "  Batch 14840/15762: Loss = 66.1568\n",
      "  Batch 14850/15762: Loss = 141.0993\n",
      "  Batch 14860/15762: Loss = 134.6029\n",
      "  Batch 14870/15762: Loss = 222.7574\n",
      "  Batch 14880/15762: Loss = 125.0805\n",
      "  Batch 14890/15762: Loss = 298.0304\n",
      "  Batch 14900/15762: Loss = 213.8380\n",
      "  Batch 14910/15762: Loss = 123.1635\n",
      "  Batch 14920/15762: Loss = 145.1562\n",
      "  Batch 14930/15762: Loss = 353.7979\n",
      "  Batch 14940/15762: Loss = 257.9439\n",
      "  Batch 14950/15762: Loss = 368.2403\n",
      "  Batch 14960/15762: Loss = 167.0880\n",
      "  Batch 14970/15762: Loss = 526.0599\n",
      "  Batch 14980/15762: Loss = 209.9745\n",
      "  Batch 14990/15762: Loss = 189.0359\n",
      "  Batch 15000/15762: Loss = 208.6033\n",
      "  Batch 15010/15762: Loss = 315.0967\n",
      "  Batch 15020/15762: Loss = 165.5773\n",
      "  Batch 15030/15762: Loss = 205.1509\n",
      "  Batch 15040/15762: Loss = 142.6635\n",
      "  Batch 15050/15762: Loss = 283.9776\n",
      "  Batch 15060/15762: Loss = 175.0314\n",
      "  Batch 15070/15762: Loss = 116.5835\n",
      "  Batch 15080/15762: Loss = 235.6417\n",
      "  Batch 15090/15762: Loss = 301.4083\n",
      "  Batch 15100/15762: Loss = 285.6794\n",
      "  Batch 15110/15762: Loss = 373.9370\n",
      "  Batch 15120/15762: Loss = 226.4120\n",
      "  Batch 15130/15762: Loss = 211.6353\n",
      "  Batch 15140/15762: Loss = 292.7855\n",
      "  Batch 15150/15762: Loss = 273.0624\n",
      "  Batch 15160/15762: Loss = 125.9079\n",
      "  Batch 15170/15762: Loss = 206.9226\n",
      "  Batch 15180/15762: Loss = 243.3451\n",
      "  Batch 15190/15762: Loss = 234.2990\n",
      "  Batch 15200/15762: Loss = 210.8857\n",
      "  Batch 15210/15762: Loss = 256.2874\n",
      "  Batch 15220/15762: Loss = 347.2067\n",
      "  Batch 15230/15762: Loss = 240.3236\n",
      "  Batch 15240/15762: Loss = 423.1895\n",
      "  Batch 15250/15762: Loss = 265.1300\n",
      "  Batch 15260/15762: Loss = 166.0958\n",
      "  Batch 15270/15762: Loss = 169.3271\n",
      "  Batch 15280/15762: Loss = 267.7948\n",
      "  Batch 15290/15762: Loss = 180.5284\n",
      "  Batch 15300/15762: Loss = 309.7158\n",
      "  Batch 15310/15762: Loss = 312.4970\n",
      "  Batch 15320/15762: Loss = 204.1750\n",
      "  Batch 15330/15762: Loss = 244.7515\n",
      "  Batch 15340/15762: Loss = 164.7660\n",
      "  Batch 15350/15762: Loss = 119.1133\n",
      "  Batch 15360/15762: Loss = 98.6032\n",
      "  Batch 15370/15762: Loss = 152.3428\n",
      "  Batch 15380/15762: Loss = 104.8685\n",
      "  Batch 15390/15762: Loss = 146.2200\n",
      "  Batch 15400/15762: Loss = 188.7072\n",
      "  Batch 15410/15762: Loss = 269.0567\n",
      "  Batch 15420/15762: Loss = 145.0192\n",
      "  Batch 15430/15762: Loss = 324.5674\n",
      "  Batch 15440/15762: Loss = 348.7982\n",
      "  Batch 15450/15762: Loss = 363.1546\n",
      "  Batch 15460/15762: Loss = 346.7204\n",
      "  Batch 15470/15762: Loss = 249.5298\n",
      "  Batch 15480/15762: Loss = 193.2731\n",
      "  Batch 15490/15762: Loss = 567.8591\n",
      "  Batch 15500/15762: Loss = 156.5307\n",
      "  Batch 15510/15762: Loss = 190.7293\n",
      "  Batch 15520/15762: Loss = 191.5582\n",
      "  Batch 15530/15762: Loss = 321.4921\n",
      "  Batch 15540/15762: Loss = 154.9217\n",
      "  Batch 15550/15762: Loss = 204.4001\n",
      "  Batch 15560/15762: Loss = 269.3905\n",
      "  Batch 15570/15762: Loss = 126.1622\n",
      "  Batch 15580/15762: Loss = 166.5189\n",
      "  Batch 15590/15762: Loss = 139.4421\n",
      "  Batch 15600/15762: Loss = 234.3234\n",
      "  Batch 15610/15762: Loss = 248.9905\n",
      "  Batch 15620/15762: Loss = 319.7132\n",
      "  Batch 15630/15762: Loss = 186.1460\n",
      "  Batch 15640/15762: Loss = 276.7170\n",
      "  Batch 15650/15762: Loss = 363.0648\n",
      "  Batch 15660/15762: Loss = 275.2854\n",
      "  Batch 15670/15762: Loss = 263.3985\n",
      "  Batch 15680/15762: Loss = 326.5662\n",
      "  Batch 15690/15762: Loss = 232.2424\n",
      "  Batch 15700/15762: Loss = 342.1928\n",
      "  Batch 15710/15762: Loss = 278.9346\n",
      "  Batch 15720/15762: Loss = 274.6403\n",
      "  Batch 15730/15762: Loss = 145.8130\n",
      "  Batch 15740/15762: Loss = 274.6225\n",
      "  Batch 15750/15762: Loss = 207.1397\n",
      "  Batch 15760/15762: Loss = 121.7235\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "def start_training():\n",
    "    \"\"\"Start the training process\"\"\"\n",
    "    # if 'train_loader' not in locals():\n",
    "    #     print(\"⚠️ Training setup not available\")\n",
    "    #     return\n",
    "    \n",
    "    print(\"🚀 Starting enhanced transformer training...\")\n",
    "    print(f\"📊 Training samples: {len(train_loader.dataset)}\")\n",
    "    print(f\"📊 Validation samples: {len(val_loader.dataset)}\")\n",
    "    print(f\"🧠 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"🔧 Epochs: {config['training_params']['n_epochs']}\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config['training_params']['n_epochs']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        train_losses = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = validate_epoch(model, val_loader, device)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record metrics\n",
    "        epoch_time = time.time() - start_time\n",
    "        training_history['train_loss'].append(train_losses['total_loss'])\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "        training_history['epoch_time'].append(epoch_time)\n",
    "        \n",
    "        # GPU memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "            training_history['gpu_memory'].append(gpu_memory)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"\\n📊 Epoch {epoch+1}/{config['training_params']['n_epochs']}\")\n",
    "            print(f\"   Train Loss: {train_losses['total_loss']:.4f} (Action: {train_losses['action_loss']:.4f})\")\n",
    "            print(f\"   Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"   LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            print(f\"   Time: {epoch_time:.1f}s\")\n",
    "            if torch.cuda.is_available():\n",
    "                print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'config': config,\n",
    "                'training_history': training_history,\n",
    "                'epoch': epoch\n",
    "            }, 'enhanced_transformer_best.pth')\n",
    "            print(f\"   💾 New best model saved!\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'training_history': training_history\n",
    "    }, 'enhanced_transformer_final.pth')\n",
    "    \n",
    "    print(\"\\n✅ Training completed!\")\n",
    "    print(f\"🏆 Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "# Uncomment to start training\n",
    "training_history = start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "def plot_training_results(history):\n",
    "    \"\"\"Plot training results\"\"\"\n",
    "    if not history or not history['train_loss']:\n",
    "        print(\"⚠️ No training history available\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss', color='blue')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss', color='red')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Learning rate plot\n",
    "    axes[0, 1].plot(history['learning_rate'], color='green')\n",
    "    axes[0, 1].set_title('Learning Rate Schedule')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Learning Rate')\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Epoch time plot\n",
    "    axes[1, 0].plot(history['epoch_time'], color='orange')\n",
    "    axes[1, 0].set_title('Training Time per Epoch')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Time (seconds)')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # GPU memory plot\n",
    "    if history['gpu_memory']:\n",
    "        axes[1, 1].plot(history['gpu_memory'], color='purple')\n",
    "        axes[1, 1].set_title('GPU Memory Usage')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Memory (GB)')\n",
    "        axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📊 Training results plotted and saved!\")\n",
    "\n",
    "# Plot results if training history exists\n",
    "if 'training_history' in locals() and training_history['train_loss']:\n",
    "    plot_training_results(training_history)\n",
    "else:\n",
    "    print(\"⚠️ No training history to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in val_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            \n",
    "            predictions.extend(outputs['action'].cpu().numpy())\n",
    "            actuals.extend(targets.cpu().numpy())\n",
    "            confidences.extend(outputs['confidence'].cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    confidences = np.array(confidences)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = np.mean((predictions - actuals) ** 2)\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    r2 = 1 - np.sum((actuals - predictions) ** 2) / np.sum((actuals - np.mean(actuals)) ** 2)\n",
    "    \n",
    "    # Direction accuracy\n",
    "    pred_direction = np.sign(predictions)\n",
    "    actual_direction = np.sign(actuals)\n",
    "    direction_accuracy = np.mean(pred_direction == actual_direction)\n",
    "    \n",
    "    print(\"📊 Model Evaluation Results:\")\n",
    "    print(f\"   MSE: {mse:.6f}\")\n",
    "    print(f\"   MAE: {mae:.6f}\")\n",
    "    print(f\"   R²: {r2:.6f}\")\n",
    "    print(f\"   Direction Accuracy: {direction_accuracy:.2%}\")\n",
    "    print(f\"   Average Confidence: {np.mean(confidences):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'direction_accuracy': direction_accuracy,\n",
    "        'confidence': np.mean(confidences)\n",
    "    }\n",
    "\n",
    "# Evaluate model if available\n",
    "if 'val_loader' in locals():\n",
    "    evaluation_results = evaluate_model(model, val_loader, device)\n",
    "else:\n",
    "    print(\"⚠️ Model not available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Model Loading and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "def load_trained_model(model_path='enhanced_transformer_best.pth'):\n",
    "    \"\"\"Load a trained model\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model file {model_path} not found\")\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Recreate model architecture\n",
    "    if 'features_df' in locals():\n",
    "        input_dim = features_df.shape[1]\n",
    "    else:\n",
    "        input_dim = 25  # Default\n",
    "    \n",
    "    loaded_model = EnhancedCryptoTransformer(\n",
    "        input_dim=input_dim,\n",
    "        **checkpoint['config']['model_params']\n",
    "    ).to(device)\n",
    "    \n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    print(f\"✅ Model loaded from {model_path}\")\n",
    "    print(f\"📊 Model was trained for {checkpoint.get('epoch', 'unknown') + 1} epochs\")\n",
    "    \n",
    "    return loaded_model, checkpoint\n",
    "\n",
    "# Function for inference\n",
    "def predict_trading_signal(model, sequence_data, device):\n",
    "    \"\"\"Generate trading signal from sequence data\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Ensure correct shape\n",
    "        if len(sequence_data.shape) == 2:\n",
    "            sequence_data = sequence_data.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        sequence_data = sequence_data.to(device)\n",
    "        \n",
    "        outputs = model(sequence_data)\n",
    "        \n",
    "        action = outputs['action'].cpu().numpy()[0][0]\n",
    "        confidence = outputs['confidence'].cpu().numpy()[0][0]\n",
    "        market_regime = outputs['market_regime'].cpu().numpy()[0]\n",
    "        volatility = outputs['volatility'].cpu().numpy()[0][0]\n",
    "        risk_assessment = outputs['risk_assessment'].cpu().numpy()[0]\n",
    "        \n",
    "    # Interpret results\n",
    "    signal_strength = abs(action) * confidence\n",
    "    \n",
    "    if action > 0.1:\n",
    "        signal = \"BUY\"\n",
    "    elif action < -0.1:\n",
    "        signal = \"SELL\"\n",
    "    else:\n",
    "        signal = \"HOLD\"\n",
    "    \n",
    "    regime_labels = ['Bull', 'Bear', 'Ranging', 'Volatile']\n",
    "    regime = regime_labels[np.argmax(market_regime)]\n",
    "    \n",
    "    risk_labels = ['Low', 'Medium', 'High']\n",
    "    risk_level = risk_labels[np.argmax(risk_assessment)]\n",
    "    \n",
    "    return {\n",
    "        'signal': signal,\n",
    "        'action': action,\n",
    "        'confidence': confidence,\n",
    "        'signal_strength': signal_strength,\n",
    "        'market_regime': regime,\n",
    "        'volatility': volatility,\n",
    "        'risk_level': risk_level\n",
    "    }\n",
    "\n",
    "# Test loading model\n",
    "if os.path.exists('enhanced_transformer_best.pth'):\n",
    "    loaded_model, checkpoint = load_trained_model()\n",
    "    if loaded_model:\n",
    "        print(\"✅ Model loading test successful!\")\n",
    "else:\n",
    "    print(\"⚠️ No trained model found for loading test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎮 Interactive Trading Signal Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive trading signal generator\n",
    "def generate_trading_signals_demo(num_signals=5):\n",
    "    \"\"\"Generate demo trading signals\"\"\"\n",
    "    if 'loaded_model' not in locals() or loaded_model is None:\n",
    "        print(\"⚠️ No loaded model available for demo\")\n",
    "        return\n",
    "    \n",
    "    if 'features_df' not in locals():\n",
    "        print(\"⚠️ No features available for demo\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🎮 Generating {num_signals} trading signals...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate random sequences from the dataset\n",
    "    for i in range(num_signals):\n",
    "        # Get random sequence\n",
    "        start_idx = np.random.randint(0, len(features_df) - 250)\n",
    "        sequence_data = features_df.iloc[start_idx:start_idx + 250].values\n",
    "        \n",
    "        # Get current price\n",
    "        current_price = sequence_data[-1, features_df.columns.get_loc('close')] if 'close' in features_df.columns else sequence_data[-1, 0]\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction = predict_trading_signal(loaded_model, sequence_data, device)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n📊 Signal {i+1}:\")\n",
    "        print(f\"   Current Price: ${current_price:,.2f}\")\n",
    "        print(f\"   Signal: {prediction['signal']}\")\n",
    "        print(f\"   Action: {prediction['action']:.3f}\")\n",
    "        print(f\"   Confidence: {prediction['confidence']:.3f}\")\n",
    "        print(f\"   Signal Strength: {prediction['signal_strength']:.3f}\")\n",
    "        print(f\"   Market Regime: {prediction['market_regime']}\")\n",
    "        print(f\"   Volatility: {prediction['volatility']:.3f}\")\n",
    "        print(f\"   Risk Level: {prediction['risk_level']}\")\n",
    "        \n",
    "        # Trading recommendation\n",
    "        if prediction['signal_strength'] > 0.7:\n",
    "            print(f\"   🎯 Recommendation: STRONG {prediction['signal']}\")\n",
    "        elif prediction['signal_strength'] > 0.4:\n",
    "            print(f\"   🎯 Recommendation: MODERATE {prediction['signal']}\")\n",
    "        else:\n",
    "            print(f\"   🎯 Recommendation: WEAK {prediction['signal']} - Consider holding\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\n✅ Demo completed!\")\n",
    "\n",
    "# Run demo if model is available\n",
    "if 'loaded_model' in locals() and loaded_model is not None:\n",
    "    generate_trading_signals_demo(3)\n",
    "else:\n",
    "    print(\"⚠️ Demo not available - no trained model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 System Information and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display system information\n",
    "import platform\n",
    "import psutil\n",
    "\n",
    "def display_system_info():\n",
    "    \"\"\"Display system information\"\"\"\n",
    "    print(\"🖥️ System Information\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "    print(f\"Python: {platform.python_version()}\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # CPU info\n",
    "    print(f\"\\n💻 CPU Info:\")\n",
    "    print(f\"   Cores: {psutil.cpu_count(logical=True)}\")\n",
    "    print(f\"   Usage: {psutil.cpu_percent()}%\")\n",
    "    print(f\"   Memory: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
    "    print(f\"   Memory Available: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # GPU info\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\n🎮 GPU Info:\")\n",
    "        print(f\"   Name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"   Allocated: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")\n",
    "        print(f\"   Cached: {torch.cuda.memory_reserved() / 1024**3:.1f} GB\")\n",
    "        print(f\"   Utilization: {torch.cuda.utilization()}%\")\n",
    "    \n",
    "    # Model info\n",
    "    if 'model' in locals():\n",
    "        print(f\"\\n🧠 Model Info:\")\n",
    "        print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        print(f\"   Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "        print(f\"   Size: {sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024:.1f} MB\")\n",
    "        \n",
    "        # Parameter count by type\n",
    "        param_counts = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            param_type = name.split('.')[0]\n",
    "            param_counts[param_type] = param_counts.get(param_type, 0) + param.numel()\n",
    "        \n",
    "        print(f\"   Parameter breakdown:\")\n",
    "        for param_type, count in param_counts.items():\n",
    "            print(f\"     {param_type}: {count:,}\")\n",
    "    \n",
    "    print(\"\\n✅ System information displayed!\")\n",
    "\n",
    "# Display system information\n",
    "display_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Quick Start Guide\n",
    "\n",
    "### **To run this notebook on GPU cloud services:**\n",
    "\n",
    "1. **Lambda Labs** (Recommended)\n",
    "   - Choose RTX A6000 instance\n",
    "   - Upload this notebook and required files\n",
    "   - Run cells sequentially\n",
    "\n",
    "2. **Vast.ai** (Cheapest)\n",
    "   - Rent RTX 4090 instance\n",
    "   - Use PyTorch Docker image\n",
    "   - Upload and run notebook\n",
    "\n",
    "3. **Google Colab Pro** (Easiest)\n",
    "   - Upload to Google Drive\n",
    "   - Open in Colab with GPU runtime\n",
    "   - Mount Drive and run\n",
    "\n",
    "### **Expected Costs:**\n",
    "- **Lambda Labs**: ~$0.60/hour = ~$6-12 for full training\n",
    "- **Vast.ai**: ~$0.30/hour = ~$3-6 for full training\n",
    "- **Colab Pro**: $10/month unlimited\n",
    "\n",
    "### **Training Time:**\n",
    "- **RTX A6000**: ~6-8 hours\n",
    "- **RTX 4090**: ~8-12 hours\n",
    "- **A100**: ~4-6 hours\n",
    "\n",
    "### **Files Needed:**\n",
    "- `enhanced_transformer_training.ipynb` (this notebook)\n",
    "- `transformer_enhanced_v2.py` (enhanced model)\n",
    "- `enhanced_features.py` (feature engineering)\n",
    "- `crypto_5min_2years.csv` (training data)\n",
    "\n",
    "### **To Start Training:**\n",
    "1. Run all cells above sequentially\n",
    "2. Uncomment the last line in the \"Start Training\" cell\n",
    "3. Execute the training cell\n",
    "4. Monitor progress and results\n",
    "\n",
    "### **After Training:**\n",
    "- Model saved as `enhanced_transformer_best.pth`\n",
    "- Training plots saved as `training_results.png`\n",
    "- Use model for inference and trading signals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
